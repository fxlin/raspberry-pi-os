{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"Introduction/","text":"Raspberry Pi OS project introduction or how to efficiently learn operating system development? A few years ago, I opened the source code of the Linux kernel for the first time. At that time, I considered myself more or less a skillful software developer: I knew a little bit of assembler and C programming, and had a high-level understanding of major operating system concepts, such as process scheduling and virtual memory management. However, my first attempt was a complete failure - I understood almost nothing. For other software projects that I have to deal with, I have a simple approach that usually works very well: I find the entry point of the program and then start reading the source code, going as deep as necessary to understand all the details that I am interested in. This approach works well, but not for something as sophisticated as an operating system. It was not just that it took me more than a week just to find an entry point - the main problem was that I quickly found myself in a situation where I was looking at a few lines of code, and I had no idea how to find any clues about what those lines were doing. This was especially true for the low-level assembler source code, but it worked no better for any other part of the system that I tried to investigate. I don't like the idea of dismissing a problem just because it looks complex from the beginning. Furthermore, I believe that there are no complex problems. Instead, there are a lot of problems we simply don't know how to address efficiently, so I started to look for an effective way to learn OS development in general and Linux in particular. Challenges in learning OS development I know that there are tons of books and documentation written about Linux kernel development, but neither of them provides me with the learning experience that I want. Half of the material are so superficial that I already know it. With the other half I have a very similar problem that I have with exploring the kernel source code: as soon as a book goes deep enough, 90% of the details appear to be irrelevant to the core concepts, but related to some security, performance or legacy considerations as well as to millions of features that the Linux kernel supports. As a result, instead of learning core operating system concepts, you always end up digging into the implementation details of those features. You may be wondering why I need to learn operating system development in the first place. For me, the main reason is that I was always interested in how things work under the hood. It is not just curiosity: the more difficult the task you are working on, frequently things begin to trace down to the operating system level. You just can't make fully informed technical decisions if you don't understand how everything works at a lower level. Another thing is that if you really like a technical challenge, working with OS development can be an exciting task for you. The next question you may ask is, why Linux? Other operating systems would probably be easier to approach. The answer is that I want my knowledge to be, at least in some way, relevant to what I am currently doing and to something I expect to be doing in the future. Linux is perfect in this regard because nowadays everything from small IoT devices to large servers tend to run Linux. When I said that most of the books about Linux kernel development didn't work well for me - I wasn't being quite honest. There was one book that explained some essential concepts using the actual source code that I was capable of fully understanding even though I am a novice in OS development. This book is \"Linux Device Drivers\", and it's no wonder that it is one of the most famous technical books about the Linux kernel. It starts by introducing source code of a simple driver that you can compile and play around with. Then it begins to introduce new driver related concepts one by one and explains how to modify the source code of the driver to use these new concepts. That is exactly what I refer to as a \"good learning experience\". The only problem with this book is that it focuses explicitly on driver development and says very little about core kernel implementation details. But why has nobody created a similar book for kernel developers? I think this is because if you use the current Linux kernel source code as a base for your book, then it's just not possible. There is no function, structure, or module that can be used as a simple starting point because there is nothing simple about the Linux source. You also can't introduce new concepts one at a time because in the source code, everything is very closely related to one another. After I realized this, an idea came to me: if the Linux kernel is too vast and too complicated to be used as a starting point for learning OS development, why don't I implement my own OS that will be explicitly designed for learning purposes? In this way, I can make the OS simple enough to provide a good learning experience. Also, if this OS will be implemented mostly by copying and simplifying different parts of the Linux kernel source, it would be straightforward to use it as a starting point to learn the Linux kernel as well. In addition to the OS, I decided to write a series of lectures that teaches major OS development concepts and fully explains the OS source code. OS requirements I started working on the project, which later became the RPi OS . The first thing I had to do was to determine what parts of kernel development I considered to be \"basic\", and what components I considered to be not so essential and can be skipped (at least in the beginning). In my understanding, each operating system has 2 fundamental goals: Run user processes in isolation. Provide each user process with a unified view of the machine hardware. To satisfy the first requirement, the RPi OS needs to have its own scheduler. If I want to implement a scheduler, I also have to handle timer interrupts. { xzl: not necessarily if you consider cooperative scheduling } The second requirement implies that the OS should support some drivers and provide system calls to expose them to user applications. Since this is for beginners, I don't want to work with complicated hardware, so the only drivers I care about are drivers that can write something to screen and read user input from a keyboard. Also, the OS needs to be able to load and execute user programs, so naturally it needs to support some sort of file system and be capable of understanding some sort of executable file format. It would be nice if the OS can support basic networking, but I don't want to focus on that in a text for beginners. So those are basically the things that I can identify as \"core concepts of any operating system\". Now let's take a look at the things that I want to ignore: 1. Performance I don't want to use any sophisticated algorithms in the OS. I am also going to disable all caches and other performance optimization techniques for simplicity. 1. Security It is true that the RPi OS has at least one security feature: virtual memory. Everything else can be safely ignored. 1. Multiprocessing and synchronization I am quite happy with my OS being executed on a single processor core. In particular, this allows me to get rid of a vast source of complexity - synchronization. 1. Support for multiple architectures and different types of devices More on this in the next section. 1. Millions of other features that any production-ready operating system supports How Raspberry Pi comes into play I already mentioned that I don't want the RPi OS to support multiple computer architectures or a lot of different devices. I felt even stronger about this after I dug into the Linux kernel driver model. It appears that even devices with similar purposes can largely vary in implementation details. This makes it very difficult to come up with simple abstractions around different driver types, and to reuse driver source code. To me, this seems like one of the primary sources of complexity in the Linux kernel, and I definitely want to avoid it in the RPi OS. Ok, but what kind of computer should I use then? I clearly don't want to test my bare metal programs using my working laptop, because I'm honestly not sure that it is going to survive. More importantly, I don't want people to buy an expensive laptop just to follow my OS development exercises (I don't think anybody would do this anyway). Emulators look like more or less a good choice, but I want to work with a real device because it gives me the feeling that I am doing something real rather than playing with bare metal programming . {xzl: very true} I ended up using the Raspberry Pi, in particular, the Raspberry Pi 3 Model B . Using this device looks like the ideal choice for a number of reasons: It costs something around $35. I think that should be an affordable price. This device is specially designed for learning. Its inner architecture is as simple as possible, and that perfectly suits my needs. {xzl: not necessarily. Rpi3 has many hardware quirks. But it's probably one of the few viable choices. More on this later} This device uses ARM v8 architecture. This is a simple RISC architecture, is very well adapted to OS authors' needs, and doesn't have so many legacy requirements as, for example, the popular x86 architecture. If you don't believe me, you can compare the amount of source code in the /arch/arm64 and /arch/x86 folders in the Linux kernel. {xzl: I concur. I want you to learn something new enough and used in real-world. This also explains why I rule out RISC-V} The OS is not compatible with the older versions of the Raspberry Pi, because neither of them support 64 bit ARM v8 architecture, though I think that support for all future devices should be trivial. Working with community One major drawback of any technical book is that very soon after release each book becomes obsolete. Technology nowadays is evolving so fast that it is almost impossible for book writers to keep up with it. That's why I like the idea of an \"open source book\" - a book that is freely available on the internet and encourages its readers to participate in content creation and validation. If the book content is available on Github, it is very easy for any reader to fix and develop new code samples, update the book content, and participate in writing new chapters. I understand that right now the project is not perfect, and at the time of writing it is even not finished. But I still want to publish it now, because I hope that with the help of the community I will be able to not only complete the project faster but also to make it much better and much more useful than it was in the beginning. Previous Page Main Page Next Page Contributing to the Raspberry PI OS","title":"Introduction"},{"location":"Introduction/#raspberry-pi-os-project-introduction-or-how-to-efficiently-learn-operating-system-development","text":"A few years ago, I opened the source code of the Linux kernel for the first time. At that time, I considered myself more or less a skillful software developer: I knew a little bit of assembler and C programming, and had a high-level understanding of major operating system concepts, such as process scheduling and virtual memory management. However, my first attempt was a complete failure - I understood almost nothing. For other software projects that I have to deal with, I have a simple approach that usually works very well: I find the entry point of the program and then start reading the source code, going as deep as necessary to understand all the details that I am interested in. This approach works well, but not for something as sophisticated as an operating system. It was not just that it took me more than a week just to find an entry point - the main problem was that I quickly found myself in a situation where I was looking at a few lines of code, and I had no idea how to find any clues about what those lines were doing. This was especially true for the low-level assembler source code, but it worked no better for any other part of the system that I tried to investigate. I don't like the idea of dismissing a problem just because it looks complex from the beginning. Furthermore, I believe that there are no complex problems. Instead, there are a lot of problems we simply don't know how to address efficiently, so I started to look for an effective way to learn OS development in general and Linux in particular.","title":"Raspberry Pi OS project introduction or how to efficiently learn operating system development?"},{"location":"Introduction/#challenges-in-learning-os-development","text":"I know that there are tons of books and documentation written about Linux kernel development, but neither of them provides me with the learning experience that I want. Half of the material are so superficial that I already know it. With the other half I have a very similar problem that I have with exploring the kernel source code: as soon as a book goes deep enough, 90% of the details appear to be irrelevant to the core concepts, but related to some security, performance or legacy considerations as well as to millions of features that the Linux kernel supports. As a result, instead of learning core operating system concepts, you always end up digging into the implementation details of those features. You may be wondering why I need to learn operating system development in the first place. For me, the main reason is that I was always interested in how things work under the hood. It is not just curiosity: the more difficult the task you are working on, frequently things begin to trace down to the operating system level. You just can't make fully informed technical decisions if you don't understand how everything works at a lower level. Another thing is that if you really like a technical challenge, working with OS development can be an exciting task for you. The next question you may ask is, why Linux? Other operating systems would probably be easier to approach. The answer is that I want my knowledge to be, at least in some way, relevant to what I am currently doing and to something I expect to be doing in the future. Linux is perfect in this regard because nowadays everything from small IoT devices to large servers tend to run Linux. When I said that most of the books about Linux kernel development didn't work well for me - I wasn't being quite honest. There was one book that explained some essential concepts using the actual source code that I was capable of fully understanding even though I am a novice in OS development. This book is \"Linux Device Drivers\", and it's no wonder that it is one of the most famous technical books about the Linux kernel. It starts by introducing source code of a simple driver that you can compile and play around with. Then it begins to introduce new driver related concepts one by one and explains how to modify the source code of the driver to use these new concepts. That is exactly what I refer to as a \"good learning experience\". The only problem with this book is that it focuses explicitly on driver development and says very little about core kernel implementation details. But why has nobody created a similar book for kernel developers? I think this is because if you use the current Linux kernel source code as a base for your book, then it's just not possible. There is no function, structure, or module that can be used as a simple starting point because there is nothing simple about the Linux source. You also can't introduce new concepts one at a time because in the source code, everything is very closely related to one another. After I realized this, an idea came to me: if the Linux kernel is too vast and too complicated to be used as a starting point for learning OS development, why don't I implement my own OS that will be explicitly designed for learning purposes? In this way, I can make the OS simple enough to provide a good learning experience. Also, if this OS will be implemented mostly by copying and simplifying different parts of the Linux kernel source, it would be straightforward to use it as a starting point to learn the Linux kernel as well. In addition to the OS, I decided to write a series of lectures that teaches major OS development concepts and fully explains the OS source code.","title":"Challenges in learning OS development"},{"location":"Introduction/#os-requirements","text":"I started working on the project, which later became the RPi OS . The first thing I had to do was to determine what parts of kernel development I considered to be \"basic\", and what components I considered to be not so essential and can be skipped (at least in the beginning). In my understanding, each operating system has 2 fundamental goals: Run user processes in isolation. Provide each user process with a unified view of the machine hardware. To satisfy the first requirement, the RPi OS needs to have its own scheduler. If I want to implement a scheduler, I also have to handle timer interrupts. { xzl: not necessarily if you consider cooperative scheduling } The second requirement implies that the OS should support some drivers and provide system calls to expose them to user applications. Since this is for beginners, I don't want to work with complicated hardware, so the only drivers I care about are drivers that can write something to screen and read user input from a keyboard. Also, the OS needs to be able to load and execute user programs, so naturally it needs to support some sort of file system and be capable of understanding some sort of executable file format. It would be nice if the OS can support basic networking, but I don't want to focus on that in a text for beginners. So those are basically the things that I can identify as \"core concepts of any operating system\". Now let's take a look at the things that I want to ignore: 1. Performance I don't want to use any sophisticated algorithms in the OS. I am also going to disable all caches and other performance optimization techniques for simplicity. 1. Security It is true that the RPi OS has at least one security feature: virtual memory. Everything else can be safely ignored. 1. Multiprocessing and synchronization I am quite happy with my OS being executed on a single processor core. In particular, this allows me to get rid of a vast source of complexity - synchronization. 1. Support for multiple architectures and different types of devices More on this in the next section. 1. Millions of other features that any production-ready operating system supports","title":"OS requirements"},{"location":"Introduction/#how-raspberry-pi-comes-into-play","text":"I already mentioned that I don't want the RPi OS to support multiple computer architectures or a lot of different devices. I felt even stronger about this after I dug into the Linux kernel driver model. It appears that even devices with similar purposes can largely vary in implementation details. This makes it very difficult to come up with simple abstractions around different driver types, and to reuse driver source code. To me, this seems like one of the primary sources of complexity in the Linux kernel, and I definitely want to avoid it in the RPi OS. Ok, but what kind of computer should I use then? I clearly don't want to test my bare metal programs using my working laptop, because I'm honestly not sure that it is going to survive. More importantly, I don't want people to buy an expensive laptop just to follow my OS development exercises (I don't think anybody would do this anyway). Emulators look like more or less a good choice, but I want to work with a real device because it gives me the feeling that I am doing something real rather than playing with bare metal programming . {xzl: very true} I ended up using the Raspberry Pi, in particular, the Raspberry Pi 3 Model B . Using this device looks like the ideal choice for a number of reasons: It costs something around $35. I think that should be an affordable price. This device is specially designed for learning. Its inner architecture is as simple as possible, and that perfectly suits my needs. {xzl: not necessarily. Rpi3 has many hardware quirks. But it's probably one of the few viable choices. More on this later} This device uses ARM v8 architecture. This is a simple RISC architecture, is very well adapted to OS authors' needs, and doesn't have so many legacy requirements as, for example, the popular x86 architecture. If you don't believe me, you can compare the amount of source code in the /arch/arm64 and /arch/x86 folders in the Linux kernel. {xzl: I concur. I want you to learn something new enough and used in real-world. This also explains why I rule out RISC-V} The OS is not compatible with the older versions of the Raspberry Pi, because neither of them support 64 bit ARM v8 architecture, though I think that support for all future devices should be trivial.","title":"How Raspberry Pi comes into play"},{"location":"Introduction/#working-with-community","text":"One major drawback of any technical book is that very soon after release each book becomes obsolete. Technology nowadays is evolving so fast that it is almost impossible for book writers to keep up with it. That's why I like the idea of an \"open source book\" - a book that is freely available on the internet and encourages its readers to participate in content creation and validation. If the book content is available on Github, it is very easy for any reader to fix and develop new code samples, update the book content, and participate in writing new chapters. I understand that right now the project is not perfect, and at the time of writing it is even not finished. But I still want to publish it now, because I hope that with the help of the community I will be able to not only complete the project faster but also to make it much better and much more useful than it was in the beginning.","title":"Working with community"},{"location":"Introduction/#previous-page","text":"Main Page","title":"Previous Page"},{"location":"Introduction/#next-page","text":"Contributing to the Raspberry PI OS","title":"Next Page"},{"location":"lesson00/exercises/","text":"Exercise Follow the given instructions and bring up everything. Stating your setup: if you use QEMU: attach a screenshot of the baremetal program running if you use Rpi3: show a picture (e.g. taken by your phone) of the baremetal program running Deliverable A docx or PDF file answering the question above.","title":"Exercises"},{"location":"lesson00/exercises/#exercise","text":"Follow the given instructions and bring up everything. Stating your setup: if you use QEMU: attach a screenshot of the baremetal program running if you use Rpi3: show a picture (e.g. taken by your phone) of the baremetal program running","title":"Exercise"},{"location":"lesson00/exercises/#deliverable","text":"A docx or PDF file answering the question above.","title":"Deliverable"},{"location":"lesson00/rpi-os/","text":"0: Sharpen your tools About the docs Terms Dev environment Host OS Toolchain Platform Setup Approach 1: the real hardware Check list Prep Raspberry Pi 3 Model B Load Raspbian OS to the SD card Plug in the serial cable Powering up RPi3 An example setup Test your dev workflow Background: what's on SD card? Update config.txt Build & load sample baremetal program Approach 2: QEMU About the docs Be aware: it may contain URLs referring to the upstream git repo, which may slightly differ from what we use. Terms baremetal; kernel; kernel binary; kernel image Dev environment This is where you develop kernel code. We have configured departmental server(s) for you to use. See here . You can develop on your local machine and test on the servers. Alternatively, you may do everything on your local machine, here are suggestions: Linux. Recommended: Ubuntu 20.04 LTS. Windows: WSL or WSL2. See instructions . OS X: (likely HomeBrew is needed. Not tested) Toolchain These are compiler, linker, etc. for us to generate the kernel code. Use the one provided by Ubuntu $ sudo apt install gcc-aarch64-linux-gnu $ aarch64-linux-gnu-gcc --version aarch64-linux-gnu-gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0 Test Platform This is where you run the kernel code. Approach 1: the real hardware Check list Required: An Rpi3 board (Model B or B+) link Required: A USB to TTL serial cable Amazon . Connection inside the dongle: black-GND; green-TXD; white-RXD; red-VCC. Required: A micro SD card. The capacity can be humble (e.g. 4GB). The speed does not matter much. The one I used was $6. Rpi's official page about uSD ) Required: SD card reader. To be plugged in your PC for loading kernel to the micro SD card. A cheap one can be $7 on Amazon Recommended: A micro USB cable for powering Rpi3. Prep Raspberry Pi 3 Model B Older versions of Raspberry Pi are not going to work with this tutorial because all lessons are designed to use a 64-bit processor that supports ARMv8 architecture, and such processor is only available in the Raspberry Pi 3. Newer versions, including Raspberry Pi 3 Model B+ should work fine. Load Raspbian OS to the SD card Raspbian is a Debian-based Linux distro. It's the official OS for Rpi3. Why we need Raspbian? 1. to test USB to TTL cable connectivity initially. 2. after installing Raspbian, the SD card is formatted in the right way. All the proprietary binary blobs needed to boot Rpi3 are also in place. Load the SD card with Raspbian OS. Follow the official instructions . Plug in the serial cable After you get a serial cable, you need to test your connection. If you never did this before I recommend you to follow this guide It describes the process of connecting your Raspberry PI via a serial cable in great details. Basically, you run Raspberry's official OS to ensure the hardware setup is fine. Powering up RPi3 We recommend you power Rpi3 through its micro USB port. Perhaps use a flip switch on the other side of the USB power for power cycling Rpi3. The guide above also describes how to power your Raspberry Pi using a serial cable. RPi OS works fine with such kind of setup, however, in this case, you need to run your terminal emulator right after you plug in the cable. Check this issue for details. Rpi3 <-- micro USB ---> PC Rpi3 <-- micro USB ---> Wall charger Power cycling Rpi3, you should see Linux kernel console output on PC terminal. An example setup This is my desktop when I hack with the Rpi3 kernel. Test your dev workflow Background: what's on SD card? On powering up, Rpi3 looks for the following files on boot partition of the SD card. bootcode.bin: the proprietary bootloader for enabling SDRAM. This comes with Raspbian. start.elf: the proprietary firmware loaded by the bootloader. Using the updated Raspbian OS. This comes with Raspbian. fixup.dat: needed to use 1GB of memory. This comes with Raspbian. config.txt: to be parsed by start.elf and decide boot behavior. It offers a great deal of options which is pretty cool. A default one comes with Raspbian. This file is to be customized by us kernel8.img: our kernel. Summary: we need to change config.txt (once) and kernel8.img (every time we re-compile kernel) on the SD card. Update config.txt Plug the SD card to PC via the card reader. Open config.txt which is on the boot partition. The following two lines are crucial. Add them to config.txt. arm_64bit=1 enable_uart=1 Note: multiple online tutorials advise options like kernel_old=1 or arm_control . You do NOT need those. With our options in config.txt above, Rpi3 will load the kernel named kernel8.img to 0x80000 . Check the official doc for config.txt above. Look for kernel_address . Ref: the official doc for config.txt. Build & load sample baremetal program ... to ensure our toolchain works fine. git clone git@github.com:fxlin/raspi3-tutorial.git cd raspi3-tutorial git checkout b026449 cd 05_uart0 make qemu-system-aarch64 -M raspi3 -kernel kernel8.img -serial stdio Note : the repo above (raspi3-tutorial.git) is NOT our project repo. It's someone's code for testing rpi3 hardware. We are just using for testing ONLY. Copy kernel8.img to the SD card. Eject the SD card from PC. Plug the SD to Rpi3. Make sure the serial connection is good and terminal emulator on your PC is ready. Power cycle Rpi3. You should see something like: (Your serial number may be different) Viola! You just built your first baremetal program for Rpi3! Approach 2: QEMU You are required to compile QEMU from source. Need QEMU >v2.12. Newer version is likely fine. The following shows the default QEMU coming with Ubuntu 18.04 is too old. For instance: $ qemu-system-aarch64 --version QEMU emulator version 2.11.1(Debian 1:2.11+dfsg-1ubuntu7.26) Copyright (c) 2003-2017 Fabrice Bellard and the QEMU Project developers Now, build QEMU from source. Clean any pre-installed qemu and install necessary tools: sudo apt remove qemu-system-arm sudo apt install gdb-multiarch build-essential pkg-config sudo apt install libglib2.0-dev libfdt-dev libpixman-1-dev zlib1g-dev Grab the source. We specify QEMU v4.2. git clone git://git.qemu.org/qemu.git cd qemu git checkout v4.2.0 ./configure --target-list=aarch64-softmmu make -j`nproc` export PATH=\"$(pwd)/aarch64-softmmu:${PATH}\" If successful, this will result in QEMU executables in ./aarch64-softmmu/. The last line above adds the path to our search path. If you encounter compilation errors (e.g. unmet dependencies), make sure you run all apt get commands above. Now try QEMU & check its version. The supported machines should include Rpi3 $ qemu-system-aarch64 --version QEMU emulator version 4.2.0 (v4.2.0-11797-g2890edc853-dirty) Copyright (c) 2003-2019 Fabrice Bellard and the QEMU Project developers $ qemu-system-aarch64 -M help|grep rasp raspi2 Raspberry Pi 2 raspi3 Raspberry Pi 3 Test QEMU with Rpi3 baremetal code (NOTE: this repo is for validating your toolchain & QEMU build; it is NOT our course project) git clone https://github.com/fxlin/raspi3-tutorial.git cd raspi3-tutorial git checkout b026449 cd 05_uart0 make qemu-system-aarch64 -M raspi3 -kernel kernel8.img -serial stdio If everything works fine, you should see QMEU print out: My serial number is: 0000000000000000 Note: the test program runs an infinite loop which will cause high CPU usage on your host machine. Kill the test program timely. On Linux (terminal) On Windows (WSL)","title":"0: Sharpen your tools"},{"location":"lesson00/rpi-os/#0-sharpen-your-tools","text":"About the docs Terms Dev environment Host OS Toolchain Platform Setup Approach 1: the real hardware Check list Prep Raspberry Pi 3 Model B Load Raspbian OS to the SD card Plug in the serial cable Powering up RPi3 An example setup Test your dev workflow Background: what's on SD card? Update config.txt Build & load sample baremetal program Approach 2: QEMU","title":"0: Sharpen your tools"},{"location":"lesson00/rpi-os/#about-the-docs","text":"Be aware: it may contain URLs referring to the upstream git repo, which may slightly differ from what we use.","title":"About the docs"},{"location":"lesson00/rpi-os/#terms","text":"baremetal; kernel; kernel binary; kernel image","title":"Terms"},{"location":"lesson00/rpi-os/#dev-environment","text":"This is where you develop kernel code. We have configured departmental server(s) for you to use. See here . You can develop on your local machine and test on the servers. Alternatively, you may do everything on your local machine, here are suggestions: Linux. Recommended: Ubuntu 20.04 LTS. Windows: WSL or WSL2. See instructions . OS X: (likely HomeBrew is needed. Not tested)","title":"Dev environment"},{"location":"lesson00/rpi-os/#toolchain","text":"These are compiler, linker, etc. for us to generate the kernel code. Use the one provided by Ubuntu $ sudo apt install gcc-aarch64-linux-gnu $ aarch64-linux-gnu-gcc --version aarch64-linux-gnu-gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0","title":"Toolchain"},{"location":"lesson00/rpi-os/#test-platform","text":"This is where you run the kernel code.","title":"Test Platform"},{"location":"lesson00/rpi-os/#approach-1-the-real-hardware","text":"","title":"Approach 1: the real hardware"},{"location":"lesson00/rpi-os/#check-list","text":"Required: An Rpi3 board (Model B or B+) link Required: A USB to TTL serial cable Amazon . Connection inside the dongle: black-GND; green-TXD; white-RXD; red-VCC. Required: A micro SD card. The capacity can be humble (e.g. 4GB). The speed does not matter much. The one I used was $6. Rpi's official page about uSD ) Required: SD card reader. To be plugged in your PC for loading kernel to the micro SD card. A cheap one can be $7 on Amazon Recommended: A micro USB cable for powering Rpi3.","title":"Check list"},{"location":"lesson00/rpi-os/#prep-raspberry-pi-3-model-b","text":"Older versions of Raspberry Pi are not going to work with this tutorial because all lessons are designed to use a 64-bit processor that supports ARMv8 architecture, and such processor is only available in the Raspberry Pi 3. Newer versions, including Raspberry Pi 3 Model B+ should work fine.","title":"Prep Raspberry Pi 3 Model B"},{"location":"lesson00/rpi-os/#load-raspbian-os-to-the-sd-card","text":"Raspbian is a Debian-based Linux distro. It's the official OS for Rpi3. Why we need Raspbian? 1. to test USB to TTL cable connectivity initially. 2. after installing Raspbian, the SD card is formatted in the right way. All the proprietary binary blobs needed to boot Rpi3 are also in place. Load the SD card with Raspbian OS. Follow the official instructions .","title":"Load Raspbian OS to the SD card"},{"location":"lesson00/rpi-os/#plug-in-the-serial-cable","text":"After you get a serial cable, you need to test your connection. If you never did this before I recommend you to follow this guide It describes the process of connecting your Raspberry PI via a serial cable in great details. Basically, you run Raspberry's official OS to ensure the hardware setup is fine.","title":"Plug in the serial cable"},{"location":"lesson00/rpi-os/#powering-up-rpi3","text":"We recommend you power Rpi3 through its micro USB port. Perhaps use a flip switch on the other side of the USB power for power cycling Rpi3. The guide above also describes how to power your Raspberry Pi using a serial cable. RPi OS works fine with such kind of setup, however, in this case, you need to run your terminal emulator right after you plug in the cable. Check this issue for details. Rpi3 <-- micro USB ---> PC Rpi3 <-- micro USB ---> Wall charger Power cycling Rpi3, you should see Linux kernel console output on PC terminal.","title":"Powering up RPi3"},{"location":"lesson00/rpi-os/#an-example-setup","text":"This is my desktop when I hack with the Rpi3 kernel.","title":"An example setup"},{"location":"lesson00/rpi-os/#test-your-dev-workflow","text":"","title":"Test your dev workflow"},{"location":"lesson00/rpi-os/#background-whats-on-sd-card","text":"On powering up, Rpi3 looks for the following files on boot partition of the SD card. bootcode.bin: the proprietary bootloader for enabling SDRAM. This comes with Raspbian. start.elf: the proprietary firmware loaded by the bootloader. Using the updated Raspbian OS. This comes with Raspbian. fixup.dat: needed to use 1GB of memory. This comes with Raspbian. config.txt: to be parsed by start.elf and decide boot behavior. It offers a great deal of options which is pretty cool. A default one comes with Raspbian. This file is to be customized by us kernel8.img: our kernel. Summary: we need to change config.txt (once) and kernel8.img (every time we re-compile kernel) on the SD card.","title":"Background: what's on SD card?"},{"location":"lesson00/rpi-os/#update-configtxt","text":"Plug the SD card to PC via the card reader. Open config.txt which is on the boot partition. The following two lines are crucial. Add them to config.txt. arm_64bit=1 enable_uart=1 Note: multiple online tutorials advise options like kernel_old=1 or arm_control . You do NOT need those. With our options in config.txt above, Rpi3 will load the kernel named kernel8.img to 0x80000 . Check the official doc for config.txt above. Look for kernel_address . Ref: the official doc for config.txt.","title":"Update config.txt"},{"location":"lesson00/rpi-os/#build-load-sample-baremetal-program","text":"... to ensure our toolchain works fine. git clone git@github.com:fxlin/raspi3-tutorial.git cd raspi3-tutorial git checkout b026449 cd 05_uart0 make qemu-system-aarch64 -M raspi3 -kernel kernel8.img -serial stdio Note : the repo above (raspi3-tutorial.git) is NOT our project repo. It's someone's code for testing rpi3 hardware. We are just using for testing ONLY. Copy kernel8.img to the SD card. Eject the SD card from PC. Plug the SD to Rpi3. Make sure the serial connection is good and terminal emulator on your PC is ready. Power cycle Rpi3. You should see something like: (Your serial number may be different) Viola! You just built your first baremetal program for Rpi3!","title":"Build &amp; load sample baremetal program"},{"location":"lesson00/rpi-os/#approach-2-qemu","text":"You are required to compile QEMU from source. Need QEMU >v2.12. Newer version is likely fine. The following shows the default QEMU coming with Ubuntu 18.04 is too old. For instance: $ qemu-system-aarch64 --version QEMU emulator version 2.11.1(Debian 1:2.11+dfsg-1ubuntu7.26) Copyright (c) 2003-2017 Fabrice Bellard and the QEMU Project developers Now, build QEMU from source. Clean any pre-installed qemu and install necessary tools: sudo apt remove qemu-system-arm sudo apt install gdb-multiarch build-essential pkg-config sudo apt install libglib2.0-dev libfdt-dev libpixman-1-dev zlib1g-dev Grab the source. We specify QEMU v4.2. git clone git://git.qemu.org/qemu.git cd qemu git checkout v4.2.0 ./configure --target-list=aarch64-softmmu make -j`nproc` export PATH=\"$(pwd)/aarch64-softmmu:${PATH}\" If successful, this will result in QEMU executables in ./aarch64-softmmu/. The last line above adds the path to our search path. If you encounter compilation errors (e.g. unmet dependencies), make sure you run all apt get commands above. Now try QEMU & check its version. The supported machines should include Rpi3 $ qemu-system-aarch64 --version QEMU emulator version 4.2.0 (v4.2.0-11797-g2890edc853-dirty) Copyright (c) 2003-2019 Fabrice Bellard and the QEMU Project developers $ qemu-system-aarch64 -M help|grep rasp raspi2 Raspberry Pi 2 raspi3 Raspberry Pi 3 Test QEMU with Rpi3 baremetal code (NOTE: this repo is for validating your toolchain & QEMU build; it is NOT our course project) git clone https://github.com/fxlin/raspi3-tutorial.git cd raspi3-tutorial git checkout b026449 cd 05_uart0 make qemu-system-aarch64 -M raspi3 -kernel kernel8.img -serial stdio If everything works fine, you should see QMEU print out: My serial number is: 0000000000000000 Note: the test program runs an infinite loop which will cause high CPU usage on your host machine. Kill the test program timely. On Linux (terminal) On Windows (WSL)","title":"Approach 2: QEMU"},{"location":"lesson01/exercises/","text":"Exercises Inspect the resultant kernel binary (kernel8.elf) with readelf and/or objdump . Answer the following questions: How many sections are in the elf file? What are these sections? How many instructions are in the elf file? Does each line of source in boot.S correspond to one instruction? If not, why? Watch the video Eben Upton on Rpi3 and answer the questions: What would be the benefit of supporting 64-bit (AArch64)? In particular, why it is important to support 64-bit by an operating system? Have you ever used Raspberry in any chance? What was for? Deliverable A docx or PDF file answering (1) (2) above.","title":"Exercises"},{"location":"lesson01/exercises/#exercises","text":"Inspect the resultant kernel binary (kernel8.elf) with readelf and/or objdump . Answer the following questions: How many sections are in the elf file? What are these sections? How many instructions are in the elf file? Does each line of source in boot.S correspond to one instruction? If not, why? Watch the video Eben Upton on Rpi3 and answer the questions: What would be the benefit of supporting 64-bit (AArch64)? In particular, why it is important to support 64-bit by an operating system? Have you ever used Raspberry in any chance? What was for?","title":"Exercises"},{"location":"lesson01/exercises/#deliverable","text":"A docx or PDF file answering (1) (2) above.","title":"Deliverable"},{"location":"lesson01/rpi-os/","text":"1: Baremetal HelloWorld Objectives Roadmap Terms Project structure Makefile walkthrough Build targets & rules Bake the kernel binaries The linker script Kernel startup Booting the kernel Kernel memory layout The kernel_main function A bit about the Rpi3 hardware Memory-mapped IO UART GPIO Walkthrough: the UART code Init: GPIO alternative function selection Init: Mini UART Sending data over UART Take the kernel for a spin Rpi3 QEMU Objectives We will build: a minimal, baremetal program that can print \"Hello world\" via Rpi3's UART. Students will experience: The C project structure The use of cross-compilation toolchain arm64 assembly (lightly) Basic knowledge on Rpi3 and its UART hardware Roadmap Create a Makefile project. Add minimum code to boot the platform. Initialize the UART hardware. Send characters to the UART registers. Terms Strictly speaking, this baremetal program is not a \"kernel\". We nevertheless call it so for ease of explanation. \"Raspberry Pi\" means the actual Rpi3 hardware. \"QEMU\" means the Rpi3 platform as emulated by QEMU. We will explain details where the real hardware behaves differently from QEMU. Project structure Makefile We will use the make utility to build the kernel. make 's behavior is configured by a Makefile, which contains instructions on how to compile and link the source code. src This folder contains all of the source code. include All of the header files are placed here. Note: Of all the subsequent experiments in p1, the source code has the same structure. Makefile walkthrough If you are not familiar with Makefiles, read this article. The complete Makefile: ARMGNU ?= aarch64-linux-gnu COPS = -Wall -nostdlib -nostartfiles -ffreestanding -Iinclude -mgeneral-regs-only -O0 -g ASMOPS = -Iinclude -g BUILD_DIR = build SRC_DIR = src all : kernel8.img clean : rm -rf $(BUILD_DIR) *.img $(BUILD_DIR)/%_c.o: $(SRC_DIR)/%.c mkdir -p $(@D) $(ARMGNU)-gcc $(COPS) -MMD -c $< -o $@ $(BUILD_DIR)/%_s.o: $(SRC_DIR)/%.S $(ARMGNU)-gcc $(ASMOPS) -MMD -c $< -o $@ C_FILES = $(wildcard $(SRC_DIR)/*.c) ASM_FILES = $(wildcard $(SRC_DIR)/*.S) OBJ_FILES = $(C_FILES:$(SRC_DIR)/%.c=$(BUILD_DIR)/%_c.o) OBJ_FILES += $(ASM_FILES:$(SRC_DIR)/%.S=$(BUILD_DIR)/%_s.o) DEP_FILES = $(OBJ_FILES:%.o=%.d) -include $(DEP_FILES) kernel8.img: $(SRC_DIR)/linker.ld $(OBJ_FILES) $(ARMGNU)-ld -T $(SRC_DIR)/linker.ld -o $(BUILD_DIR)/kernel8.elf $(OBJ_FILES) $(ARMGNU)-objcopy $(BUILD_DIR)/kernel8.elf -O binary kernel8.img Let's inspect this file in detail: ARMGNU ?= aarch64-linux-gnu The Makefile starts with a variable definition. ARMGNU is a cross-compiler prefix. We need to use a cross-compiler because we are compiling the source code for the arm64 architecture on an x86 machine. So instead of gcc , we will use aarch64-linux-gnu-gcc . COPS = -Wall -nostdlib -nostartfiles -ffreestanding -Iinclude -mgeneral-regs-only ASMOPS = -Iinclude COPS and ASMOPS are options that we pass to the compiler when compiling C and assembler code, respectively. These options require a short explanation: -Wall Show all warnings. A good practice. -nostdlib Don't use the C standard library. Most of the calls in the C standard library eventually interact with the operating system. We are writing a bare-metal program, and we don't have any underlying operating system, so the C standard library is not going to work for us anyway. -nostartfiles Don't use standard startup files. Startup files are responsible for setting an initial stack pointer, initializing static data, and jumping to the main entry point. We are going to do all of this by ourselves. -ffreestanding A freestanding environment is an environment in which the standard library may not exist, and program startup may not necessarily be at main. The option -ffreestanding directs the compiler to not assume that standard functions have their usual definition. -Iinclude Search for header files in the include folder. -mgeneral-regs-only . Use only general-purpose registers. ARM processors also have NEON registers. We don't want the compiler to use them because they add additional complexity (since, for example, we will need to store the registers during a context switch). -g Include debugging info in the resultant ELF binary. -O0 Turn off any compiler optimization. For ease of debugging. BUILD_DIR = build SRC_DIR = src SRC_DIR and BUILD_DIR are directories that contain source code and compiled object files, respectively. all : kernel8.img clean : rm -rf $(BUILD_DIR) *.img Build targets & rules The first two targets are pretty simple: the all target is the default one, and it is executed whenever you type make without any arguments ( make always uses the first target as the default). This target just redirects all work to a different target, kernel8.img . The name \"kernel8.img\" is mandated by the Rpi3 firmware. The trailing 8 denotes ARMv8 which is a 64-bit architecture. This filename tells the firmware to boot the processor into 64-bit mode. The clean target is responsible for deleting all compilation artifacts and the compiled kernel image. $(BUILD_DIR)/%_c.o: $(SRC_DIR)/%.c mkdir -p $(@D) $(ARMGNU)-gcc $(COPS) -MMD -c $< -o $@ $(BUILD_DIR)/%_s.o: $(SRC_DIR)/%.S $(ARMGNU)-gcc $(ASMOPS) -MMD -c $< -o $@ The next two targets are responsible for compiling C and assembler files. If, for example, in the src directory we have foo.c and foo.S files, they will be compiled into build/foo_c.o and build/foo_s.o , respectively. $< and $@ are substituted at runtime with the input and output filenames ( foo.c and foo_c.o ). Before compiling C files, we also create a build directory in case it doesn't exist yet. C_FILES = $(wildcard $(SRC_DIR)/*.c) ASM_FILES = $(wildcard $(SRC_DIR)/*.S) OBJ_FILES = $(C_FILES:$(SRC_DIR)/%.c=$(BUILD_DIR)/%_c.o) OBJ_FILES += $(ASM_FILES:$(SRC_DIR)/%.S=$(BUILD_DIR)/%_s.o) Here we are building an array of all object files ( OBJ_FILES ) created from the concatenation of both C and assembler source files. DEP_FILES = $(OBJ_FILES:%.o=%.d) -include $(DEP_FILES) The next two lines are a little bit tricky. If you take a look at how we defined our compilation targets for both C and assembler source files, you will notice that we used the -MMD parameter. This parameter instructs the gcc compiler to create a dependency file for each generated object file. A dependency file defines all of the dependencies for a particular source file. These dependencies usually contain a list of all included headers. We need to include all of the generated dependency files so that make knows what exactly to recompile in case a header changes. Bake the kernel binaries $(ARMGNU)-ld -T $(SRC_DIR)/linker.ld -o kernel8.elf $(OBJ_FILES) We use the OBJ_FILES array to build the kernel8.elf file. We use the linker script src/linker.ld to define the basic layout of the resulting executable image (we will discuss the linker script in the next section). $(ARMGNU)-objcopy kernel8.elf -O binary kernel8.img kernel8.elf & kernel8.img build/kernel8.elf (\"kernel binary\"): Our build outcome as an ELF file. It contains all code, data, and debugging info. Often, to execute an ELF program in user space, there should be a loader to parse ELF, load code & data to designated memory locations, etc. For our kernel experiment, we do NOT have such a loader for the kernel itself. kernel8.img (\"kernel image\"): The raw instructions & data as extracted from kernel8.elf. The raw image is to be loaded to memory. Since it's a memory dump (see below), the load is as simple as byte-by-byte copy. The kernel image is produced by objcopy . Its manual says: \" objcopy can be used to generate a raw binary file by using an output target of \u2018binary\u2019 (e.g., use -O binary). When objcopy generates a raw binary file, it will essentially produce a memory dump of the contents of the input object file. All symbols and relocation information will be discarded. The memory dump will start at the load address of the lowest section copied into the output file.\" Q: can you use readelf to examine kernel8.elf, and explain your observation? The linker script A linker script describes how the sections in the input object files ( _c.o and _s.o ) should be mapped into the output file ( .elf ); it also controls the addresses of all program symbols (e.g. functions and variables). More information can be found here . Now let's take a look at the linker script: SECTIONS { .text.boot : { *(.text.boot) } .text : { *(.text) } .rodata : { *(.rodata) } .data : { *(.data) } . = ALIGN(0x8); bss_begin = .; .bss : { *(.bss*) } bss_end = .; } After startup, the Rpi3 GPU loads kernel8.img into memory 0x0 and starts execution from the beginning of the file. That's why the .text.boot section must come first; we are going to put the kernel startup code inside this section. QEMU behaves differently: it loads the kernel image at 0x80000. Q: How to tweak the linker script to update the start address? The .text , .rodata , and .data sections contain kernel instructions, read-only data, and global data with init values. The .bss section contains data that should be initialized to 0. By putting such data in a separate section, the compiler can save some space in the ELF binary\u2013\u2013only the section size is stored in the ELF header, but the section content is omitted. After booting up, our kernel initializes the .bss section to 0; that's why we need to record the start and end of the section (hence the bss_begin and bss_end symbols) and align the section so that it starts at an address that is a multiple of 8. This eases kernel programming because the str instruction can be used only with 8-byte-aligned addresses. Kernel startup Booting the kernel boot.S contains the kernel startup code: #include \"mm.h\" .section \".text.boot\" .globl _start _start: mrs x0, mpidr_el1 and x0, x0,#0xFF // Check processor id cbz x0, master // Hang for all non-primary CPU b proc_hang proc_hang: b proc_hang master: adr x0, bss_begin adr x1, bss_end sub x1, x1, x0 bl memzero mov sp, #LOW_MEMORY bl kernel_main Let's review this file in detail: .section \".text.boot\" First, we specify that everything defined in boot.S should go in the .text.boot section. Previously, we saw that this section is placed at the beginning of the kernel image by the linker script. So when the kernel is started, execution begins at the start function: .globl _start _start: mrs x0, mpidr_el1 and x0, x0,#0xFF // Check processor id cbz x0, master // Hang for all non-primary CPU b proc_hang Rpi3 has 4 cores, and after the device is powered on, each core begins to execute the same code. Our kernel only works with the first one and put all of the other cores in an endless loop. This is exactly what the _start function is responsible for. It gets the processor ID from the mpidr_el1 system register. Q: It may make more sense to put core 1-3 in deep sleep using wfi . How? Kernel memory layout If the current processor ID is 0, then execution branches to the master function: master: adr x0, bss_begin adr x1, bss_end sub x1, x1, x0 bl memzero Here, we clean the .bss section by calling memzero . We will define this function later. In ARMv8 architecture, by convention, the first seven arguments are passed to the called function via registers x0\u2013x6 (cf: our cheat sheet). The memzero function accepts only two arguments: the start address ( bss_begin ) and the size of the section needed to be cleaned ( bss_end - bss_begin ). mov sp, #LOW_MEMORY bl kernel_main After cleaning the .bss section, the kernel initializes the stack pointer and passes execution to the kernel_main function. The Rpi3 loads the kernel at address 0 (QEMU loads at 0x80000); that's why the initial stack pointer can be set to any location high enough so that stack will not override the kernel image when it grows sufficiently large. LOW_MEMORY is defined in mm.h and is equal to 4MB. As our kernel's stack won't grow very large and the image itself is tiny, 4MB is more than enough for us. Aside: Some ARM64 instructions used For those of you who are not familiar with ARM assembler syntax, let me quickly summarize the instructions that we have used: mrs Load value from a system register to one of the general purpose registers (x0\u2013x30) and Perform the logical AND operation. We use this command to strip the last byte from the value we obtain from the mpidr_el1 register. cbz Compare the result of the previously executed operation to 0 and jump (or branch in ARM terminology) to the provided label if the comparison yields true. b Perform an unconditional branch to some label. adr Load a label's relative address into the target register. In this case, we want pointers to the start and end of the .bss region. sub Subtract values from two registers. bl \"Branch with a link\": perform an unconditional branch and store the return address in x30 (the link register). When the subroutine is finished, use the ret instruction to jump back to the return address. mov Move a value between registers or from a constant to a register. Our cheat sheet summarizes common ARM64 instructions. For official documentation, here is the ARMv8-A developer's guide. It's a good resource if the ARM ISA is unfamiliar to you. This page specifically outlines the register usage convention in the ABI. The kernel_main function We have seen that the boot code eventually passes control to the kernel_main function. Let's take a look at it: #include \"mini_uart.h\" void kernel_main(void) { uart_init(); uart_send_string(\"Hello, world!\\r\\n\"); while (1) { uart_send(uart_recv()); } } This function is one of the simplest in the kernel. It works with the Mini UART device to print to screen and read user input. The kernel just prints Hello, world! and then enters an infinite loop that reads characters from the user and sends them back to the screen. A bit about the Rpi3 hardware The Rpi3 board is based on the BCM2837 SoC by Broadcom. The SoC manual is here . The SoC is not friendly for OS hackers: Broadcom poorly documents it and the hardware has many quirks. Despite so, the community figured out most of the SoC details over years because Rpi3's popularity. It's not our goal to dive in the SoC. Rather, our philosophy is to deal BCM2837-specific details as few as possible -- just enough to get our kernel working. We will spend more efforts on explaining generic hardware such as ARM64 cores, generic timers, irq controllers, etc. Rpi4 seems more friendly to kernel hackers. Memory-mapped IO On ARM-based SoCs, access to all devices is performed via memory-mapped registers. The Rpi3 SoC reserves physical memory address 0x3F000000 for IO devices. To configure a particular device, software reads/writes device registers. A device register is just a 32-bit region of memory. The meaning of each bit in each IO register is described in the SoC manual. The term \"device\" is heavily overloaded in many tech docs. Sometimes it means a board, e.g. \"an Rpi3 device\"; sometimes it means an IO peripheral, e.g. \"UART device\". We will be explicit. UART UART is a simple character device allowing software to send out text characters to a different machine. If you do not care about performance, UART requires very minimum software code. Therefore, it is often the first few IO devices to bring up when we build system software for a new machine. Only with UART meaning debugging is possible. (JTAG is another option which however requires more complex setup). In the simplest form, software writes ascii values to UART registers. The UART device converts written values to a sequence of high and low voltages on wire. This sequence is transmitted to your via the TTL-to-serial cable and is interpreted by your terminal emulator (e.g. PuTTY on Windows). Rpi3 has the two UART devices. Oddly enough, they are different. Name Type Comments UART0 PL011 Secondary, intended as Bluetooth connector UART1 mini UART Primary, intended as debug console UART1/Mini UART: easier to program; limited performance/functionalities. That's fine for our goal. For specification of the Mini UART registers: see page 8 of the SoC manual. UART0/PL011: richer functions; higher speed. Yet one needs to configure the board clock by talking to the GPU firmware. We won't do that. see Example code if you are interested. The above information is enough. More about Raspberry Pi UARTs: see the official web page . GPIO Another IO device is GPIO General-purpose input/output . GPIO provides a bunch of registers. Each bit in such a register corresponds to a pin on the Rpi3 board. By writing 1 or 0 to register bits, software can control the output voltage on the pins, e.g. for turning on/off LEDs connected to such pins. Reading is done in a similar fashion. The picture below shows GPIO pin headers populated on Rpi3. (Note: the picture shows Rpi2, which has the same pinout as Rpi3) An SoC often has limited number of pins. Software can control the use of these pins, e.g. for GPIO or for UART. Software does so by writing to specific memory-mapped registers. The GPIO can be used to configure the behavior of different GPIO pins. For example, to be able to use the Mini UART, we need to activate pins 14 and 15 and set them up to use this device. The image below illustrates how numbers are assigned to the GPIO pins: Walkthrough: the UART code The following init code configures pin 14 & 15 as UART in/out, sets up UART clock and its modes, etc. Much of the UART init code is irrelevant to QEMU. Since QEMU \"emulates\" the UARTs, it can dump whatever our kernel writes to the emulated UART registers to stdio. Example: qemu-system-aarch64 -M raspi3 -kernel ./kernel8.img -serial null -serial stdio The first -serial means UART0 which we do not touch; the second -serial means we direct UART1 to stdio. void uart_init ( void ) { unsigned int selector; selector = get32(GPFSEL1); selector &= ~(7<<12); // clean gpio14 selector |= 2<<12; // set alt5 for gpio14 selector &= ~(7<<15); // clean gpio15 selector |= 2<<15; // set alt5 for gpio 15 put32(GPFSEL1,selector); put32(GPPUD,0); delay(150); put32(GPPUDCLK0,(1<<14)|(1<<15)); delay(150); put32(GPPUDCLK0,0); put32(AUX_ENABLES,1); //Enable mini uart (this also enables access to it registers) put32(AUX_MU_CNTL_REG,0); //Disable auto flow control and disable receiver and transmitter (for now) put32(AUX_MU_IER_REG,0); //Disable receive and transmit interrupts put32(AUX_MU_LCR_REG,3); //Enable 8 bit mode put32(AUX_MU_MCR_REG,0); //Set RTS line to be always high put32(AUX_MU_BAUD_REG,270); //Set baud rate to 115200 put32(AUX_MU_CNTL_REG,3); //Finally, enable transmitter and receiver } Here, we use the two functions put32 and get32 . Those functions are very simple -- read and write some data to and from a 32-bit register. You can take a look at how they are implemented in utils.S . uart_init is one of the most complex and important functions in this lesson, and we will continue to examine it in the next three sections. Init: GPIO alternative function selection First, we need to activate the GPIO pins. Most of the pins can be used with different IO devices. So before using a particular pin, we need to select the pin's alternative function, a number from 0 to 5 that can be set for each pin and configures which IO device is virtually \"connected\" to the pin. See the list of all available GPIO alternative functions in the image below (taken from page 102 of the SoC manual) Here you can see that pins 14 and 15 have the TXD1 and RXD1 alternative functions available. This means that if we select alternative function number 5 for pins 14 and 15, they will be used as a Mini UART Transmit Data pin and Mini UART Receive Data pin, respectively. The GPFSEL1 register is used to control alternative functions for pins 10-19. The meaning of all the bits in those registers is shown in the following table (page 92 of the SoC manual): So now you know everything you need to understand the following lines of code that are used to configure GPIO pins 14 and 15 to work with the Mini UART device: unsigned int selector; selector = get32(GPFSEL1); selector &= ~(7<<12); // clean gpio14 selector |= 2<<12; // set alt5 for gpio14 selector &= ~(7<<15); // clean gpio15 selector |= 2<<15; // set alt5 for gpio 15 put32(GPFSEL1,selector); Init: GPIO pull-up/down & how we disable it When working with GPIO pins, you will often encounter terms such as pull-up/pull-down. These concepts are explained in great detail in this article. For those who are too lazy to read the whole article, I will briefly explain the pull-up/pull-down concept. If you use a particular pin as input and don't connect anything to this pin, you will not be able to identify whether the value of the pin is 1 or 0. In fact, the device will report random values. The pull-up/pull-down mechanism allows you to overcome this issue. If you set the pin to the pull-up state and nothing is connected to it, it will report 1 all the time (for the pull-down state, the value will always be 0). In our case, we need neither the pull-up nor the pull-down state, because both the 14 and 15 pins are going to be connected all the time. The pin state is preserved even after a reboot, so before using any pin, we always have to initialize its state. There are three available states: pull-up, pull-down, and neither (to remove the current pull-up or pull-down state), and we need the third one. Switching between pin states is not a very simple procedure because it requires physically toggling a switch on the electric circuit. This process involves the GPPUD and GPPUDCLK registers and is described on page 101 of the SoC manual: The GPIO Pull-up/down Clock Registers control the actuation of internal pull-downs on the respective GPIO pins. These registers must be used in conjunction with the GPPUD register to effect GPIO Pull-up/down changes. The following sequence of events is required: 1. Write to GPPUD to set the required control signal (i.e. Pull-up or Pull-Down or neither to remove the current Pull-up/down) 2. Wait 150 cycles \u2013 this provides the required set-up time for the control signal 3. Write to GPPUDCLK0/1 to clock the control signal into the GPIO pads you wish to modify \u2013 NOTE only the pads which receive a clock will be modified, all others will retain their previous state. 4. Wait 150 cycles \u2013 this provides the required hold time for the control signal 5. Write to GPPUD to remove the control signal 6. Write to GPPUDCLK0/1 to remove the clock This procedure describes how we can remove both the pull-up and pull-down states from a pin , which is what we are doing for pins 14 and 15 in the following code: put32(GPPUD,0); delay(150); put32(GPPUDCLK0,(1<<14)|(1<<15)); delay(150); put32(GPPUDCLK0,0); Init: Mini UART Now our Mini UART is connected to the GPIO pins, and the pins are configured. The rest of the uart_init function is dedicated to Mini UART initialization. put32(AUX_ENABLES,1); //Enable mini uart (this also enables access to its registers) put32(AUX_MU_CNTL_REG,0); //Disable auto flow control and disable receiver and transmitter (for now) put32(AUX_MU_IER_REG,0); //Disable receive and transmit interrupts put32(AUX_MU_LCR_REG,3); //Enable 8 bit mode put32(AUX_MU_MCR_REG,0); //Set RTS line to be always high put32(AUX_MU_BAUD_REG,270); //Set baud rate to 115200 put32(AUX_MU_CNTL_REG,3); //Finally, enable transmitter and receiver Let's examine this code snippet line by line. put32(AUX_ENABLES,1); //Enable mini uart (this also enables access to its registers) This line enables the Mini UART. We must do this in the beginning, because this also enables access to all the other Mini UART registers. put32(AUX_MU_CNTL_REG,0); //Disable auto flow control and disable receiver and transmitter (for now) Here we disable the receiver and transmitter before the configuration is finished. We also permanently disable auto-flow control because it requires us to use additional GPIO pins, and the TTL-to-serial cable doesn't support it. For more information about auto-flow control, you can refer to this article. put32(AUX_MU_IER_REG,0); //Disable receive and transmit interrupts It is possible to configure the Mini UART to generate a processor interrupt each time new data is available. We want to be as simple as possible. So for now, we will just disable this feature. put32(AUX_MU_LCR_REG,3); //Enable 8 bit mode Mini UART can support either 7- or 8-bit operations. This is because an ASCII character is 7 bits for the standard set and 8 bits for the extended. We are going to use 8-bit mode. put32(AUX_MU_MCR_REG,0); //Set RTS line to be always high The RTS line is used in the flow control and we don't need it. Set it to be high all the time. put32(AUX_MU_BAUD_REG,270); //Set baud rate to 115200 The baud rate is the rate at which information is transferred in a communication channel. \u201c115200 baud\u201d means that the serial port is capable of transferring a maximum of 115200 bits per second. The baud rate of your Raspberry Pi mini UART device should be the same as the baud rate in your terminal emulator. The Mini UART calculates baud rate according to the following equation: baudrate = system_clock_freq / (8 * ( baudrate_reg + 1 )) The system_clock_freq is 250 MHz, so we can easily calculate the value of baudrate_reg as 270. put32(AUX_MU_CNTL_REG,3); //Finally, enable transmitter and receiver After this line is executed, the Mini UART is ready for work! Sending data over UART After the Mini UART is ready, we can try to use it to send and receive some data. To do this, we can use the following two functions: void uart_send ( char c ) { while(1) { if(get32(AUX_MU_LSR_REG)&0x20) break; } put32(AUX_MU_IO_REG,c); } char uart_recv ( void ) { while(1) { if(get32(AUX_MU_LSR_REG)&0x01) break; } return(get32(AUX_MU_IO_REG)&0xFF); } Both of the functions start with an infinite loop, the purpose of which is to verify whether the device is ready to transmit or receive data. We are using the AUX_MU_LSR_REG register to do this. Bit zero, if set to 1, indicates that the data is ready; this means that we can read from the UART. Bit five, if set to 1, tells us that the transmitter is empty, meaning that we can write to the UART. Next, we use AUX_MU_IO_REG to either store the value of the transmitted character or read the value of the received character. We also have a very simple function that is capable of sending strings instead of characters: void uart_send_string(char* str) { for (int i = 0; str[i] != '\\0'; i ++) { uart_send((char)str[i]); } } This function just iterates over all characters in a string and sends them one by one. Low efficiency? Apparently Tx/Rx with busy wait burn lots of CPU cycles for no good. It's fine for our baremetal program -- simple & less error-prone. Production software often do interrupt-driven Rx/Tx. Take the kernel for a spin Run make to build the kernel. Rpi3 The Raspberry Pi startup sequence is the following (simplified): The device is powered on. The GPU starts up and reads the config.txt file from the boot partition. This file contains some configuration parameters that the GPU uses to further adjust the startup sequence. kernel8.img is loaded into memory and executed. Setup To be able to run our simple OS, the config.txt file should be the following: enable_uart=1 arm_64bit=1 kernel_old=1 disable_commandline_tags=1 kernel_old=1 specifies that the kernel image should be loaded at address 0. disable_commandline_tags instructs the GPU to not pass any command line arguments to the booted image. Run Copy the generated kernel8.img file to the boot partition of your Raspberry Pi flash card and delete kernel7.img as well as any other kernel*.img files on your SD card. Make sure you left all other files in the boot partition untouched (see 43 and 158 issues for details). Modify the config.txt file as described above. Connect the USB-to-TTL serial cable as described in the Prerequisites . Power on your Raspberry Pi. Open your terminal emulator. You should be able to see the Hello, world! message there. Aside (optional): prepare the SD card from scratch (w/o Raspbian) The steps above assume that you have Raspbian installed on your SD card. It is also possible to run the RPi OS using an empty SD card. Prepare your SD card: Use an MBR partition table Format the boot partition as FAT32 The card should be formatted exactly in the same way as it is required to install Raspbian. Check HOW TO FORMAT AN SD CARD AS FAT section in the official documenation for more information. Copy the following files to the card: bootcode.bin This is the GPU bootloader, it contains the GPU code to start the GPU and load the GPU firmware. start.elf This is the GPU firmware. It reads config.txt and enables the GPU to load and run ARM specific user code from kernel8.img Copy kernel8.img and config.txt files. Connect the USB-to-TTL serial cable. Power on your Raspberry Pi. Use your terminal emulator to connect to the RPi OS. Unfortunately, all Raspberry Pi firmware files are closed-sourced and undocumented. For more information about the Raspberry Pi startup sequence, you can refer to some unofficial sources, like this StackExchange question or this Github repository. QEMU Setup Follow the instructions in Prerequisites . Run $ qemu-system-aarch64 -M raspi3 -kernel ./kernel8.img -serial null -serial stdio VNC server running on 127.0.0.1:5900 Hello, world! <Ctrl-C>","title":"1: Baremetal HelloWorld"},{"location":"lesson01/rpi-os/#1-baremetal-helloworld","text":"Objectives Roadmap Terms Project structure Makefile walkthrough Build targets & rules Bake the kernel binaries The linker script Kernel startup Booting the kernel Kernel memory layout The kernel_main function A bit about the Rpi3 hardware Memory-mapped IO UART GPIO Walkthrough: the UART code Init: GPIO alternative function selection Init: Mini UART Sending data over UART Take the kernel for a spin Rpi3 QEMU","title":"1: Baremetal HelloWorld"},{"location":"lesson01/rpi-os/#objectives","text":"We will build: a minimal, baremetal program that can print \"Hello world\" via Rpi3's UART. Students will experience: The C project structure The use of cross-compilation toolchain arm64 assembly (lightly) Basic knowledge on Rpi3 and its UART hardware","title":"Objectives"},{"location":"lesson01/rpi-os/#roadmap","text":"Create a Makefile project. Add minimum code to boot the platform. Initialize the UART hardware. Send characters to the UART registers.","title":"Roadmap"},{"location":"lesson01/rpi-os/#terms","text":"Strictly speaking, this baremetal program is not a \"kernel\". We nevertheless call it so for ease of explanation. \"Raspberry Pi\" means the actual Rpi3 hardware. \"QEMU\" means the Rpi3 platform as emulated by QEMU. We will explain details where the real hardware behaves differently from QEMU.","title":"Terms"},{"location":"lesson01/rpi-os/#project-structure","text":"Makefile We will use the make utility to build the kernel. make 's behavior is configured by a Makefile, which contains instructions on how to compile and link the source code. src This folder contains all of the source code. include All of the header files are placed here. Note: Of all the subsequent experiments in p1, the source code has the same structure.","title":"Project structure"},{"location":"lesson01/rpi-os/#makefile-walkthrough","text":"If you are not familiar with Makefiles, read this article. The complete Makefile: ARMGNU ?= aarch64-linux-gnu COPS = -Wall -nostdlib -nostartfiles -ffreestanding -Iinclude -mgeneral-regs-only -O0 -g ASMOPS = -Iinclude -g BUILD_DIR = build SRC_DIR = src all : kernel8.img clean : rm -rf $(BUILD_DIR) *.img $(BUILD_DIR)/%_c.o: $(SRC_DIR)/%.c mkdir -p $(@D) $(ARMGNU)-gcc $(COPS) -MMD -c $< -o $@ $(BUILD_DIR)/%_s.o: $(SRC_DIR)/%.S $(ARMGNU)-gcc $(ASMOPS) -MMD -c $< -o $@ C_FILES = $(wildcard $(SRC_DIR)/*.c) ASM_FILES = $(wildcard $(SRC_DIR)/*.S) OBJ_FILES = $(C_FILES:$(SRC_DIR)/%.c=$(BUILD_DIR)/%_c.o) OBJ_FILES += $(ASM_FILES:$(SRC_DIR)/%.S=$(BUILD_DIR)/%_s.o) DEP_FILES = $(OBJ_FILES:%.o=%.d) -include $(DEP_FILES) kernel8.img: $(SRC_DIR)/linker.ld $(OBJ_FILES) $(ARMGNU)-ld -T $(SRC_DIR)/linker.ld -o $(BUILD_DIR)/kernel8.elf $(OBJ_FILES) $(ARMGNU)-objcopy $(BUILD_DIR)/kernel8.elf -O binary kernel8.img Let's inspect this file in detail: ARMGNU ?= aarch64-linux-gnu The Makefile starts with a variable definition. ARMGNU is a cross-compiler prefix. We need to use a cross-compiler because we are compiling the source code for the arm64 architecture on an x86 machine. So instead of gcc , we will use aarch64-linux-gnu-gcc . COPS = -Wall -nostdlib -nostartfiles -ffreestanding -Iinclude -mgeneral-regs-only ASMOPS = -Iinclude COPS and ASMOPS are options that we pass to the compiler when compiling C and assembler code, respectively. These options require a short explanation: -Wall Show all warnings. A good practice. -nostdlib Don't use the C standard library. Most of the calls in the C standard library eventually interact with the operating system. We are writing a bare-metal program, and we don't have any underlying operating system, so the C standard library is not going to work for us anyway. -nostartfiles Don't use standard startup files. Startup files are responsible for setting an initial stack pointer, initializing static data, and jumping to the main entry point. We are going to do all of this by ourselves. -ffreestanding A freestanding environment is an environment in which the standard library may not exist, and program startup may not necessarily be at main. The option -ffreestanding directs the compiler to not assume that standard functions have their usual definition. -Iinclude Search for header files in the include folder. -mgeneral-regs-only . Use only general-purpose registers. ARM processors also have NEON registers. We don't want the compiler to use them because they add additional complexity (since, for example, we will need to store the registers during a context switch). -g Include debugging info in the resultant ELF binary. -O0 Turn off any compiler optimization. For ease of debugging. BUILD_DIR = build SRC_DIR = src SRC_DIR and BUILD_DIR are directories that contain source code and compiled object files, respectively. all : kernel8.img clean : rm -rf $(BUILD_DIR) *.img","title":"Makefile walkthrough"},{"location":"lesson01/rpi-os/#build-targets-rules","text":"The first two targets are pretty simple: the all target is the default one, and it is executed whenever you type make without any arguments ( make always uses the first target as the default). This target just redirects all work to a different target, kernel8.img . The name \"kernel8.img\" is mandated by the Rpi3 firmware. The trailing 8 denotes ARMv8 which is a 64-bit architecture. This filename tells the firmware to boot the processor into 64-bit mode. The clean target is responsible for deleting all compilation artifacts and the compiled kernel image. $(BUILD_DIR)/%_c.o: $(SRC_DIR)/%.c mkdir -p $(@D) $(ARMGNU)-gcc $(COPS) -MMD -c $< -o $@ $(BUILD_DIR)/%_s.o: $(SRC_DIR)/%.S $(ARMGNU)-gcc $(ASMOPS) -MMD -c $< -o $@ The next two targets are responsible for compiling C and assembler files. If, for example, in the src directory we have foo.c and foo.S files, they will be compiled into build/foo_c.o and build/foo_s.o , respectively. $< and $@ are substituted at runtime with the input and output filenames ( foo.c and foo_c.o ). Before compiling C files, we also create a build directory in case it doesn't exist yet. C_FILES = $(wildcard $(SRC_DIR)/*.c) ASM_FILES = $(wildcard $(SRC_DIR)/*.S) OBJ_FILES = $(C_FILES:$(SRC_DIR)/%.c=$(BUILD_DIR)/%_c.o) OBJ_FILES += $(ASM_FILES:$(SRC_DIR)/%.S=$(BUILD_DIR)/%_s.o) Here we are building an array of all object files ( OBJ_FILES ) created from the concatenation of both C and assembler source files. DEP_FILES = $(OBJ_FILES:%.o=%.d) -include $(DEP_FILES) The next two lines are a little bit tricky. If you take a look at how we defined our compilation targets for both C and assembler source files, you will notice that we used the -MMD parameter. This parameter instructs the gcc compiler to create a dependency file for each generated object file. A dependency file defines all of the dependencies for a particular source file. These dependencies usually contain a list of all included headers. We need to include all of the generated dependency files so that make knows what exactly to recompile in case a header changes.","title":"Build targets &amp; rules"},{"location":"lesson01/rpi-os/#bake-the-kernel-binaries","text":"$(ARMGNU)-ld -T $(SRC_DIR)/linker.ld -o kernel8.elf $(OBJ_FILES) We use the OBJ_FILES array to build the kernel8.elf file. We use the linker script src/linker.ld to define the basic layout of the resulting executable image (we will discuss the linker script in the next section). $(ARMGNU)-objcopy kernel8.elf -O binary kernel8.img kernel8.elf & kernel8.img build/kernel8.elf (\"kernel binary\"): Our build outcome as an ELF file. It contains all code, data, and debugging info. Often, to execute an ELF program in user space, there should be a loader to parse ELF, load code & data to designated memory locations, etc. For our kernel experiment, we do NOT have such a loader for the kernel itself. kernel8.img (\"kernel image\"): The raw instructions & data as extracted from kernel8.elf. The raw image is to be loaded to memory. Since it's a memory dump (see below), the load is as simple as byte-by-byte copy. The kernel image is produced by objcopy . Its manual says: \" objcopy can be used to generate a raw binary file by using an output target of \u2018binary\u2019 (e.g., use -O binary). When objcopy generates a raw binary file, it will essentially produce a memory dump of the contents of the input object file. All symbols and relocation information will be discarded. The memory dump will start at the load address of the lowest section copied into the output file.\" Q: can you use readelf to examine kernel8.elf, and explain your observation?","title":"Bake the kernel binaries"},{"location":"lesson01/rpi-os/#the-linker-script","text":"A linker script describes how the sections in the input object files ( _c.o and _s.o ) should be mapped into the output file ( .elf ); it also controls the addresses of all program symbols (e.g. functions and variables). More information can be found here . Now let's take a look at the linker script: SECTIONS { .text.boot : { *(.text.boot) } .text : { *(.text) } .rodata : { *(.rodata) } .data : { *(.data) } . = ALIGN(0x8); bss_begin = .; .bss : { *(.bss*) } bss_end = .; } After startup, the Rpi3 GPU loads kernel8.img into memory 0x0 and starts execution from the beginning of the file. That's why the .text.boot section must come first; we are going to put the kernel startup code inside this section. QEMU behaves differently: it loads the kernel image at 0x80000. Q: How to tweak the linker script to update the start address? The .text , .rodata , and .data sections contain kernel instructions, read-only data, and global data with init values. The .bss section contains data that should be initialized to 0. By putting such data in a separate section, the compiler can save some space in the ELF binary\u2013\u2013only the section size is stored in the ELF header, but the section content is omitted. After booting up, our kernel initializes the .bss section to 0; that's why we need to record the start and end of the section (hence the bss_begin and bss_end symbols) and align the section so that it starts at an address that is a multiple of 8. This eases kernel programming because the str instruction can be used only with 8-byte-aligned addresses.","title":"The linker script"},{"location":"lesson01/rpi-os/#kernel-startup","text":"","title":"Kernel startup"},{"location":"lesson01/rpi-os/#booting-the-kernel","text":"boot.S contains the kernel startup code: #include \"mm.h\" .section \".text.boot\" .globl _start _start: mrs x0, mpidr_el1 and x0, x0,#0xFF // Check processor id cbz x0, master // Hang for all non-primary CPU b proc_hang proc_hang: b proc_hang master: adr x0, bss_begin adr x1, bss_end sub x1, x1, x0 bl memzero mov sp, #LOW_MEMORY bl kernel_main Let's review this file in detail: .section \".text.boot\" First, we specify that everything defined in boot.S should go in the .text.boot section. Previously, we saw that this section is placed at the beginning of the kernel image by the linker script. So when the kernel is started, execution begins at the start function: .globl _start _start: mrs x0, mpidr_el1 and x0, x0,#0xFF // Check processor id cbz x0, master // Hang for all non-primary CPU b proc_hang Rpi3 has 4 cores, and after the device is powered on, each core begins to execute the same code. Our kernel only works with the first one and put all of the other cores in an endless loop. This is exactly what the _start function is responsible for. It gets the processor ID from the mpidr_el1 system register. Q: It may make more sense to put core 1-3 in deep sleep using wfi . How?","title":"Booting the kernel"},{"location":"lesson01/rpi-os/#kernel-memory-layout","text":"If the current processor ID is 0, then execution branches to the master function: master: adr x0, bss_begin adr x1, bss_end sub x1, x1, x0 bl memzero Here, we clean the .bss section by calling memzero . We will define this function later. In ARMv8 architecture, by convention, the first seven arguments are passed to the called function via registers x0\u2013x6 (cf: our cheat sheet). The memzero function accepts only two arguments: the start address ( bss_begin ) and the size of the section needed to be cleaned ( bss_end - bss_begin ). mov sp, #LOW_MEMORY bl kernel_main After cleaning the .bss section, the kernel initializes the stack pointer and passes execution to the kernel_main function. The Rpi3 loads the kernel at address 0 (QEMU loads at 0x80000); that's why the initial stack pointer can be set to any location high enough so that stack will not override the kernel image when it grows sufficiently large. LOW_MEMORY is defined in mm.h and is equal to 4MB. As our kernel's stack won't grow very large and the image itself is tiny, 4MB is more than enough for us. Aside: Some ARM64 instructions used For those of you who are not familiar with ARM assembler syntax, let me quickly summarize the instructions that we have used: mrs Load value from a system register to one of the general purpose registers (x0\u2013x30) and Perform the logical AND operation. We use this command to strip the last byte from the value we obtain from the mpidr_el1 register. cbz Compare the result of the previously executed operation to 0 and jump (or branch in ARM terminology) to the provided label if the comparison yields true. b Perform an unconditional branch to some label. adr Load a label's relative address into the target register. In this case, we want pointers to the start and end of the .bss region. sub Subtract values from two registers. bl \"Branch with a link\": perform an unconditional branch and store the return address in x30 (the link register). When the subroutine is finished, use the ret instruction to jump back to the return address. mov Move a value between registers or from a constant to a register. Our cheat sheet summarizes common ARM64 instructions. For official documentation, here is the ARMv8-A developer's guide. It's a good resource if the ARM ISA is unfamiliar to you. This page specifically outlines the register usage convention in the ABI.","title":"Kernel memory layout"},{"location":"lesson01/rpi-os/#the-kernel_main-function","text":"We have seen that the boot code eventually passes control to the kernel_main function. Let's take a look at it: #include \"mini_uart.h\" void kernel_main(void) { uart_init(); uart_send_string(\"Hello, world!\\r\\n\"); while (1) { uart_send(uart_recv()); } } This function is one of the simplest in the kernel. It works with the Mini UART device to print to screen and read user input. The kernel just prints Hello, world! and then enters an infinite loop that reads characters from the user and sends them back to the screen.","title":"The kernel_main function"},{"location":"lesson01/rpi-os/#a-bit-about-the-rpi3-hardware","text":"The Rpi3 board is based on the BCM2837 SoC by Broadcom. The SoC manual is here . The SoC is not friendly for OS hackers: Broadcom poorly documents it and the hardware has many quirks. Despite so, the community figured out most of the SoC details over years because Rpi3's popularity. It's not our goal to dive in the SoC. Rather, our philosophy is to deal BCM2837-specific details as few as possible -- just enough to get our kernel working. We will spend more efforts on explaining generic hardware such as ARM64 cores, generic timers, irq controllers, etc. Rpi4 seems more friendly to kernel hackers.","title":"A bit about the Rpi3 hardware"},{"location":"lesson01/rpi-os/#memory-mapped-io","text":"On ARM-based SoCs, access to all devices is performed via memory-mapped registers. The Rpi3 SoC reserves physical memory address 0x3F000000 for IO devices. To configure a particular device, software reads/writes device registers. A device register is just a 32-bit region of memory. The meaning of each bit in each IO register is described in the SoC manual. The term \"device\" is heavily overloaded in many tech docs. Sometimes it means a board, e.g. \"an Rpi3 device\"; sometimes it means an IO peripheral, e.g. \"UART device\". We will be explicit.","title":"Memory-mapped IO"},{"location":"lesson01/rpi-os/#uart","text":"UART is a simple character device allowing software to send out text characters to a different machine. If you do not care about performance, UART requires very minimum software code. Therefore, it is often the first few IO devices to bring up when we build system software for a new machine. Only with UART meaning debugging is possible. (JTAG is another option which however requires more complex setup). In the simplest form, software writes ascii values to UART registers. The UART device converts written values to a sequence of high and low voltages on wire. This sequence is transmitted to your via the TTL-to-serial cable and is interpreted by your terminal emulator (e.g. PuTTY on Windows). Rpi3 has the two UART devices. Oddly enough, they are different. Name Type Comments UART0 PL011 Secondary, intended as Bluetooth connector UART1 mini UART Primary, intended as debug console UART1/Mini UART: easier to program; limited performance/functionalities. That's fine for our goal. For specification of the Mini UART registers: see page 8 of the SoC manual. UART0/PL011: richer functions; higher speed. Yet one needs to configure the board clock by talking to the GPU firmware. We won't do that. see Example code if you are interested. The above information is enough. More about Raspberry Pi UARTs: see the official web page .","title":"UART"},{"location":"lesson01/rpi-os/#gpio","text":"Another IO device is GPIO General-purpose input/output . GPIO provides a bunch of registers. Each bit in such a register corresponds to a pin on the Rpi3 board. By writing 1 or 0 to register bits, software can control the output voltage on the pins, e.g. for turning on/off LEDs connected to such pins. Reading is done in a similar fashion. The picture below shows GPIO pin headers populated on Rpi3. (Note: the picture shows Rpi2, which has the same pinout as Rpi3) An SoC often has limited number of pins. Software can control the use of these pins, e.g. for GPIO or for UART. Software does so by writing to specific memory-mapped registers. The GPIO can be used to configure the behavior of different GPIO pins. For example, to be able to use the Mini UART, we need to activate pins 14 and 15 and set them up to use this device. The image below illustrates how numbers are assigned to the GPIO pins:","title":"GPIO"},{"location":"lesson01/rpi-os/#walkthrough-the-uart-code","text":"The following init code configures pin 14 & 15 as UART in/out, sets up UART clock and its modes, etc. Much of the UART init code is irrelevant to QEMU. Since QEMU \"emulates\" the UARTs, it can dump whatever our kernel writes to the emulated UART registers to stdio. Example: qemu-system-aarch64 -M raspi3 -kernel ./kernel8.img -serial null -serial stdio The first -serial means UART0 which we do not touch; the second -serial means we direct UART1 to stdio. void uart_init ( void ) { unsigned int selector; selector = get32(GPFSEL1); selector &= ~(7<<12); // clean gpio14 selector |= 2<<12; // set alt5 for gpio14 selector &= ~(7<<15); // clean gpio15 selector |= 2<<15; // set alt5 for gpio 15 put32(GPFSEL1,selector); put32(GPPUD,0); delay(150); put32(GPPUDCLK0,(1<<14)|(1<<15)); delay(150); put32(GPPUDCLK0,0); put32(AUX_ENABLES,1); //Enable mini uart (this also enables access to it registers) put32(AUX_MU_CNTL_REG,0); //Disable auto flow control and disable receiver and transmitter (for now) put32(AUX_MU_IER_REG,0); //Disable receive and transmit interrupts put32(AUX_MU_LCR_REG,3); //Enable 8 bit mode put32(AUX_MU_MCR_REG,0); //Set RTS line to be always high put32(AUX_MU_BAUD_REG,270); //Set baud rate to 115200 put32(AUX_MU_CNTL_REG,3); //Finally, enable transmitter and receiver } Here, we use the two functions put32 and get32 . Those functions are very simple -- read and write some data to and from a 32-bit register. You can take a look at how they are implemented in utils.S . uart_init is one of the most complex and important functions in this lesson, and we will continue to examine it in the next three sections.","title":"Walkthrough: the UART code"},{"location":"lesson01/rpi-os/#init-gpio-alternative-function-selection","text":"First, we need to activate the GPIO pins. Most of the pins can be used with different IO devices. So before using a particular pin, we need to select the pin's alternative function, a number from 0 to 5 that can be set for each pin and configures which IO device is virtually \"connected\" to the pin. See the list of all available GPIO alternative functions in the image below (taken from page 102 of the SoC manual) Here you can see that pins 14 and 15 have the TXD1 and RXD1 alternative functions available. This means that if we select alternative function number 5 for pins 14 and 15, they will be used as a Mini UART Transmit Data pin and Mini UART Receive Data pin, respectively. The GPFSEL1 register is used to control alternative functions for pins 10-19. The meaning of all the bits in those registers is shown in the following table (page 92 of the SoC manual): So now you know everything you need to understand the following lines of code that are used to configure GPIO pins 14 and 15 to work with the Mini UART device: unsigned int selector; selector = get32(GPFSEL1); selector &= ~(7<<12); // clean gpio14 selector |= 2<<12; // set alt5 for gpio14 selector &= ~(7<<15); // clean gpio15 selector |= 2<<15; // set alt5 for gpio 15 put32(GPFSEL1,selector); Init: GPIO pull-up/down & how we disable it When working with GPIO pins, you will often encounter terms such as pull-up/pull-down. These concepts are explained in great detail in this article. For those who are too lazy to read the whole article, I will briefly explain the pull-up/pull-down concept. If you use a particular pin as input and don't connect anything to this pin, you will not be able to identify whether the value of the pin is 1 or 0. In fact, the device will report random values. The pull-up/pull-down mechanism allows you to overcome this issue. If you set the pin to the pull-up state and nothing is connected to it, it will report 1 all the time (for the pull-down state, the value will always be 0). In our case, we need neither the pull-up nor the pull-down state, because both the 14 and 15 pins are going to be connected all the time. The pin state is preserved even after a reboot, so before using any pin, we always have to initialize its state. There are three available states: pull-up, pull-down, and neither (to remove the current pull-up or pull-down state), and we need the third one. Switching between pin states is not a very simple procedure because it requires physically toggling a switch on the electric circuit. This process involves the GPPUD and GPPUDCLK registers and is described on page 101 of the SoC manual: The GPIO Pull-up/down Clock Registers control the actuation of internal pull-downs on the respective GPIO pins. These registers must be used in conjunction with the GPPUD register to effect GPIO Pull-up/down changes. The following sequence of events is required: 1. Write to GPPUD to set the required control signal (i.e. Pull-up or Pull-Down or neither to remove the current Pull-up/down) 2. Wait 150 cycles \u2013 this provides the required set-up time for the control signal 3. Write to GPPUDCLK0/1 to clock the control signal into the GPIO pads you wish to modify \u2013 NOTE only the pads which receive a clock will be modified, all others will retain their previous state. 4. Wait 150 cycles \u2013 this provides the required hold time for the control signal 5. Write to GPPUD to remove the control signal 6. Write to GPPUDCLK0/1 to remove the clock This procedure describes how we can remove both the pull-up and pull-down states from a pin , which is what we are doing for pins 14 and 15 in the following code: put32(GPPUD,0); delay(150); put32(GPPUDCLK0,(1<<14)|(1<<15)); delay(150); put32(GPPUDCLK0,0);","title":"Init: GPIO alternative function selection"},{"location":"lesson01/rpi-os/#init-mini-uart","text":"Now our Mini UART is connected to the GPIO pins, and the pins are configured. The rest of the uart_init function is dedicated to Mini UART initialization. put32(AUX_ENABLES,1); //Enable mini uart (this also enables access to its registers) put32(AUX_MU_CNTL_REG,0); //Disable auto flow control and disable receiver and transmitter (for now) put32(AUX_MU_IER_REG,0); //Disable receive and transmit interrupts put32(AUX_MU_LCR_REG,3); //Enable 8 bit mode put32(AUX_MU_MCR_REG,0); //Set RTS line to be always high put32(AUX_MU_BAUD_REG,270); //Set baud rate to 115200 put32(AUX_MU_CNTL_REG,3); //Finally, enable transmitter and receiver Let's examine this code snippet line by line. put32(AUX_ENABLES,1); //Enable mini uart (this also enables access to its registers) This line enables the Mini UART. We must do this in the beginning, because this also enables access to all the other Mini UART registers. put32(AUX_MU_CNTL_REG,0); //Disable auto flow control and disable receiver and transmitter (for now) Here we disable the receiver and transmitter before the configuration is finished. We also permanently disable auto-flow control because it requires us to use additional GPIO pins, and the TTL-to-serial cable doesn't support it. For more information about auto-flow control, you can refer to this article. put32(AUX_MU_IER_REG,0); //Disable receive and transmit interrupts It is possible to configure the Mini UART to generate a processor interrupt each time new data is available. We want to be as simple as possible. So for now, we will just disable this feature. put32(AUX_MU_LCR_REG,3); //Enable 8 bit mode Mini UART can support either 7- or 8-bit operations. This is because an ASCII character is 7 bits for the standard set and 8 bits for the extended. We are going to use 8-bit mode. put32(AUX_MU_MCR_REG,0); //Set RTS line to be always high The RTS line is used in the flow control and we don't need it. Set it to be high all the time. put32(AUX_MU_BAUD_REG,270); //Set baud rate to 115200 The baud rate is the rate at which information is transferred in a communication channel. \u201c115200 baud\u201d means that the serial port is capable of transferring a maximum of 115200 bits per second. The baud rate of your Raspberry Pi mini UART device should be the same as the baud rate in your terminal emulator. The Mini UART calculates baud rate according to the following equation: baudrate = system_clock_freq / (8 * ( baudrate_reg + 1 )) The system_clock_freq is 250 MHz, so we can easily calculate the value of baudrate_reg as 270. put32(AUX_MU_CNTL_REG,3); //Finally, enable transmitter and receiver After this line is executed, the Mini UART is ready for work!","title":"Init: Mini UART"},{"location":"lesson01/rpi-os/#sending-data-over-uart","text":"After the Mini UART is ready, we can try to use it to send and receive some data. To do this, we can use the following two functions: void uart_send ( char c ) { while(1) { if(get32(AUX_MU_LSR_REG)&0x20) break; } put32(AUX_MU_IO_REG,c); } char uart_recv ( void ) { while(1) { if(get32(AUX_MU_LSR_REG)&0x01) break; } return(get32(AUX_MU_IO_REG)&0xFF); } Both of the functions start with an infinite loop, the purpose of which is to verify whether the device is ready to transmit or receive data. We are using the AUX_MU_LSR_REG register to do this. Bit zero, if set to 1, indicates that the data is ready; this means that we can read from the UART. Bit five, if set to 1, tells us that the transmitter is empty, meaning that we can write to the UART. Next, we use AUX_MU_IO_REG to either store the value of the transmitted character or read the value of the received character. We also have a very simple function that is capable of sending strings instead of characters: void uart_send_string(char* str) { for (int i = 0; str[i] != '\\0'; i ++) { uart_send((char)str[i]); } } This function just iterates over all characters in a string and sends them one by one. Low efficiency? Apparently Tx/Rx with busy wait burn lots of CPU cycles for no good. It's fine for our baremetal program -- simple & less error-prone. Production software often do interrupt-driven Rx/Tx.","title":"Sending data over UART"},{"location":"lesson01/rpi-os/#take-the-kernel-for-a-spin","text":"Run make to build the kernel.","title":"Take the kernel for a spin"},{"location":"lesson01/rpi-os/#rpi3","text":"The Raspberry Pi startup sequence is the following (simplified): The device is powered on. The GPU starts up and reads the config.txt file from the boot partition. This file contains some configuration parameters that the GPU uses to further adjust the startup sequence. kernel8.img is loaded into memory and executed. Setup To be able to run our simple OS, the config.txt file should be the following: enable_uart=1 arm_64bit=1 kernel_old=1 disable_commandline_tags=1 kernel_old=1 specifies that the kernel image should be loaded at address 0. disable_commandline_tags instructs the GPU to not pass any command line arguments to the booted image. Run Copy the generated kernel8.img file to the boot partition of your Raspberry Pi flash card and delete kernel7.img as well as any other kernel*.img files on your SD card. Make sure you left all other files in the boot partition untouched (see 43 and 158 issues for details). Modify the config.txt file as described above. Connect the USB-to-TTL serial cable as described in the Prerequisites . Power on your Raspberry Pi. Open your terminal emulator. You should be able to see the Hello, world! message there. Aside (optional): prepare the SD card from scratch (w/o Raspbian) The steps above assume that you have Raspbian installed on your SD card. It is also possible to run the RPi OS using an empty SD card. Prepare your SD card: Use an MBR partition table Format the boot partition as FAT32 The card should be formatted exactly in the same way as it is required to install Raspbian. Check HOW TO FORMAT AN SD CARD AS FAT section in the official documenation for more information. Copy the following files to the card: bootcode.bin This is the GPU bootloader, it contains the GPU code to start the GPU and load the GPU firmware. start.elf This is the GPU firmware. It reads config.txt and enables the GPU to load and run ARM specific user code from kernel8.img Copy kernel8.img and config.txt files. Connect the USB-to-TTL serial cable. Power on your Raspberry Pi. Use your terminal emulator to connect to the RPi OS. Unfortunately, all Raspberry Pi firmware files are closed-sourced and undocumented. For more information about the Raspberry Pi startup sequence, you can refer to some unofficial sources, like this StackExchange question or this Github repository.","title":"Rpi3"},{"location":"lesson01/rpi-os/#qemu","text":"Setup Follow the instructions in Prerequisites . Run $ qemu-system-aarch64 -M raspi3 -kernel ./kernel8.img -serial null -serial stdio VNC server running on 127.0.0.1:5900 Hello, world! <Ctrl-C>","title":"QEMU"},{"location":"lesson01/linux/build-system/","text":"1.3: Kernel build system After we examined Linux kernel structure, it worth spending some time investigating how we can build and run it. Linux also uses make utility to build the kernel, though Linux makefile is much more complicated. Before we will take a look at the makefile, let's learn some important concepts about Linux build system, which is called \"kbuild\". A few essential kbuild concepts Build process can be customized by using kbuild variables. Those variables are defined in Kconfig files. Here you can define the variables themselves and their default values. Variables can have different types, including string, boolean and integer. In a Kconfig file you can also define dependencies between variables (for example, you can say that if variable X is selected then variable Y will be selected implicitly). As an example, you can take a look at arm64 Kconfig file . This file defines all variables, specific for arm64 architecture. Kconfig functionality is not part of the standard make and is implemented in the Linux makefile. Variables, defined in Kconfig are exposed to the kernel source code as well as to the nested makefiles. Variable values can be set during kernel configuration step (for example, if you type make menuconfig a console GUI will be shown. It allows you to customize values for all kernel variables and stores the values in .config . Use make help command to view all possible options to configure the kernel) Linux uses recursive build. This means that each subfolder of the Linux kernel can define it's own Makefile and Kconfig . Most of the nested Makefiles are very simple and just define what object files need to be compiled. Usually, such definitions have the following format. obj-$(SOME_CONFIG_VARIABLE) += some_file.o This definition means that some_file.c will be compiled and linked to the kernel only if SOME_CONFIG_VARIABLE is set. If you want to compile and link a file unconditionally, you need to change the previous definition to look like this. obj-y += some_file.o An example of the nested Makefile can be found here . Before we move forward, you need to understand the structure of a basic make rule and be comfortable with make terminology. Common rule structure is illustrated in the following diagram. targets : prerequisites recipe \u2026 * targets are file names, separated by spaces. Targets are generated after the rule is executed. Usually, there is only one target per rule. * prerequisites are files that make trackes to see whether it needs to update the targets. * recipe is a bash script. Make calls it when some of the prerequisites have been updated. The recipe is responsible for generating the targets. * Both targets and prerequisites can include wildcards ( % ). When wildcards are used the recipe is executed for each of the mached prerequisites separately. In this case, you can use $< and $@ variables to refer to the prerequisite and the target inside the recipe. We already did it in the RPi OS makefile . For additional information about make rules, please refer to the official documentation . make is very good in detecting whether any of the prerequisites have been changed and updating only targets that need to be rebuilt. However, if a recipe is dynamically updated, make is unable to detect this change. How can this happen? Very easily. One good example is when you change some configuration variable, which results in appending an additional option to the recipe. By default, in this case, make will not recompile previously generated object files, because their prerequisites haven't been changed, only the recipe have been updated. To overcome this behavior Linux introduces if_changed function. To see how it works let's consider the following example. ``` cmd_compile = gcc $(flags) -o $@ $< %.o: %.c FORCE $(call if_changed,compile) ``` Here for each .c file we build corresponding .o file by calling if_changed function with the argument compile . if_changed then looks for cmd_compile variable (it adds cmd_ prefix to the first argument) and checks whether this variable has been updated since the last execution, or any of the prerequisites has been changed. If yes - cmd_compile command is executed and object file is regenerated. Our sample rule has 2 prerequisites: source .c file and FORCE . FORCE is a special prerequisite that forces the recipe to be called each time when make command is called. Without it, the recipe would be called only if .c file was changed. You can read more about FORCE target here . Building the kernel Now, that we learned some important concepts about the Linux build system, let's try to figure out what exactly is going on after you type make command. This process is very complicated and includes a lot of details, most of which we will skip. Our goal will be to answer 2 questions. How exactly are source files compiled into object files? How are object files linked into the OS image? We are going to tackle the second question first. Link stage As you might see from the output of make help command, the default target, which is responsible for building the kernel, is called vmlinux . vmlinux target definition can be found here and it looks like this. ``` cmd_link-vmlinux = \\ $(CONFIG_SHELL) $< $(LD) $(LDFLAGS) $(LDFLAGS_vmlinux) ; \\ $(if $(ARCH_POSTLINK), $(MAKE) -f $(ARCH_POSTLINK) $@, true) vmlinux: scripts/link-vmlinux.sh vmlinux_prereq $(vmlinux-deps) FORCE +$(call if_changed,link-vmlinux) ``` This target uses already familiar to us if_changed function. Whenever some of the prerequsities are updated cmd_link-vmlinux command is executed. This command executes scripts/link-vmlinux.sh script (Note usage of $< automatic variable in the cmd_link-vmlinux command). It also executes architecture specific postlink script , but we are not very interested in it. When scripts/link-vmlinux.sh is executed it assumes that all required object files are already built and their locations are stored in 3 variables: KBUILD_VMLINUX_INIT , KBUILD_VMLINUX_MAIN , KBUILD_VMLINUX_LIBS . link-vmlinux.sh script first creates thin archive from all available object files. thin archive is a special object that contains references to a set of object files as well as their combined symbol table. This is done inside archive_builtin function. In order to create thin archive this function uses ar utility. Generated thin archive is stored as built-in.o file and has the format that is understandable by the linker, so it can be used as any other normal object file. Next modpost_link is called. This function calls linker and generates vmlinux.o object file. We need this object file to perform Section mismatch analysis . This analysis is performed by the modpost program and is triggered at this line. Next kernel symbol table is generated. It contains information about all functions and global variables as well as their location in the vmlinux binary. The main work is done inside kallsyms function. This function first uses nm to extract all symbols from vmlinux binary. Then it uses scripts/kallsyms utility to generate a special assembler file containing all symbols in a special format, understandable by the Linux kernel. Next, this assembler file is compiled and linked together with the original binary. This process is repeated several times because after the final link addresses of some symbols can be changed. Information from the kernel symbol table is used to generate '/proc/kallsyms' file at runtime. Finally vmlinux binary is ready and System.map is build. System.map contains the same information as /proc/kallsyms but this is static file and unlike /proc/kallsyms it is not generated at runtime. System.map is mostly used to resolve addresses to symbol names during kernel oops . The same nm utility is used to build System.map . This is done here . Build stage Now let's take one step backward and examine how source code files are compiled into object files. As you might remember one of the prerequisites of the vmlinux target is $(vmlinux-deps) variable. Let me now copy a few relevant lines from the main Linux makefile to demonstrate how this variable is built. ``` init-y := init/ drivers-y := drivers/ sound/ firmware/ net-y := net/ libs-y := lib/ core-y := usr/ core-y += kernel/ certs/ mm/ fs/ ipc/ security/ crypto/ block/ init-y := $(patsubst %/, %/built-in.o, $(init-y)) core-y := $(patsubst %/, %/built-in.o, $(core-y)) drivers-y := $(patsubst %/, %/built-in.o, $(drivers-y)) net-y := $(patsubst %/, %/built-in.o, $(net-y)) export KBUILD_VMLINUX_INIT := $(head-y) $(init-y) export KBUILD_VMLINUX_MAIN := $(core-y) $(libs-y2) $(drivers-y) $(net-y) $(virt-y) export KBUILD_VMLINUX_LIBS := $(libs-y1) export KBUILD_LDS := arch/$(SRCARCH)/kernel/vmlinux.lds vmlinux-deps := $(KBUILD_LDS) $(KBUILD_VMLINUX_INIT) $(KBUILD_VMLINUX_MAIN) $(KBUILD_VMLINUX_LIBS) ``` It all starts with variables like init-y , core-y , etc., which combined contains all subfolders of the Linux kernel that contains buildable source code. Then built-in.o is appended to all the subfolder names, so, for example, drivers/ becomes drivers/built-in.o . vmlinux-deps then just aggregates all resulting values. This explains how vmlinux eventually becomes dependent on all built-in.o files. Next question is how all built-in.o objects are created? Once again, let me copy all relevant lines and explain how it all works. ``` $(sort $(vmlinux-deps)): $(vmlinux-dirs) ; vmlinux-dirs := $(patsubst %/,%,$(filter %/, $(init-y) $(init-m) \\ $(core-y) $(core-m) $(drivers-y) $(drivers-m) \\ $(net-y) $(net-m) $(libs-y) $(libs-m) $(virt-y))) build := -f $(srctree)/scripts/Makefile.build obj #Copied from scripts/Kbuild.include $(vmlinux-dirs): prepare scripts $(Q)$(MAKE) $(build)=$@ ``` The first line tells us that vmlinux-deps depends on vmlinux-dirs . Next, we can see that vmlinux-dirs is a variable that contains all direct root subfolders without / character at the end. And the most important line here is the recipe to build $(vmlinux-dirs) target. After substitution of all variables, this recipe will look like the following (we use drivers folder as an example, but this rule will be executed for all root subfolders) make -f scripts/Makefile.build obj=drivers This line just calls another makefile ( scripts/Makefile.build ) and passes obj variable, which contains a folder to be compiled. Next logical step is to take a look at scripts/Makefile.build . The first important thing that happens after it is executed is that all variables from Makefile or Kbuild files, defined in the current directory, are included. By current directory I mean the directory referenced by the obj variable. The inclusion is done in the following 3 lines . kbuild-dir := $(if $(filter /%,$(src)),$(src),$(srctree)/$(src)) kbuild-file := $(if $(wildcard $(kbuild-dir)/Kbuild),$(kbuild-dir)/Kbuild,$(kbuild-dir)/Makefile) include $(kbuild-file) Nested makefiles are mostly responsible for initializing variables like obj-y . As a quick reminder: obj-y variable should contain list of all source code files, located in the current directory. Another important variable that is initialized by the nested makefiles is subdir-y . This variable contains a list of all subfolders that need to be visited before the source code in the curent directory can be built. subdir-y is used to implement recursive descending into subfolders. When make is called without specifying the target (as it is in the case when scripts/Makefile.build is executed) it uses the first target. The first target for scripts/Makefile.build is called __build and it can be found here Let's take a look at it. __build: $(if $(KBUILD_BUILTIN),$(builtin-target) $(lib-target) $(extra-y)) \\ $(if $(KBUILD_MODULES),$(obj-m) $(modorder-target)) \\ $(subdir-ym) $(always) @: As you can see __build target doesn't have a receipt, but it depends on a bunch of other targets. We are only interested in $(builtin-target) - it is responsible for creating built-in.o file, and $(subdir-ym) - it is responsible for descending into nested directories. Let's take a look at subdir-ym . This variable is initialized here and it is just a concatenation of subdir-y and subdir-m variables. ( subdir-m variable is similar to subdir-y , but it defines subfolders need to be included in a separate kernel module . We skip the discussion of modules, for now, to keep focused.) subdir-ym target is defined here and should look familiar to you. $(subdir-ym): $(Q)$(MAKE) $(build)=$@ This target just triggers execution of the scripts/Makefile.build in one of the nested subfolders. Now it is time to examine the builtin-target target. Once again I am copying only relevant lines here. ``` cmd_make_builtin = rm -f $@; $(AR) rcSTP$(KBUILD_ARFLAGS) cmd_make_empty_builtin = rm -f $@; $(AR) rcSTP$(KBUILD_ARFLAGS) cmd_link_o_target = $(if $(strip $(obj-y)),\\ $(cmd_make_builtin) $@ $(filter $(obj-y), $^) \\ $(cmd_secanalysis),\\ $(cmd_make_empty_builtin) $@) $(builtin-target): $(obj-y) FORCE $(call if_changed,link_o_target) ``` This target depends on $(obj-y) target and obj-y is a list of all object files that need to be built in the current folder. After those files become ready cmd_link_o_target command is executed. In case if obj-y variable is empty cmd_make_empty_builtin is called, which just creates an empty built-in.o . Otherwise, cmd_make_builtin command is executed; it uses familiar to us ar tool to create built-in.o thin archive. Finally we got to the point where we need to compile something. You remember that our last unexplored dependency is $(obj-y) and obj-y is just a list of object files. The target that compiles all object files from corresponding .c files is defined here . Let's examine all lines, needed to understand this target. ``` cmd_cc_o_c = $(CC) $(c_flags) -c -o $@ $< define rule_cc_o_c $(call echo-cmd,checksrc) $(cmd_checksrc) \\ $(call cmd_and_fixdep,cc_o_c) \\ $(cmd_modversions_c) \\ $(call echo-cmd,objtool) $(cmd_objtool) \\ $(call echo-cmd,record_mcount) $(cmd_record_mcount) endef $(obj)/%.o: $(src)/%.c $(recordmcount_source) $(objtool_dep) FORCE $(call cmd,force_checksrc) $(call if_changed_rule,cc_o_c) ``` Inside it's recipe this target calls rule_cc_o_c . This rule is responsible for a lot of things, like checking the source code for some common errors ( cmd_checksrc ), enabling versioning for exported module symbols ( cmd_modversions_c ), using objtool to validate some aspects of generated object files and constructing a list of calls to mcount function so that ftrace can find them quickly. But most importantly it calls cmd_cc_o_c command that actually compiles all .c files to object files. Conclusion Wow, it was a long journey inside kernel build system internals! Still, we skipped a lot of details and, for those who want to learn more about the subject, I can recommend to read the following document and continue reading Makefiles source code. Let me now emphasize the important points, that you should take as a take-home message from this chapter. How .c files are compiled into object files. How object files are combined into built-in.o files. How recursive build pick up all child built-in.o files and combines them into a single one. How vmlinux is linked from all top-level built-in.o files. My main goal was that after reading this chapter you will gain a general understanding of all above points. Previous Page 1.2 Kernel Initialization: Linux project structure Next Page 1.4 Kernel Initialization: Linux startup sequence","title":"Build system"},{"location":"lesson01/linux/build-system/#13-kernel-build-system","text":"After we examined Linux kernel structure, it worth spending some time investigating how we can build and run it. Linux also uses make utility to build the kernel, though Linux makefile is much more complicated. Before we will take a look at the makefile, let's learn some important concepts about Linux build system, which is called \"kbuild\".","title":"1.3: Kernel build system"},{"location":"lesson01/linux/build-system/#a-few-essential-kbuild-concepts","text":"Build process can be customized by using kbuild variables. Those variables are defined in Kconfig files. Here you can define the variables themselves and their default values. Variables can have different types, including string, boolean and integer. In a Kconfig file you can also define dependencies between variables (for example, you can say that if variable X is selected then variable Y will be selected implicitly). As an example, you can take a look at arm64 Kconfig file . This file defines all variables, specific for arm64 architecture. Kconfig functionality is not part of the standard make and is implemented in the Linux makefile. Variables, defined in Kconfig are exposed to the kernel source code as well as to the nested makefiles. Variable values can be set during kernel configuration step (for example, if you type make menuconfig a console GUI will be shown. It allows you to customize values for all kernel variables and stores the values in .config . Use make help command to view all possible options to configure the kernel) Linux uses recursive build. This means that each subfolder of the Linux kernel can define it's own Makefile and Kconfig . Most of the nested Makefiles are very simple and just define what object files need to be compiled. Usually, such definitions have the following format. obj-$(SOME_CONFIG_VARIABLE) += some_file.o This definition means that some_file.c will be compiled and linked to the kernel only if SOME_CONFIG_VARIABLE is set. If you want to compile and link a file unconditionally, you need to change the previous definition to look like this. obj-y += some_file.o An example of the nested Makefile can be found here . Before we move forward, you need to understand the structure of a basic make rule and be comfortable with make terminology. Common rule structure is illustrated in the following diagram. targets : prerequisites recipe \u2026 * targets are file names, separated by spaces. Targets are generated after the rule is executed. Usually, there is only one target per rule. * prerequisites are files that make trackes to see whether it needs to update the targets. * recipe is a bash script. Make calls it when some of the prerequisites have been updated. The recipe is responsible for generating the targets. * Both targets and prerequisites can include wildcards ( % ). When wildcards are used the recipe is executed for each of the mached prerequisites separately. In this case, you can use $< and $@ variables to refer to the prerequisite and the target inside the recipe. We already did it in the RPi OS makefile . For additional information about make rules, please refer to the official documentation . make is very good in detecting whether any of the prerequisites have been changed and updating only targets that need to be rebuilt. However, if a recipe is dynamically updated, make is unable to detect this change. How can this happen? Very easily. One good example is when you change some configuration variable, which results in appending an additional option to the recipe. By default, in this case, make will not recompile previously generated object files, because their prerequisites haven't been changed, only the recipe have been updated. To overcome this behavior Linux introduces if_changed function. To see how it works let's consider the following example. ``` cmd_compile = gcc $(flags) -o $@ $< %.o: %.c FORCE $(call if_changed,compile) ``` Here for each .c file we build corresponding .o file by calling if_changed function with the argument compile . if_changed then looks for cmd_compile variable (it adds cmd_ prefix to the first argument) and checks whether this variable has been updated since the last execution, or any of the prerequisites has been changed. If yes - cmd_compile command is executed and object file is regenerated. Our sample rule has 2 prerequisites: source .c file and FORCE . FORCE is a special prerequisite that forces the recipe to be called each time when make command is called. Without it, the recipe would be called only if .c file was changed. You can read more about FORCE target here .","title":"A few essential kbuild concepts"},{"location":"lesson01/linux/build-system/#building-the-kernel","text":"Now, that we learned some important concepts about the Linux build system, let's try to figure out what exactly is going on after you type make command. This process is very complicated and includes a lot of details, most of which we will skip. Our goal will be to answer 2 questions. How exactly are source files compiled into object files? How are object files linked into the OS image? We are going to tackle the second question first.","title":"Building the kernel"},{"location":"lesson01/linux/build-system/#link-stage","text":"As you might see from the output of make help command, the default target, which is responsible for building the kernel, is called vmlinux . vmlinux target definition can be found here and it looks like this. ``` cmd_link-vmlinux = \\ $(CONFIG_SHELL) $< $(LD) $(LDFLAGS) $(LDFLAGS_vmlinux) ; \\ $(if $(ARCH_POSTLINK), $(MAKE) -f $(ARCH_POSTLINK) $@, true) vmlinux: scripts/link-vmlinux.sh vmlinux_prereq $(vmlinux-deps) FORCE +$(call if_changed,link-vmlinux) ``` This target uses already familiar to us if_changed function. Whenever some of the prerequsities are updated cmd_link-vmlinux command is executed. This command executes scripts/link-vmlinux.sh script (Note usage of $< automatic variable in the cmd_link-vmlinux command). It also executes architecture specific postlink script , but we are not very interested in it. When scripts/link-vmlinux.sh is executed it assumes that all required object files are already built and their locations are stored in 3 variables: KBUILD_VMLINUX_INIT , KBUILD_VMLINUX_MAIN , KBUILD_VMLINUX_LIBS . link-vmlinux.sh script first creates thin archive from all available object files. thin archive is a special object that contains references to a set of object files as well as their combined symbol table. This is done inside archive_builtin function. In order to create thin archive this function uses ar utility. Generated thin archive is stored as built-in.o file and has the format that is understandable by the linker, so it can be used as any other normal object file. Next modpost_link is called. This function calls linker and generates vmlinux.o object file. We need this object file to perform Section mismatch analysis . This analysis is performed by the modpost program and is triggered at this line. Next kernel symbol table is generated. It contains information about all functions and global variables as well as their location in the vmlinux binary. The main work is done inside kallsyms function. This function first uses nm to extract all symbols from vmlinux binary. Then it uses scripts/kallsyms utility to generate a special assembler file containing all symbols in a special format, understandable by the Linux kernel. Next, this assembler file is compiled and linked together with the original binary. This process is repeated several times because after the final link addresses of some symbols can be changed. Information from the kernel symbol table is used to generate '/proc/kallsyms' file at runtime. Finally vmlinux binary is ready and System.map is build. System.map contains the same information as /proc/kallsyms but this is static file and unlike /proc/kallsyms it is not generated at runtime. System.map is mostly used to resolve addresses to symbol names during kernel oops . The same nm utility is used to build System.map . This is done here .","title":"Link stage"},{"location":"lesson01/linux/build-system/#build-stage","text":"Now let's take one step backward and examine how source code files are compiled into object files. As you might remember one of the prerequisites of the vmlinux target is $(vmlinux-deps) variable. Let me now copy a few relevant lines from the main Linux makefile to demonstrate how this variable is built. ``` init-y := init/ drivers-y := drivers/ sound/ firmware/ net-y := net/ libs-y := lib/ core-y := usr/ core-y += kernel/ certs/ mm/ fs/ ipc/ security/ crypto/ block/ init-y := $(patsubst %/, %/built-in.o, $(init-y)) core-y := $(patsubst %/, %/built-in.o, $(core-y)) drivers-y := $(patsubst %/, %/built-in.o, $(drivers-y)) net-y := $(patsubst %/, %/built-in.o, $(net-y)) export KBUILD_VMLINUX_INIT := $(head-y) $(init-y) export KBUILD_VMLINUX_MAIN := $(core-y) $(libs-y2) $(drivers-y) $(net-y) $(virt-y) export KBUILD_VMLINUX_LIBS := $(libs-y1) export KBUILD_LDS := arch/$(SRCARCH)/kernel/vmlinux.lds vmlinux-deps := $(KBUILD_LDS) $(KBUILD_VMLINUX_INIT) $(KBUILD_VMLINUX_MAIN) $(KBUILD_VMLINUX_LIBS) ``` It all starts with variables like init-y , core-y , etc., which combined contains all subfolders of the Linux kernel that contains buildable source code. Then built-in.o is appended to all the subfolder names, so, for example, drivers/ becomes drivers/built-in.o . vmlinux-deps then just aggregates all resulting values. This explains how vmlinux eventually becomes dependent on all built-in.o files. Next question is how all built-in.o objects are created? Once again, let me copy all relevant lines and explain how it all works. ``` $(sort $(vmlinux-deps)): $(vmlinux-dirs) ; vmlinux-dirs := $(patsubst %/,%,$(filter %/, $(init-y) $(init-m) \\ $(core-y) $(core-m) $(drivers-y) $(drivers-m) \\ $(net-y) $(net-m) $(libs-y) $(libs-m) $(virt-y))) build := -f $(srctree)/scripts/Makefile.build obj #Copied from scripts/Kbuild.include $(vmlinux-dirs): prepare scripts $(Q)$(MAKE) $(build)=$@ ``` The first line tells us that vmlinux-deps depends on vmlinux-dirs . Next, we can see that vmlinux-dirs is a variable that contains all direct root subfolders without / character at the end. And the most important line here is the recipe to build $(vmlinux-dirs) target. After substitution of all variables, this recipe will look like the following (we use drivers folder as an example, but this rule will be executed for all root subfolders) make -f scripts/Makefile.build obj=drivers This line just calls another makefile ( scripts/Makefile.build ) and passes obj variable, which contains a folder to be compiled. Next logical step is to take a look at scripts/Makefile.build . The first important thing that happens after it is executed is that all variables from Makefile or Kbuild files, defined in the current directory, are included. By current directory I mean the directory referenced by the obj variable. The inclusion is done in the following 3 lines . kbuild-dir := $(if $(filter /%,$(src)),$(src),$(srctree)/$(src)) kbuild-file := $(if $(wildcard $(kbuild-dir)/Kbuild),$(kbuild-dir)/Kbuild,$(kbuild-dir)/Makefile) include $(kbuild-file) Nested makefiles are mostly responsible for initializing variables like obj-y . As a quick reminder: obj-y variable should contain list of all source code files, located in the current directory. Another important variable that is initialized by the nested makefiles is subdir-y . This variable contains a list of all subfolders that need to be visited before the source code in the curent directory can be built. subdir-y is used to implement recursive descending into subfolders. When make is called without specifying the target (as it is in the case when scripts/Makefile.build is executed) it uses the first target. The first target for scripts/Makefile.build is called __build and it can be found here Let's take a look at it. __build: $(if $(KBUILD_BUILTIN),$(builtin-target) $(lib-target) $(extra-y)) \\ $(if $(KBUILD_MODULES),$(obj-m) $(modorder-target)) \\ $(subdir-ym) $(always) @: As you can see __build target doesn't have a receipt, but it depends on a bunch of other targets. We are only interested in $(builtin-target) - it is responsible for creating built-in.o file, and $(subdir-ym) - it is responsible for descending into nested directories. Let's take a look at subdir-ym . This variable is initialized here and it is just a concatenation of subdir-y and subdir-m variables. ( subdir-m variable is similar to subdir-y , but it defines subfolders need to be included in a separate kernel module . We skip the discussion of modules, for now, to keep focused.) subdir-ym target is defined here and should look familiar to you. $(subdir-ym): $(Q)$(MAKE) $(build)=$@ This target just triggers execution of the scripts/Makefile.build in one of the nested subfolders. Now it is time to examine the builtin-target target. Once again I am copying only relevant lines here. ``` cmd_make_builtin = rm -f $@; $(AR) rcSTP$(KBUILD_ARFLAGS) cmd_make_empty_builtin = rm -f $@; $(AR) rcSTP$(KBUILD_ARFLAGS) cmd_link_o_target = $(if $(strip $(obj-y)),\\ $(cmd_make_builtin) $@ $(filter $(obj-y), $^) \\ $(cmd_secanalysis),\\ $(cmd_make_empty_builtin) $@) $(builtin-target): $(obj-y) FORCE $(call if_changed,link_o_target) ``` This target depends on $(obj-y) target and obj-y is a list of all object files that need to be built in the current folder. After those files become ready cmd_link_o_target command is executed. In case if obj-y variable is empty cmd_make_empty_builtin is called, which just creates an empty built-in.o . Otherwise, cmd_make_builtin command is executed; it uses familiar to us ar tool to create built-in.o thin archive. Finally we got to the point where we need to compile something. You remember that our last unexplored dependency is $(obj-y) and obj-y is just a list of object files. The target that compiles all object files from corresponding .c files is defined here . Let's examine all lines, needed to understand this target. ``` cmd_cc_o_c = $(CC) $(c_flags) -c -o $@ $< define rule_cc_o_c $(call echo-cmd,checksrc) $(cmd_checksrc) \\ $(call cmd_and_fixdep,cc_o_c) \\ $(cmd_modversions_c) \\ $(call echo-cmd,objtool) $(cmd_objtool) \\ $(call echo-cmd,record_mcount) $(cmd_record_mcount) endef $(obj)/%.o: $(src)/%.c $(recordmcount_source) $(objtool_dep) FORCE $(call cmd,force_checksrc) $(call if_changed_rule,cc_o_c) ``` Inside it's recipe this target calls rule_cc_o_c . This rule is responsible for a lot of things, like checking the source code for some common errors ( cmd_checksrc ), enabling versioning for exported module symbols ( cmd_modversions_c ), using objtool to validate some aspects of generated object files and constructing a list of calls to mcount function so that ftrace can find them quickly. But most importantly it calls cmd_cc_o_c command that actually compiles all .c files to object files.","title":"Build stage"},{"location":"lesson01/linux/build-system/#conclusion","text":"Wow, it was a long journey inside kernel build system internals! Still, we skipped a lot of details and, for those who want to learn more about the subject, I can recommend to read the following document and continue reading Makefiles source code. Let me now emphasize the important points, that you should take as a take-home message from this chapter. How .c files are compiled into object files. How object files are combined into built-in.o files. How recursive build pick up all child built-in.o files and combines them into a single one. How vmlinux is linked from all top-level built-in.o files. My main goal was that after reading this chapter you will gain a general understanding of all above points.","title":"Conclusion"},{"location":"lesson01/linux/build-system/#previous-page","text":"1.2 Kernel Initialization: Linux project structure","title":"Previous Page"},{"location":"lesson01/linux/build-system/#next-page","text":"1.4 Kernel Initialization: Linux startup sequence","title":"Next Page"},{"location":"lesson01/linux/kernel-startup/","text":"1.4: Linux startup sequence Searching for the entry point After taking a look at the Linux project structure and examining how it can be built, next logical step is to find the program entry point. This step might be trivial for a lot of programs, but not for the Linux kernel. The first thing we are going to do is to take a look at arm64 linker script . We have already seen how the linker script is used in the main makefile . From this line, we can easily infer, where the linker script for a particular architecture can be found. It should be mentioned that the file we are going to examine is not an actual linker script - it is a template, from which the actual linker script is built by substituting some macros with their actual values. But precisely because this file consists mostly of macros it becomes much easier to read and to port between different architectures. The first section that we can find in the linker script is called .head.text . This is very important for us because the entry point should be defined in this section. If you think a little about it, it makes a perfect sense: after the kernel is loaded, the content of the binary image is copied into some memory area and execution is started from the beginning of that area. This means that just by searching who is using .head.text section we should be able to find the entry point. And indeed, arm64 architecture has a single file that uses __HEAD macro, which is expanded to .section \".head.text\",\"ax\" - this file is head.S . The first executable line, that we can find in head.S file is this one . Here we use arm assembler b of branch instruction to jump to the stext function. And this is the first function that is executed after you boot the kernel. Next logical step is to explore what is going on inside the stext function - but we are not ready yet. First, we have to implement similar functionality in the RPi OS, and that is something we will cover in the next few lessons. What we are going to do right now is to examine a few critical concepts, related to kernel boot. Linux bootloader and boot protocol When linux kernel boots it assumes that the machine hardware is prepared in some \"known state\". The set of rules that defines this state is called \"boot protocol\", and for arm64 architecture it is documented here . Among other things, it defines, for example, that the execution must start only on primary CPU, Memory Mapping Unit must be turned off and all interrupts must be disabled. Ok, but who is responsible for bringing a machine into that known state? Usually, there is a special program that runs before the kernel and performs all initializations. This program is called bootloader . Bootloader code may be very machine specific, and this is the case, with Raspberry PI. Raspberry PI has a bootloader that is is built into the board. We can only use config.txt file to customize its behavior. UEFI boot However, there is one boot loader that can be built into the kernel image itself. This bootloader can be used only on the platforms that support Unified Extensible Firmware Interface (UEFI) . Devices that support UEFI provide a set of standardized services to the running software and those services can be used to figure out all necessary information about the machine itself and its capabilities. UEFI also requires that computer firmware should be capable of running executable files in Portable Executable (PE) format. Linux kernel UEFI bootloader makes use of this feature: it injects PE header at the beginning of the Linux kernel image so that computer firmware think that the image is a normal PE file. This is done in efi-header.S file. This file defines __EFI_PE_HEADER macro, which is used inside head.S . One important property that is defined inside __EFI_PE_HEADER is the one that tells about the location of the UEFI entry point and the entry point itself can be found in efi-entry.S . Starting from this location, you can follow the source code and examine what exactly UEFI bootloader is doing (the source code itself is more or less straightforward). But we are going to stop here because the purpose of this section is not to examine UEFI bootloader in details, but instead, give you a general idea what UEFI is and how Linux kernel uses it. Device Tree When I started to examine the startup code of the Linux kernel, I found a lot of mentions of Device Trees . It appears to be an essential concept, and I consider it necessary to discuss it. When we were working on Raspberry PI OS kernel, we used BCM2837 ARM Peripherals manual to figure out what is the exact offset at which a particular memory mapped register is located. This information obviously is different for each board, and we are lucky that we have to support only one of them. But what if we need to support hundreds of different boards? It would be a total mess if we try to hardcode information about each board in the kernel code. And even if we manage to do so, how would we figure out what board we are using right now? BCM2837 , for example, doesn't provide any means of communicating such information to the running kernel. Device tree provides us with the solution to the problem, mentioned above. It is a special format that can be used to describe computer hardware. Device tree specification can be found here . Before the kernel is executed, bootloader selects proper device tree file and passes it as an argument to the kernel. If you take a look at the files in the boot partition on a Raspberry PI SD card, you can find a lot of .dtb files here. .dtb are compiled device tree files. You can select some of them in the config.txt to enable or disable some Raspberry PI hardware. This process is described in more details in the Raspberry PI official documentation . Ok, now it is time to take a look at how an actual device tree looks like. As a quick exercise, let's try to find a device tree for Raspberry PI 3 Model B . From the documentation we can figure out that Raspberry PI 3 Model B uses a chip that is called BCM2837 . If you search for this name you can find /arch/arm64/boot/dts/broadcom/bcm2837-rpi-3-b.dts file. As you might see it just includes the same file from arm architecture. This makes a perfect sense because ARM.v8 processor supports 32-bit mode as well. Next, we can find bcm2837-rpi-3-b.dts belonging to the arm architecture. We already saw that device tree files could include on another. This is the case with the bcm2837-rpi-3-b.dts - it only contains those definitions, that are specific for BCM2837 and reuses everything else. For example, bcm2837-rpi-3-b.dts specifies that the device now have 1GB of memory . As I mentioned previously, BCM2837 and BCM2835 have an identical peripheral hardware, and, if you follow the chain of includes, you can find bcm283x.dtsi that actually defines most of this hardware. A device tree definition consists of the blocks nested one in another. At the top level we usually can find such blocks as cpus or memory The meaning of those blocks should be quite self-explanatory. Another interesting top-level element that we can find in the bcm283x.dtsi is SoC that means System on a chip It tells us that all peripheral devices are directly mapped to some memory area via memory mapped registers. soc element serves as a parent element for all peripheral devices. One of its children is gpio element. This element defines reg = <0x7e200000 0xb4> property that tells us that GPIO memory mapped registers are located in the [0x7e200000 : 0x7e2000b4] region. One of the childern of gpio element has the following definition uart1_gpio14: uart1_gpio14 { brcm,pins = <14 15>; brcm,function = <BCM2835_FSEL_ALT5>; }; This definition tells us that if alternative function 5 is selected for pins 14 and 15 those pins will be connection to uart1 device. You can easily gues that uart1 device is the Mini UART that we have used already. One important thing that you need to know about device trees is that the format is extendable. Each device can define its own properties and nested blocks. Those properties are transparently passed to the device driver, and it is driver responsibility to interpret them. But how can the kernel figure out the correspondence between a block in a device tree and the right driver? It uses compatible property to do this. For example, for uart1 device compatible property is specified like this compatible = \"brcm,bcm2835-aux-uart\"; And indeed, if you search for bcm2835-aux-uart in the Linux source code, you can find a matching driver, it is defined in 8250_bcm2835aux.c Conclusion You can think about this chapter as a preparation for reading arm64 boot code - without understanding the concepts that we've just explored you would have a hard time learning it. In the next lesson, we will go back to the stext function and examine in details how it works. Previous Page 1.3 Kernel Initialization: Kernel build system Next Page 1.5 Kernel Initialization: Exercises","title":"Kernel startup"},{"location":"lesson01/linux/kernel-startup/#14-linux-startup-sequence","text":"","title":"1.4: Linux startup sequence"},{"location":"lesson01/linux/kernel-startup/#searching-for-the-entry-point","text":"After taking a look at the Linux project structure and examining how it can be built, next logical step is to find the program entry point. This step might be trivial for a lot of programs, but not for the Linux kernel. The first thing we are going to do is to take a look at arm64 linker script . We have already seen how the linker script is used in the main makefile . From this line, we can easily infer, where the linker script for a particular architecture can be found. It should be mentioned that the file we are going to examine is not an actual linker script - it is a template, from which the actual linker script is built by substituting some macros with their actual values. But precisely because this file consists mostly of macros it becomes much easier to read and to port between different architectures. The first section that we can find in the linker script is called .head.text . This is very important for us because the entry point should be defined in this section. If you think a little about it, it makes a perfect sense: after the kernel is loaded, the content of the binary image is copied into some memory area and execution is started from the beginning of that area. This means that just by searching who is using .head.text section we should be able to find the entry point. And indeed, arm64 architecture has a single file that uses __HEAD macro, which is expanded to .section \".head.text\",\"ax\" - this file is head.S . The first executable line, that we can find in head.S file is this one . Here we use arm assembler b of branch instruction to jump to the stext function. And this is the first function that is executed after you boot the kernel. Next logical step is to explore what is going on inside the stext function - but we are not ready yet. First, we have to implement similar functionality in the RPi OS, and that is something we will cover in the next few lessons. What we are going to do right now is to examine a few critical concepts, related to kernel boot.","title":"Searching for the entry point"},{"location":"lesson01/linux/kernel-startup/#linux-bootloader-and-boot-protocol","text":"When linux kernel boots it assumes that the machine hardware is prepared in some \"known state\". The set of rules that defines this state is called \"boot protocol\", and for arm64 architecture it is documented here . Among other things, it defines, for example, that the execution must start only on primary CPU, Memory Mapping Unit must be turned off and all interrupts must be disabled. Ok, but who is responsible for bringing a machine into that known state? Usually, there is a special program that runs before the kernel and performs all initializations. This program is called bootloader . Bootloader code may be very machine specific, and this is the case, with Raspberry PI. Raspberry PI has a bootloader that is is built into the board. We can only use config.txt file to customize its behavior.","title":"Linux bootloader and boot protocol"},{"location":"lesson01/linux/kernel-startup/#uefi-boot","text":"However, there is one boot loader that can be built into the kernel image itself. This bootloader can be used only on the platforms that support Unified Extensible Firmware Interface (UEFI) . Devices that support UEFI provide a set of standardized services to the running software and those services can be used to figure out all necessary information about the machine itself and its capabilities. UEFI also requires that computer firmware should be capable of running executable files in Portable Executable (PE) format. Linux kernel UEFI bootloader makes use of this feature: it injects PE header at the beginning of the Linux kernel image so that computer firmware think that the image is a normal PE file. This is done in efi-header.S file. This file defines __EFI_PE_HEADER macro, which is used inside head.S . One important property that is defined inside __EFI_PE_HEADER is the one that tells about the location of the UEFI entry point and the entry point itself can be found in efi-entry.S . Starting from this location, you can follow the source code and examine what exactly UEFI bootloader is doing (the source code itself is more or less straightforward). But we are going to stop here because the purpose of this section is not to examine UEFI bootloader in details, but instead, give you a general idea what UEFI is and how Linux kernel uses it.","title":"UEFI boot"},{"location":"lesson01/linux/kernel-startup/#device-tree","text":"When I started to examine the startup code of the Linux kernel, I found a lot of mentions of Device Trees . It appears to be an essential concept, and I consider it necessary to discuss it. When we were working on Raspberry PI OS kernel, we used BCM2837 ARM Peripherals manual to figure out what is the exact offset at which a particular memory mapped register is located. This information obviously is different for each board, and we are lucky that we have to support only one of them. But what if we need to support hundreds of different boards? It would be a total mess if we try to hardcode information about each board in the kernel code. And even if we manage to do so, how would we figure out what board we are using right now? BCM2837 , for example, doesn't provide any means of communicating such information to the running kernel. Device tree provides us with the solution to the problem, mentioned above. It is a special format that can be used to describe computer hardware. Device tree specification can be found here . Before the kernel is executed, bootloader selects proper device tree file and passes it as an argument to the kernel. If you take a look at the files in the boot partition on a Raspberry PI SD card, you can find a lot of .dtb files here. .dtb are compiled device tree files. You can select some of them in the config.txt to enable or disable some Raspberry PI hardware. This process is described in more details in the Raspberry PI official documentation . Ok, now it is time to take a look at how an actual device tree looks like. As a quick exercise, let's try to find a device tree for Raspberry PI 3 Model B . From the documentation we can figure out that Raspberry PI 3 Model B uses a chip that is called BCM2837 . If you search for this name you can find /arch/arm64/boot/dts/broadcom/bcm2837-rpi-3-b.dts file. As you might see it just includes the same file from arm architecture. This makes a perfect sense because ARM.v8 processor supports 32-bit mode as well. Next, we can find bcm2837-rpi-3-b.dts belonging to the arm architecture. We already saw that device tree files could include on another. This is the case with the bcm2837-rpi-3-b.dts - it only contains those definitions, that are specific for BCM2837 and reuses everything else. For example, bcm2837-rpi-3-b.dts specifies that the device now have 1GB of memory . As I mentioned previously, BCM2837 and BCM2835 have an identical peripheral hardware, and, if you follow the chain of includes, you can find bcm283x.dtsi that actually defines most of this hardware. A device tree definition consists of the blocks nested one in another. At the top level we usually can find such blocks as cpus or memory The meaning of those blocks should be quite self-explanatory. Another interesting top-level element that we can find in the bcm283x.dtsi is SoC that means System on a chip It tells us that all peripheral devices are directly mapped to some memory area via memory mapped registers. soc element serves as a parent element for all peripheral devices. One of its children is gpio element. This element defines reg = <0x7e200000 0xb4> property that tells us that GPIO memory mapped registers are located in the [0x7e200000 : 0x7e2000b4] region. One of the childern of gpio element has the following definition uart1_gpio14: uart1_gpio14 { brcm,pins = <14 15>; brcm,function = <BCM2835_FSEL_ALT5>; }; This definition tells us that if alternative function 5 is selected for pins 14 and 15 those pins will be connection to uart1 device. You can easily gues that uart1 device is the Mini UART that we have used already. One important thing that you need to know about device trees is that the format is extendable. Each device can define its own properties and nested blocks. Those properties are transparently passed to the device driver, and it is driver responsibility to interpret them. But how can the kernel figure out the correspondence between a block in a device tree and the right driver? It uses compatible property to do this. For example, for uart1 device compatible property is specified like this compatible = \"brcm,bcm2835-aux-uart\"; And indeed, if you search for bcm2835-aux-uart in the Linux source code, you can find a matching driver, it is defined in 8250_bcm2835aux.c","title":"Device Tree"},{"location":"lesson01/linux/kernel-startup/#conclusion","text":"You can think about this chapter as a preparation for reading arm64 boot code - without understanding the concepts that we've just explored you would have a hard time learning it. In the next lesson, we will go back to the stext function and examine in details how it works.","title":"Conclusion"},{"location":"lesson01/linux/kernel-startup/#previous-page","text":"1.3 Kernel Initialization: Kernel build system","title":"Previous Page"},{"location":"lesson01/linux/kernel-startup/#next-page","text":"1.5 Kernel Initialization: Exercises","title":"Next Page"},{"location":"lesson01/linux/project-structure/","text":"1.2: Linux project structure This is the first time we are going to talk about Linux. The idea is first to complete some small step in writing our own kernel, and then take a look at how the same things work in Linux. So far we have done very little: we just implemented our first bare metal hello world program, Still, we will be able to find some similarities between the RPi OS and Linux. And now we are going to explore some of them. Project structure Whenever you start investigating any large software project, it worth taking a quick look at the project structure. This is very important because it allows you to understand what modules compose the project and what is the high-level architecture. Let's try to explore project structure of the Linux kernel. First of all, you need to clone the Linux repository. git clone -b v4.14 --depth 1 https://github.com/torvalds/linux.git We are using v4.14 version because this was the latest version at the time of writing. All references to the Linux source code will be made using this specific version. Next, let's take a look at the folders that we can find inside the Linux repository. We are not going to look at all of them, but only at those that I consider the most important to start with. arch This folder contains subfolders, each for a specific processor architecture. Mostly we are going to work with arm64 - this is the one that is compatible with ARM.v8 processors. init Kernel is always booted by architecture specific code. But then execution is passed to the start_kernel function that is responsible for common kernel initialization and is an architecture independent kernel starting point. start_kernel function, together with some other initialization functions, is defined in the init folder. kernel This is the core of the Linux kernel. Almost all major kernel subsystems are implemented there. mm All data structures and methods related to memory management are defined there. drivers This is the largest folder in the Linux kernel. It contains implementations of all device drivers. fs You can look here to find different filesystem implementations. This explanation is very high-level, but this is enough for now. In the next chapter, we will try to examine Linux build system in some details. Previous Page 1.1 Kernel Initialization: Introducing RPi OS, or bare metal \"Hello, world!\" Next Page 1.3 Kernel Initialization: Kernel build system","title":"Project structure"},{"location":"lesson01/linux/project-structure/#12-linux-project-structure","text":"This is the first time we are going to talk about Linux. The idea is first to complete some small step in writing our own kernel, and then take a look at how the same things work in Linux. So far we have done very little: we just implemented our first bare metal hello world program, Still, we will be able to find some similarities between the RPi OS and Linux. And now we are going to explore some of them.","title":"1.2: Linux project structure"},{"location":"lesson01/linux/project-structure/#project-structure","text":"Whenever you start investigating any large software project, it worth taking a quick look at the project structure. This is very important because it allows you to understand what modules compose the project and what is the high-level architecture. Let's try to explore project structure of the Linux kernel. First of all, you need to clone the Linux repository. git clone -b v4.14 --depth 1 https://github.com/torvalds/linux.git We are using v4.14 version because this was the latest version at the time of writing. All references to the Linux source code will be made using this specific version. Next, let's take a look at the folders that we can find inside the Linux repository. We are not going to look at all of them, but only at those that I consider the most important to start with. arch This folder contains subfolders, each for a specific processor architecture. Mostly we are going to work with arm64 - this is the one that is compatible with ARM.v8 processors. init Kernel is always booted by architecture specific code. But then execution is passed to the start_kernel function that is responsible for common kernel initialization and is an architecture independent kernel starting point. start_kernel function, together with some other initialization functions, is defined in the init folder. kernel This is the core of the Linux kernel. Almost all major kernel subsystems are implemented there. mm All data structures and methods related to memory management are defined there. drivers This is the largest folder in the Linux kernel. It contains implementations of all device drivers. fs You can look here to find different filesystem implementations. This explanation is very high-level, but this is enough for now. In the next chapter, we will try to examine Linux build system in some details.","title":"Project structure"},{"location":"lesson01/linux/project-structure/#previous-page","text":"1.1 Kernel Initialization: Introducing RPi OS, or bare metal \"Hello, world!\"","title":"Previous Page"},{"location":"lesson01/linux/project-structure/#next-page","text":"1.3 Kernel Initialization: Kernel build system","title":"Next Page"},{"location":"lesson02/exercises/","text":"Exercises Modify boot.S, so that your kernel switches to EL0 (instead of switching to EL1). After landing in EL0, can your kernel print out the current exception level? If so, how? If not, why? Describe in a short paragraph: what will happen if we execute an eret instruction at EL0? Deliverable A code tarball implementing (1) above. A docx or PDF file answering (2) (3) above.","title":"Exercises"},{"location":"lesson02/exercises/#exercises","text":"Modify boot.S, so that your kernel switches to EL0 (instead of switching to EL1). After landing in EL0, can your kernel print out the current exception level? If so, how? If not, why? Describe in a short paragraph: what will happen if we execute an eret instruction at EL0?","title":"Exercises"},{"location":"lesson02/exercises/#deliverable","text":"A code tarball implementing (1) above. A docx or PDF file answering (2) (3) above.","title":"Deliverable"},{"location":"lesson02/linux/","text":"2.2: Processor initialization (Linux) We stopped our exploration of the Linux kernel at stext function, which is the entry point of arm64 architecture. This time we are going to go a little bit deeper and find some similarities with the code that we have already implemented in this and previous lessons. You may find this chapter a little bit boring because it mostly discusses different ARM system registers and how they are used in the Linux kernel. But I still consider it very important for the following reasons: It is necessary to understand the interface that the hardware provides to the software. Just by knowing this interface you will be able, in many cases, to deconstruct how a particular kernel feature is implemented and how software and hardware collaborate to implement this feature. Different options in the system register are usually related to enabling/disabling various hardware features. If you learn what different system registers an ARM processor have you will already have an idea what kind of functionality it supports. Ok, now let's resume our investigation of the stext function. ENTRY(stext) bl preserve_boot_args bl el2_setup // Drop to EL1, w0=cpu_boot_mode adrp x23, __PHYS_OFFSET and x23, x23, MIN_KIMG_ALIGN - 1 // KASLR offset, defaults to 0 bl set_cpu_boot_mode_flag bl __create_page_tables /* * The following calls CPU setup code, see arch/arm64/mm/proc.S for * details. * On return, the CPU will be ready for the MMU to be turned on and * the TCR will have been set. */ bl __cpu_setup // initialise processor b __primary_switch ENDPROC(stext) preserve_boot_args preserve_boot_args function is responsible for saving parameters, passed to the kernel by the bootloader. preserve_boot_args: mov x21, x0 // x21=FDT adr_l x0, boot_args // record the contents of stp x21, x1, [x0] // x0 .. x3 at kernel entry stp x2, x3, [x0, #16] dmb sy // needed before dc ivac with // MMU off mov x1, #0x20 // 4 x 8 bytes b __inval_dcache_area // tail call ENDPROC(preserve_boot_args) Accordingly to the kernel boot protocol , parameters are passed to the kernel in registers x0 - x3 . x0 contains the physical address of device tree blob ( .dtb ) in system RAM. x1 - x3 are reserved for future usage. What this function is doing is copying the content of x0 - x3 registers to the boot_args array and then invalidate the corresponding cache line from the data cache. Cache maintenance in a multiprocessor system is a large topic on its own, and we are going to skip it for now. For those who are interested in this subject, I can recommend reading Caches and Multi-core processors chapters of the ARM Programmer\u2019s Guide . el2_setup Accordingly to the arm64boot protocol , the kernel can be booted in either EL1 or EL2. In the second case, the kernel has access to the virtualization extensions and is able to act as a host operating system. If we are lucky enough to be booted in EL2, el2_setup function is called. It is responsible for configuring different parameters, accessible only at EL2, and dropping to EL1. Now I am going to split this function into small parts and explain each piece one by one. msr SPsel, #1 // We want to use SP_EL{1,2} Dedicated stack pointer will be used for both EL1 and EL2. Another option is to reuse stack pointer from EL0. mrs x0, CurrentEL cmp x0, #CurrentEL_EL2 b.eq 1f Only if current EL is EL2 branch to label 1 , otherwise we can't do EL2 setup and not much is left to be done in this function. mrs x0, sctlr_el1 CPU_BE( orr x0, x0, #(3 << 24) ) // Set the EE and E0E bits for EL1 CPU_LE( bic x0, x0, #(3 << 24) ) // Clear the EE and E0E bits for EL1 msr sctlr_el1, x0 mov w0, #BOOT_CPU_MODE_EL1 // This cpu booted in EL1 isb ret If it happens that we execute at EL1, sctlr_el1 register is updated so that CPU works in either big-endian of little-endian mode depending on the value of CPU_BIG_ENDIAN config setting. Then we just exit from the el2_setup function and return BOOT_CPU_MODE_EL1 constant. Accordingly to ARM64 Function Calling Conventions return value should be placed in x0 register (or w0 in our case. You can think about w0 register as the first 32 bit of x0 .). 1: mrs x0, sctlr_el2 CPU_BE( orr x0, x0, #(1 << 25) ) // Set the EE bit for EL2 CPU_LE( bic x0, x0, #(1 << 25) ) // Clear the EE bit for EL2 msr sctlr_el2, x0 If it appears that we are booted in EL2 we are doing the same kind of setup for EL2 (note that this time sctlr_el2 register is used instead of sctlr_el1 .). #ifdef CONFIG_ARM64_VHE /* * Check for VHE being present. For the rest of the EL2 setup, * x2 being non-zero indicates that we do have VHE, and that the * kernel is intended to run at EL2. */ mrs x2, id_aa64mmfr1_el1 ubfx x2, x2, #8, #4 #else mov x2, xzr #endif If Virtualization Host Extensions (VHE) is enabled via ARM64_VHE config variable and the host machine supports them, x2 then is updated with non zero value. x2 will be used to check whether VHE is enabled later in the same function. mov x0, #HCR_RW // 64-bit EL1 cbz x2, set_hcr orr x0, x0, #HCR_TGE // Enable Host Extensions orr x0, x0, #HCR_E2H set_hcr: msr hcr_el2, x0 isb Here we set hcr_el2 register. We used the same register to set 64-bit execution mode for EL1 in the RPi OS. This is exactly what is done in the first line of the provided code sample. Also if x2 != 0 , which means that VHE is available and the kernel is configured to use it, hcr_el2 is also used to enable VHE. /* * Allow Non-secure EL1 and EL0 to access physical timer and counter. * This is not necessary for VHE, since the host kernel runs in EL2, * and EL0 accesses are configured in the later stage of boot process. * Note that when HCR_EL2.E2H == 1, CNTHCTL_EL2 has the same bit layout * as CNTKCTL_EL1, and CNTKCTL_EL1 accessing instructions are redefined * to access CNTHCTL_EL2. This allows the kernel designed to run at EL1 * to transparently mess with the EL0 bits via CNTKCTL_EL1 access in * EL2. */ cbnz x2, 1f mrs x0, cnthctl_el2 orr x0, x0, #3 // Enable EL1 physical timers msr cnthctl_el2, x0 1: msr cntvoff_el2, xzr // Clear virtual offset Next piece of code is well explained in the comment above it. I have nothing to add. #ifdef CONFIG_ARM_GIC_V3 /* GICv3 system register access */ mrs x0, id_aa64pfr0_el1 ubfx x0, x0, #24, #4 cmp x0, #1 b.ne 3f mrs_s x0, SYS_ICC_SRE_EL2 orr x0, x0, #ICC_SRE_EL2_SRE // Set ICC_SRE_EL2.SRE==1 orr x0, x0, #ICC_SRE_EL2_ENABLE // Set ICC_SRE_EL2.Enable==1 msr_s SYS_ICC_SRE_EL2, x0 isb // Make sure SRE is now set mrs_s x0, SYS_ICC_SRE_EL2 // Read SRE back, tbz x0, #0, 3f // and check that it sticks msr_s SYS_ICH_HCR_EL2, xzr // Reset ICC_HCR_EL2 to defaults 3: #endif Next code snippet is executed only if GICv3 is available and enabled. GIC stands for Generic Interrupt Controller. v3 version of the GIC specification adds a few features, that are particularly useful in virtualization context. For example, with GICv3 it becomes possible to have LPIs (Locality-specific Peripheral Interrupt). Such interrupts are routed via message bus and their configuration is held in special tables in memory. The provided code is responsible for enabling SRE (System Register Interface) This step must be done before we will be able to use ICC_*_ELn registers and take advantages of GICv3 features. /* Populate ID registers. */ mrs x0, midr_el1 mrs x1, mpidr_el1 msr vpidr_el2, x0 msr vmpidr_el2, x1 midr_el1 and mpidr_el1 are read-only registers from the Identification registers group. They provide various information about processor manufacturer, processor architecture name, number of cores and some other info. It is possible to change this information for all readers that try to access it from EL1. Here we populate vpidr_el2 and vmpidr_el2 with the values taken from midr_el1 and mpidr_el1 , so this information is the same whether you try to access it from EL1 or higer exception levels. #ifdef CONFIG_COMPAT msr hstr_el2, xzr // Disable CP15 traps to EL2 #endif When the processor is executing in 32-bit execution mode, there is a concept of \"coprocessor\". The coprocessor can be used to access information, that in 64-bit execution mode is typically accessed via system registers. You can read about what exactly is accessible via coprocessor in the official documentation . msr hstr_el2, xzr instruction allows using coprocessor from lower exception levels. This makes sense to do only when compatibility mode is enabled (in this mode kernel can run 32-bit user applications on top of 64-bit kernel.). /* EL2 debug */ mrs x1, id_aa64dfr0_el1 // Check ID_AA64DFR0_EL1 PMUVer sbfx x0, x1, #8, #4 cmp x0, #1 b.lt 4f // Skip if no PMU present mrs x0, pmcr_el0 // Disable debug access traps ubfx x0, x0, #11, #5 // to EL2 and allow access to 4: csel x3, xzr, x0, lt // all PMU counters from EL1 /* Statistical profiling */ ubfx x0, x1, #32, #4 // Check ID_AA64DFR0_EL1 PMSVer cbz x0, 6f // Skip if SPE not present cbnz x2, 5f // VHE? mov x1, #(MDCR_EL2_E2PB_MASK << MDCR_EL2_E2PB_SHIFT) orr x3, x3, x1 // If we don't have VHE, then b 6f // use EL1&0 translation. 5: // For VHE, use EL2 translation orr x3, x3, #MDCR_EL2_TPMS // and disable access from EL1 6: msr mdcr_el2, x3 // Configure debug traps This piece of code is responsible for configuring mdcr_el2 (Monitor Debug Configuration Register (EL2)). This register is responsible for setting different debug traps, related to the virtualization extension. I am going to leave the details of this code block unexplained because debug and tracing are a little bit out of scope for our discussion. If you are interested in details, I can recommend you to read the description of mdcr_el2 register on page 2810 of the AArch64-Reference-Manual . /* Stage-2 translation */ msr vttbr_el2, xzr When your OS is used as a hypervisor it should provide complete memory isolation for its guest OSes. Stage 2 virtual memory translation is used precisely for this purpose: each guest OS thinks that it owns all system memory, though in reality each memory access is mapped to the physical memory by stage 2 translation. vttbr_el2 holds the base address of the translation table for the stage 2 translation. At this point, stage 2 translation is disabled, and vttbr_el2 should be set to 0. cbz x2, install_el2_stub mov w0, #BOOT_CPU_MODE_EL2 // This CPU booted in EL2 isb ret First x2 is compared to 0 to check whether VHE is enabled. If yes - jump to install_el2_stub label, otherwise record that CPU is booted in EL2 mode and exit from el2_setup function. In the latter case, the processor continues to operate in EL2 mode and EL1 will not be used at all. install_el2_stub: /* sctlr_el1 */ mov x0, #0x0800 // Set/clear RES{1,0} bits CPU_BE( movk x0, #0x33d0, lsl #16 ) // Set EE and E0E on BE systems CPU_LE( movk x0, #0x30d0, lsl #16 ) // Clear EE and E0E on LE systems msr sctlr_el1, x0 If we reach this point it means that we don't need VHE and are going to switch to EL1 soon, so early EL1 initialization needs to be done here. The copied code snippet is responsible for sctlr_el1 (System Control Register) initialization. We already did the same job here for the RPi OS. /* Coprocessor traps. */ mov x0, #0x33ff msr cptr_el2, x0 // Disable copro. traps to EL2 This code allows EL1 to access cpacr_el1 register and, as a result, to control access to Trace, Floating-point, and Advanced SIMD functionality. /* Hypervisor stub */ adr_l x0, __hyp_stub_vectors msr vbar_el2, x0 We don't plan to use EL2 now, though some functionality requires it. We need it, for example, to implement kexec system call that enables you to load and boot into another kernel from the currently running kernel. _hyp_stub_vectors holds the addresses of all exception handlers for EL2. We are going to implement exception handling functionality for EL1 in the next lesson, after we talk about interrupts and exception handling in details. /* spsr */ mov x0, #(PSR_F_BIT | PSR_I_BIT | PSR_A_BIT | PSR_D_BIT |\\ PSR_MODE_EL1h) msr spsr_el2, x0 msr elr_el2, lr mov w0, #BOOT_CPU_MODE_EL2 // This CPU booted in EL2 eret Finally, we need to initialize processor state at EL1 and switch exception levels. We already did it for the RPi OS so I am not going to explain the details of this code. The only new thing here is the way how elr_el2 is initialized. lr or Link Register is an alias for x30 . Whenever you execute bl (Branch Link) instruction x30 is automatically populated with the address of the current instruction. This fact is usually used by ret instruction, so it knows where exactly to return. In our case, lr points here and, because of the way how we initialized elr_el2 , this is also the place from which the execution is going to be resumed after switching to EL1. Processor initialization at EL1 Now we are back to the stext function. Next few lines are not very important for us, but I want to explain them for the sake of completeness. adrp x23, __PHYS_OFFSET and x23, x23, MIN_KIMG_ALIGN - 1 // KASLR offset, defaults to 0 KASLR , or Kernel address space layout randomization, is a technique that allows to place the kernel at a random address in the memory. This is required only for security reasons. For more information, you can read the link above. bl set_cpu_boot_mode_flag Here CPU boot mode is saved into __boot_cpu_mode variable. The code that does this is very similar to preserve_boot_args function that we explored previously. bl __create_page_tables bl __cpu_setup // initialise processor b __primary_switch The last 3 functions are very important, but they all are related to virtual memory management, so we are going to postpone their detailed exploration until the lesson 6. For now, I just want to brefely describe there meanings. * __create_page_tables As its name stands this one is responsible for creating Page Tables. * __cpu_setup Initialize various processor settings, mostly specific for virtual memory management. * __primary_switch Enable MMU and jump to start_kernel function, which is architecture independent starting point. Conclusion In this chapter, we briefly discussed how a processor is initialized when the Linux kernel is booted. In the next lesson, we will continue to closely work with the ARM processor and investigate a vital topic for any OS: interrupt handling. Previous Page 2.1 Processor initialization: RPi OS Next Page 2.3 Processor initialization: Exercises","title":"Linux"},{"location":"lesson02/linux/#22-processor-initialization-linux","text":"We stopped our exploration of the Linux kernel at stext function, which is the entry point of arm64 architecture. This time we are going to go a little bit deeper and find some similarities with the code that we have already implemented in this and previous lessons. You may find this chapter a little bit boring because it mostly discusses different ARM system registers and how they are used in the Linux kernel. But I still consider it very important for the following reasons: It is necessary to understand the interface that the hardware provides to the software. Just by knowing this interface you will be able, in many cases, to deconstruct how a particular kernel feature is implemented and how software and hardware collaborate to implement this feature. Different options in the system register are usually related to enabling/disabling various hardware features. If you learn what different system registers an ARM processor have you will already have an idea what kind of functionality it supports. Ok, now let's resume our investigation of the stext function. ENTRY(stext) bl preserve_boot_args bl el2_setup // Drop to EL1, w0=cpu_boot_mode adrp x23, __PHYS_OFFSET and x23, x23, MIN_KIMG_ALIGN - 1 // KASLR offset, defaults to 0 bl set_cpu_boot_mode_flag bl __create_page_tables /* * The following calls CPU setup code, see arch/arm64/mm/proc.S for * details. * On return, the CPU will be ready for the MMU to be turned on and * the TCR will have been set. */ bl __cpu_setup // initialise processor b __primary_switch ENDPROC(stext)","title":"2.2: Processor initialization (Linux)"},{"location":"lesson02/linux/#preserve_boot_args","text":"preserve_boot_args function is responsible for saving parameters, passed to the kernel by the bootloader. preserve_boot_args: mov x21, x0 // x21=FDT adr_l x0, boot_args // record the contents of stp x21, x1, [x0] // x0 .. x3 at kernel entry stp x2, x3, [x0, #16] dmb sy // needed before dc ivac with // MMU off mov x1, #0x20 // 4 x 8 bytes b __inval_dcache_area // tail call ENDPROC(preserve_boot_args) Accordingly to the kernel boot protocol , parameters are passed to the kernel in registers x0 - x3 . x0 contains the physical address of device tree blob ( .dtb ) in system RAM. x1 - x3 are reserved for future usage. What this function is doing is copying the content of x0 - x3 registers to the boot_args array and then invalidate the corresponding cache line from the data cache. Cache maintenance in a multiprocessor system is a large topic on its own, and we are going to skip it for now. For those who are interested in this subject, I can recommend reading Caches and Multi-core processors chapters of the ARM Programmer\u2019s Guide .","title":"preserve_boot_args"},{"location":"lesson02/linux/#el2_setup","text":"Accordingly to the arm64boot protocol , the kernel can be booted in either EL1 or EL2. In the second case, the kernel has access to the virtualization extensions and is able to act as a host operating system. If we are lucky enough to be booted in EL2, el2_setup function is called. It is responsible for configuring different parameters, accessible only at EL2, and dropping to EL1. Now I am going to split this function into small parts and explain each piece one by one. msr SPsel, #1 // We want to use SP_EL{1,2} Dedicated stack pointer will be used for both EL1 and EL2. Another option is to reuse stack pointer from EL0. mrs x0, CurrentEL cmp x0, #CurrentEL_EL2 b.eq 1f Only if current EL is EL2 branch to label 1 , otherwise we can't do EL2 setup and not much is left to be done in this function. mrs x0, sctlr_el1 CPU_BE( orr x0, x0, #(3 << 24) ) // Set the EE and E0E bits for EL1 CPU_LE( bic x0, x0, #(3 << 24) ) // Clear the EE and E0E bits for EL1 msr sctlr_el1, x0 mov w0, #BOOT_CPU_MODE_EL1 // This cpu booted in EL1 isb ret If it happens that we execute at EL1, sctlr_el1 register is updated so that CPU works in either big-endian of little-endian mode depending on the value of CPU_BIG_ENDIAN config setting. Then we just exit from the el2_setup function and return BOOT_CPU_MODE_EL1 constant. Accordingly to ARM64 Function Calling Conventions return value should be placed in x0 register (or w0 in our case. You can think about w0 register as the first 32 bit of x0 .). 1: mrs x0, sctlr_el2 CPU_BE( orr x0, x0, #(1 << 25) ) // Set the EE bit for EL2 CPU_LE( bic x0, x0, #(1 << 25) ) // Clear the EE bit for EL2 msr sctlr_el2, x0 If it appears that we are booted in EL2 we are doing the same kind of setup for EL2 (note that this time sctlr_el2 register is used instead of sctlr_el1 .). #ifdef CONFIG_ARM64_VHE /* * Check for VHE being present. For the rest of the EL2 setup, * x2 being non-zero indicates that we do have VHE, and that the * kernel is intended to run at EL2. */ mrs x2, id_aa64mmfr1_el1 ubfx x2, x2, #8, #4 #else mov x2, xzr #endif If Virtualization Host Extensions (VHE) is enabled via ARM64_VHE config variable and the host machine supports them, x2 then is updated with non zero value. x2 will be used to check whether VHE is enabled later in the same function. mov x0, #HCR_RW // 64-bit EL1 cbz x2, set_hcr orr x0, x0, #HCR_TGE // Enable Host Extensions orr x0, x0, #HCR_E2H set_hcr: msr hcr_el2, x0 isb Here we set hcr_el2 register. We used the same register to set 64-bit execution mode for EL1 in the RPi OS. This is exactly what is done in the first line of the provided code sample. Also if x2 != 0 , which means that VHE is available and the kernel is configured to use it, hcr_el2 is also used to enable VHE. /* * Allow Non-secure EL1 and EL0 to access physical timer and counter. * This is not necessary for VHE, since the host kernel runs in EL2, * and EL0 accesses are configured in the later stage of boot process. * Note that when HCR_EL2.E2H == 1, CNTHCTL_EL2 has the same bit layout * as CNTKCTL_EL1, and CNTKCTL_EL1 accessing instructions are redefined * to access CNTHCTL_EL2. This allows the kernel designed to run at EL1 * to transparently mess with the EL0 bits via CNTKCTL_EL1 access in * EL2. */ cbnz x2, 1f mrs x0, cnthctl_el2 orr x0, x0, #3 // Enable EL1 physical timers msr cnthctl_el2, x0 1: msr cntvoff_el2, xzr // Clear virtual offset Next piece of code is well explained in the comment above it. I have nothing to add. #ifdef CONFIG_ARM_GIC_V3 /* GICv3 system register access */ mrs x0, id_aa64pfr0_el1 ubfx x0, x0, #24, #4 cmp x0, #1 b.ne 3f mrs_s x0, SYS_ICC_SRE_EL2 orr x0, x0, #ICC_SRE_EL2_SRE // Set ICC_SRE_EL2.SRE==1 orr x0, x0, #ICC_SRE_EL2_ENABLE // Set ICC_SRE_EL2.Enable==1 msr_s SYS_ICC_SRE_EL2, x0 isb // Make sure SRE is now set mrs_s x0, SYS_ICC_SRE_EL2 // Read SRE back, tbz x0, #0, 3f // and check that it sticks msr_s SYS_ICH_HCR_EL2, xzr // Reset ICC_HCR_EL2 to defaults 3: #endif Next code snippet is executed only if GICv3 is available and enabled. GIC stands for Generic Interrupt Controller. v3 version of the GIC specification adds a few features, that are particularly useful in virtualization context. For example, with GICv3 it becomes possible to have LPIs (Locality-specific Peripheral Interrupt). Such interrupts are routed via message bus and their configuration is held in special tables in memory. The provided code is responsible for enabling SRE (System Register Interface) This step must be done before we will be able to use ICC_*_ELn registers and take advantages of GICv3 features. /* Populate ID registers. */ mrs x0, midr_el1 mrs x1, mpidr_el1 msr vpidr_el2, x0 msr vmpidr_el2, x1 midr_el1 and mpidr_el1 are read-only registers from the Identification registers group. They provide various information about processor manufacturer, processor architecture name, number of cores and some other info. It is possible to change this information for all readers that try to access it from EL1. Here we populate vpidr_el2 and vmpidr_el2 with the values taken from midr_el1 and mpidr_el1 , so this information is the same whether you try to access it from EL1 or higer exception levels. #ifdef CONFIG_COMPAT msr hstr_el2, xzr // Disable CP15 traps to EL2 #endif When the processor is executing in 32-bit execution mode, there is a concept of \"coprocessor\". The coprocessor can be used to access information, that in 64-bit execution mode is typically accessed via system registers. You can read about what exactly is accessible via coprocessor in the official documentation . msr hstr_el2, xzr instruction allows using coprocessor from lower exception levels. This makes sense to do only when compatibility mode is enabled (in this mode kernel can run 32-bit user applications on top of 64-bit kernel.). /* EL2 debug */ mrs x1, id_aa64dfr0_el1 // Check ID_AA64DFR0_EL1 PMUVer sbfx x0, x1, #8, #4 cmp x0, #1 b.lt 4f // Skip if no PMU present mrs x0, pmcr_el0 // Disable debug access traps ubfx x0, x0, #11, #5 // to EL2 and allow access to 4: csel x3, xzr, x0, lt // all PMU counters from EL1 /* Statistical profiling */ ubfx x0, x1, #32, #4 // Check ID_AA64DFR0_EL1 PMSVer cbz x0, 6f // Skip if SPE not present cbnz x2, 5f // VHE? mov x1, #(MDCR_EL2_E2PB_MASK << MDCR_EL2_E2PB_SHIFT) orr x3, x3, x1 // If we don't have VHE, then b 6f // use EL1&0 translation. 5: // For VHE, use EL2 translation orr x3, x3, #MDCR_EL2_TPMS // and disable access from EL1 6: msr mdcr_el2, x3 // Configure debug traps This piece of code is responsible for configuring mdcr_el2 (Monitor Debug Configuration Register (EL2)). This register is responsible for setting different debug traps, related to the virtualization extension. I am going to leave the details of this code block unexplained because debug and tracing are a little bit out of scope for our discussion. If you are interested in details, I can recommend you to read the description of mdcr_el2 register on page 2810 of the AArch64-Reference-Manual . /* Stage-2 translation */ msr vttbr_el2, xzr When your OS is used as a hypervisor it should provide complete memory isolation for its guest OSes. Stage 2 virtual memory translation is used precisely for this purpose: each guest OS thinks that it owns all system memory, though in reality each memory access is mapped to the physical memory by stage 2 translation. vttbr_el2 holds the base address of the translation table for the stage 2 translation. At this point, stage 2 translation is disabled, and vttbr_el2 should be set to 0. cbz x2, install_el2_stub mov w0, #BOOT_CPU_MODE_EL2 // This CPU booted in EL2 isb ret First x2 is compared to 0 to check whether VHE is enabled. If yes - jump to install_el2_stub label, otherwise record that CPU is booted in EL2 mode and exit from el2_setup function. In the latter case, the processor continues to operate in EL2 mode and EL1 will not be used at all. install_el2_stub: /* sctlr_el1 */ mov x0, #0x0800 // Set/clear RES{1,0} bits CPU_BE( movk x0, #0x33d0, lsl #16 ) // Set EE and E0E on BE systems CPU_LE( movk x0, #0x30d0, lsl #16 ) // Clear EE and E0E on LE systems msr sctlr_el1, x0 If we reach this point it means that we don't need VHE and are going to switch to EL1 soon, so early EL1 initialization needs to be done here. The copied code snippet is responsible for sctlr_el1 (System Control Register) initialization. We already did the same job here for the RPi OS. /* Coprocessor traps. */ mov x0, #0x33ff msr cptr_el2, x0 // Disable copro. traps to EL2 This code allows EL1 to access cpacr_el1 register and, as a result, to control access to Trace, Floating-point, and Advanced SIMD functionality. /* Hypervisor stub */ adr_l x0, __hyp_stub_vectors msr vbar_el2, x0 We don't plan to use EL2 now, though some functionality requires it. We need it, for example, to implement kexec system call that enables you to load and boot into another kernel from the currently running kernel. _hyp_stub_vectors holds the addresses of all exception handlers for EL2. We are going to implement exception handling functionality for EL1 in the next lesson, after we talk about interrupts and exception handling in details. /* spsr */ mov x0, #(PSR_F_BIT | PSR_I_BIT | PSR_A_BIT | PSR_D_BIT |\\ PSR_MODE_EL1h) msr spsr_el2, x0 msr elr_el2, lr mov w0, #BOOT_CPU_MODE_EL2 // This CPU booted in EL2 eret Finally, we need to initialize processor state at EL1 and switch exception levels. We already did it for the RPi OS so I am not going to explain the details of this code. The only new thing here is the way how elr_el2 is initialized. lr or Link Register is an alias for x30 . Whenever you execute bl (Branch Link) instruction x30 is automatically populated with the address of the current instruction. This fact is usually used by ret instruction, so it knows where exactly to return. In our case, lr points here and, because of the way how we initialized elr_el2 , this is also the place from which the execution is going to be resumed after switching to EL1.","title":"el2_setup"},{"location":"lesson02/linux/#processor-initialization-at-el1","text":"Now we are back to the stext function. Next few lines are not very important for us, but I want to explain them for the sake of completeness. adrp x23, __PHYS_OFFSET and x23, x23, MIN_KIMG_ALIGN - 1 // KASLR offset, defaults to 0 KASLR , or Kernel address space layout randomization, is a technique that allows to place the kernel at a random address in the memory. This is required only for security reasons. For more information, you can read the link above. bl set_cpu_boot_mode_flag Here CPU boot mode is saved into __boot_cpu_mode variable. The code that does this is very similar to preserve_boot_args function that we explored previously. bl __create_page_tables bl __cpu_setup // initialise processor b __primary_switch The last 3 functions are very important, but they all are related to virtual memory management, so we are going to postpone their detailed exploration until the lesson 6. For now, I just want to brefely describe there meanings. * __create_page_tables As its name stands this one is responsible for creating Page Tables. * __cpu_setup Initialize various processor settings, mostly specific for virtual memory management. * __primary_switch Enable MMU and jump to start_kernel function, which is architecture independent starting point.","title":"Processor initialization at EL1"},{"location":"lesson02/linux/#conclusion","text":"In this chapter, we briefly discussed how a processor is initialized when the Linux kernel is booted. In the next lesson, we will continue to closely work with the ARM processor and investigate a vital topic for any OS: interrupt handling.","title":"Conclusion"},{"location":"lesson02/linux/#previous-page","text":"2.1 Processor initialization: RPi OS","title":"Previous Page"},{"location":"lesson02/linux/#next-page","text":"2.3 Processor initialization: Exercises","title":"Next Page"},{"location":"lesson02/rpi-os/","text":"2: Processor initialization Objectives Background: Exception levels (EL) Switching ELs Enhanced debugging Bring up printf() QEMU + GDB debugging Code Walkthrough Finding out the current EL Switching to EL1 SCTLR_EL1, System Control Register (EL1) HCR_EL2, Hypervisor Configuration (EL2) SCR_EL3, Secure Configuration (EL3) SPSR_EL3, Saved Program Status (EL3) ELR_EL3, Exception Link (EL3) Conclusion Objectives We are going to build: A baremetal program that can switch among CPU exception levels and print out the current level. Students will: Experiment with exception levels (ELs) Observe switches among ELs -- crucial for subsequent experiments! Tinker with the kernel, e.g. debugging Background: Exception levels (EL) ARMv8 defines 4 exception levels. You can think about an exception level as a processor execution mode in which only a subset of all operations and registers is available. The least privileged exception level, i.e. lowest level, is level 0. When processor operates at this level, it mostly uses only general purpose registers (X0 - X30) and stack pointer register (SP). EL0 also allows using STR and LDR commands to load and store data to and from memory and a few other instructions commonly used by a user program. An OS kernel deals with exception levels because it needs to implement isolation . A user process should not be able to access other process's data. To achieve such behavior, a kernel always runs each user process at EL0. Operating at this exception level a process can only use it's own virtual memory and can't access any instructions that change ELs, MMUs, etc. The kernel itself usually works at EL1. While running at this exception level CPU gets access to the registers that allows configuring MMU as well as some system registers. About EL2/3 : this lab is NOT going to use EL 2 or EL 3, but I just want to briefly describe them so you can get an idea why they are needed. EL2 is used in a scenario when we are using a hypervisor. In this case hypervisor runs at EL2 and guest OSes run at EL1. This allows the hypervisor to isolate guest OSes in a similar way how OS isolates user processes. EL3 pertains to Arm TrustZone. It is used for transitions from ARM \"Secure World\" to \"Insecure world\". This abstraction exist to provide full hardware isolation between the software running in two different \"worlds\". Application from an \"Insecure world\" can in no way access or modify information (both instruction and data) that belongs to \"secure world\", and this restriction is enforced at the hardware level. We will do TrustZone experiments soon! Switching ELs In Arm architecture, there is no way a program can raise its own exception level without involving software that already runs on a higher level. This makes a perfect sense: otherwise, any program would be able to escape its assigned EL and makes unauthorized access to memory or registers. Current EL can be changed only if an exception is generated. Common causes of exceptions include (not limited to): software executes some illegal instruction (for example, tries to access memory location at a nonexisting address; software tries to divide an integer by 0; software executes special instructions (e.g. svc ) to request exceptions. In Arm's lingo, IO-generated interrupts are also handled as a special type of exceptions. Whenever an exception is generated the following sequence of steps takes place (In the description I am assuming that the exception is handled at EL n , were n could be 1, 2 or 3). Address of the current instruction is saved in the ELR_ELn register. (It is called Exception link register ) Current processor state is stored in SPSR_ELn register ( Saved Program Status Register ) NB: As some of you may know, other CPU hardware may automatically push registers on stack prior to exception handling. Armv8 does NOT do that. An exception handler is executed and does whatever job it needs to do. Exception handler calls eret instruction. This instruction restores processor state from SPSR_ELn and resumes execution starting from the address, stored in the ELR_ELn register. There are more details, e.g. the exception handler software also needs to store the state of all general purpose registers and restore it back afterwards, as we will discuss this process in details in the upcoming experiment. For now, we need just to understand the process in general and remember the meaning of the ELR_ELm and SPSR_ELn registers. An important thing to know is that exception handler is not obliged to return to the same instruction where the exception originates. Both ELR_ELm and SPSR_ELn are writable and the exception handler can modify them in order to specify the instructions to execute right after the EL switch. We are going to use this technique to our advantage when we try to switch from EL3 to EL1 in our code. Enhanced debugging Bring up printf() Right now, the kernel can only print some constant string on a screen, but what I need is some analog of printf function. With printf I can easily display values of different registers and variables. Such functionality is essential for the kernel development because you don't have any other debugger support and printf becomes the only mean for figuring out what is going on inside Rpi3. Let's not reinvent the wheel and use one of existing printf implementations This function consists mostly from string manipulations and is not very interesting from a kernel developer point of view. The implementation that I used is very small and don't have external dependencies, that allows it to be easily integrated into the kernel. The only thing that I have to do is to define putc function that can send a single character to the screen. This function is defined here and it just uses already existing uart_send function. Also, we need to initialize the printf library and specify the location of the putc function. This is done in a single line of code . QEMU + GDB debugging GDB allows you to do single step, etc. It may help understand/debug specific instructions. You can find extensive information online. A quick note is here . Code Walkthrough Finding out the current EL As we are equipped with the printf function, we can proceed to figure out at which exception level the kernel is booted. A small function that can answer this question looks like this. .globl get_el get_el: mrs x0, CurrentEL lsr x0, x0, #2 ret Here we use mrs instruction to read the value from CurrentEL system register into x0 register. Then we shift this value 2 bits to the right (because the lowest 2 bits in the CurrentEL register are reserved and always have value 0). Finally the register x0 contains an integer number indicating current exception level. Now the only thing that is left is to display this value, like this . int el = get_el(); printf(\"Exception level: %d \\r\\n\", el); Rpi3: If you reproduce this experiment, you should see Exception level: 3 on the screen. This means the CPU executes as the security monitor when it boots up. QEMU: You will see Exception level: 2 because this is how QEMU emulates the CPU: setting the initial EL as 2. Why? Switching to EL1 EL1 is intended for OS kernels. Strictly speaking, our kernel is not obliged to switch to EL1 when it boots up, but EL1 is a natural choice for us because this level has just the right set of privileges to implement all common OS tasks. It also will be an interesting exercise to see how switching exceptions levels works in action. Let's take a look at the source code that does this . master: ldr x0, =SCTLR_VALUE_MMU_DISABLED msr sctlr_el1, x0 ldr x0, =HCR_VALUE msr hcr_el2, x0 ldr x0, =SCR_VALUE msr scr_el3, x0 ldr x0, =SPSR_VALUE msr spsr_el3, x0 adr x0, el1_entry msr elr_el3, x0 eret The code configures a few system registers. Now we are going to examine those registers one by one. The register details are documented in the Armv8 architecture manual which we will refer to as needed. SCTLR_EL1, System Control Register (EL1) ldr x0, =SCTLR_VALUE_MMU_DISABLED msr sctlr_el1, x0 Here we set the value of the sctlr_el1 system register. sctlr_el1 is responsible for configuring different parameters of CPU when CPU operates at EL1. For example, it controls whether the cache is enabled and, what is most important for us, whether the MMU (Memory Management Unit) is turned on. sctlr_el1 is accessible from all exception levels higher or equal than EL1 (you can infer this from _el1 postfix) SCTLR_VALUE_MMU_DISABLED constant is defined here Individual bits of this value are defined like this: #define SCTLR_RESERVED (3 << 28) | (3 << 22) | (1 << 20) | (1 << 11) Some bits in the description of sctlr_el1 register are marked as RES1 . Those bits are reserved for future usage and should be initialized with 1 . #define SCTLR_EE_LITTLE_ENDIAN (0 << 25) Exception Endianness . This field controls endianess of explicit data access at EL1. We are going to configure the processor to work only with little-endian format. #define SCTLR_EOE_LITTLE_ENDIAN (0 << 24) Similar to previous field but this one controls endianess of explicit data access at EL0, instead of EL1. #define SCTLR_I_CACHE_DISABLED (0 << 12) Disable instruction cache. We are going to disable all caches for simplicity. You can find more information about data and instruction caches here . #define SCTLR_D_CACHE_DISABLED (0 << 2) Disable data cache. #define SCTLR_MMU_DISABLED (0 << 0) Disable MMU. MMU must be disabled until the lesson 6, where we are going to prepare page tables and start working with virtual memory. FYI - official doc HCR_EL2, Hypervisor Configuration (EL2) ldr x0, =HCR_VALUE msr hcr_el2, x0 We are NOT going to implement our own hypervisor . Still we need to use this register. Among other settings, bit 31 (RW) controls the execution state at EL1, being AArch64 (1) or AArch32 (0). This register also controls at which EL we will handle IRQ. In sysregs.h we set HCR_VALUE to be (1<<31). Official doc SCR_EL3, Secure Configuration (EL3) ldr x0, =SCR_VALUE msr scr_el3, x0 This register is responsible for configuring security settings. For example, it controls whether all lower levels are executed in \"secure\" or \"nonsecure\" world. It also controls execution state at EL2. Here we set that EL2 will execute at AArch64 state, and all lower exception levels will be \"non secure\". This register has no counterpart at EL2. Therefore, we don't have to set it on qemu emulation. Official doc SPSR_EL3, Saved Program Status (EL3) ldr x0, =SPSR_VALUE msr spsr_el3, x0 spsr_el3 contains CPU state, that will be restored after we execute eret instruction. What is CPU state? It consists of the following information: Condition Flags Those flags contains information about previously executed executions: whether the result was negative (N flag), zero (A flag), has unsigned overflow (C flag) or has signed overflow (V flag). Values of those flags can be used in conditional branch instructions. For example, b.eq instruction will jump to the provided label only if the result of the last comparison operation is equal to 0. The processor checks this by testing whether Z flag is set to 1. Interrupt disable bits Those bits allows to enable/disable different types of interrupts. EL & other information , required to fully restore the processor execution state after an exception is handled. Usually spsr_el3 is saved automatically by CPU hardware, when an exception is taken to EL3. Furthermore, this register is writable by our code, so we take advantage of this fact and manually prepare CPU state. SPSR_VALUE is prepared here and we initialize the following fields: #define SPSR_MASK_ALL (7 << 6) After we change EL to EL1 all types of interrupts will be masked (or disabled, which is the same). #define SPSR_EL1h (5 << 0) This indicates to which EL the eret instruction will take the CPU to. It's EL1. About EL1h: At EL1 we can either use our own dedicated stack pointer or use EL0 stack pointer. EL1h mode means that we are using EL1 dedicated stack pointer. Official doc ELR_EL3, Exception Link (EL3) adr x0, el1_entry msr elr_el3, x0 eret elr_el3 holds the address, to which we are going to return after eret instruction will be executed. Here we set this address to the location of el1_entry label. Official doc Conclusion That is pretty much it: when we enter el1_entry function the execution should be already at EL1 mode. Our subsequent experiments will switch between EL1 (kernel) and EL0 (user) frequently. Go ahead and try it out!","title":"2: Processor initialization"},{"location":"lesson02/rpi-os/#2-processor-initialization","text":"Objectives Background: Exception levels (EL) Switching ELs Enhanced debugging Bring up printf() QEMU + GDB debugging Code Walkthrough Finding out the current EL Switching to EL1 SCTLR_EL1, System Control Register (EL1) HCR_EL2, Hypervisor Configuration (EL2) SCR_EL3, Secure Configuration (EL3) SPSR_EL3, Saved Program Status (EL3) ELR_EL3, Exception Link (EL3) Conclusion","title":"2: Processor initialization"},{"location":"lesson02/rpi-os/#objectives","text":"We are going to build: A baremetal program that can switch among CPU exception levels and print out the current level. Students will: Experiment with exception levels (ELs) Observe switches among ELs -- crucial for subsequent experiments! Tinker with the kernel, e.g. debugging","title":"Objectives"},{"location":"lesson02/rpi-os/#background-exception-levels-el","text":"ARMv8 defines 4 exception levels. You can think about an exception level as a processor execution mode in which only a subset of all operations and registers is available. The least privileged exception level, i.e. lowest level, is level 0. When processor operates at this level, it mostly uses only general purpose registers (X0 - X30) and stack pointer register (SP). EL0 also allows using STR and LDR commands to load and store data to and from memory and a few other instructions commonly used by a user program. An OS kernel deals with exception levels because it needs to implement isolation . A user process should not be able to access other process's data. To achieve such behavior, a kernel always runs each user process at EL0. Operating at this exception level a process can only use it's own virtual memory and can't access any instructions that change ELs, MMUs, etc. The kernel itself usually works at EL1. While running at this exception level CPU gets access to the registers that allows configuring MMU as well as some system registers. About EL2/3 : this lab is NOT going to use EL 2 or EL 3, but I just want to briefly describe them so you can get an idea why they are needed. EL2 is used in a scenario when we are using a hypervisor. In this case hypervisor runs at EL2 and guest OSes run at EL1. This allows the hypervisor to isolate guest OSes in a similar way how OS isolates user processes. EL3 pertains to Arm TrustZone. It is used for transitions from ARM \"Secure World\" to \"Insecure world\". This abstraction exist to provide full hardware isolation between the software running in two different \"worlds\". Application from an \"Insecure world\" can in no way access or modify information (both instruction and data) that belongs to \"secure world\", and this restriction is enforced at the hardware level. We will do TrustZone experiments soon!","title":"Background: Exception levels (EL)"},{"location":"lesson02/rpi-os/#switching-els","text":"In Arm architecture, there is no way a program can raise its own exception level without involving software that already runs on a higher level. This makes a perfect sense: otherwise, any program would be able to escape its assigned EL and makes unauthorized access to memory or registers. Current EL can be changed only if an exception is generated. Common causes of exceptions include (not limited to): software executes some illegal instruction (for example, tries to access memory location at a nonexisting address; software tries to divide an integer by 0; software executes special instructions (e.g. svc ) to request exceptions. In Arm's lingo, IO-generated interrupts are also handled as a special type of exceptions. Whenever an exception is generated the following sequence of steps takes place (In the description I am assuming that the exception is handled at EL n , were n could be 1, 2 or 3). Address of the current instruction is saved in the ELR_ELn register. (It is called Exception link register ) Current processor state is stored in SPSR_ELn register ( Saved Program Status Register ) NB: As some of you may know, other CPU hardware may automatically push registers on stack prior to exception handling. Armv8 does NOT do that. An exception handler is executed and does whatever job it needs to do. Exception handler calls eret instruction. This instruction restores processor state from SPSR_ELn and resumes execution starting from the address, stored in the ELR_ELn register. There are more details, e.g. the exception handler software also needs to store the state of all general purpose registers and restore it back afterwards, as we will discuss this process in details in the upcoming experiment. For now, we need just to understand the process in general and remember the meaning of the ELR_ELm and SPSR_ELn registers. An important thing to know is that exception handler is not obliged to return to the same instruction where the exception originates. Both ELR_ELm and SPSR_ELn are writable and the exception handler can modify them in order to specify the instructions to execute right after the EL switch. We are going to use this technique to our advantage when we try to switch from EL3 to EL1 in our code.","title":"Switching ELs"},{"location":"lesson02/rpi-os/#enhanced-debugging","text":"","title":"Enhanced debugging"},{"location":"lesson02/rpi-os/#bring-up-printf","text":"Right now, the kernel can only print some constant string on a screen, but what I need is some analog of printf function. With printf I can easily display values of different registers and variables. Such functionality is essential for the kernel development because you don't have any other debugger support and printf becomes the only mean for figuring out what is going on inside Rpi3. Let's not reinvent the wheel and use one of existing printf implementations This function consists mostly from string manipulations and is not very interesting from a kernel developer point of view. The implementation that I used is very small and don't have external dependencies, that allows it to be easily integrated into the kernel. The only thing that I have to do is to define putc function that can send a single character to the screen. This function is defined here and it just uses already existing uart_send function. Also, we need to initialize the printf library and specify the location of the putc function. This is done in a single line of code .","title":"Bring up printf()"},{"location":"lesson02/rpi-os/#qemu-gdb-debugging","text":"GDB allows you to do single step, etc. It may help understand/debug specific instructions. You can find extensive information online. A quick note is here .","title":"QEMU + GDB debugging"},{"location":"lesson02/rpi-os/#code-walkthrough","text":"","title":"Code Walkthrough"},{"location":"lesson02/rpi-os/#finding-out-the-current-el","text":"As we are equipped with the printf function, we can proceed to figure out at which exception level the kernel is booted. A small function that can answer this question looks like this. .globl get_el get_el: mrs x0, CurrentEL lsr x0, x0, #2 ret Here we use mrs instruction to read the value from CurrentEL system register into x0 register. Then we shift this value 2 bits to the right (because the lowest 2 bits in the CurrentEL register are reserved and always have value 0). Finally the register x0 contains an integer number indicating current exception level. Now the only thing that is left is to display this value, like this . int el = get_el(); printf(\"Exception level: %d \\r\\n\", el); Rpi3: If you reproduce this experiment, you should see Exception level: 3 on the screen. This means the CPU executes as the security monitor when it boots up. QEMU: You will see Exception level: 2 because this is how QEMU emulates the CPU: setting the initial EL as 2. Why?","title":"Finding out the current EL"},{"location":"lesson02/rpi-os/#switching-to-el1","text":"EL1 is intended for OS kernels. Strictly speaking, our kernel is not obliged to switch to EL1 when it boots up, but EL1 is a natural choice for us because this level has just the right set of privileges to implement all common OS tasks. It also will be an interesting exercise to see how switching exceptions levels works in action. Let's take a look at the source code that does this . master: ldr x0, =SCTLR_VALUE_MMU_DISABLED msr sctlr_el1, x0 ldr x0, =HCR_VALUE msr hcr_el2, x0 ldr x0, =SCR_VALUE msr scr_el3, x0 ldr x0, =SPSR_VALUE msr spsr_el3, x0 adr x0, el1_entry msr elr_el3, x0 eret The code configures a few system registers. Now we are going to examine those registers one by one. The register details are documented in the Armv8 architecture manual which we will refer to as needed.","title":"Switching to EL1"},{"location":"lesson02/rpi-os/#sctlr_el1-system-control-register-el1","text":"ldr x0, =SCTLR_VALUE_MMU_DISABLED msr sctlr_el1, x0 Here we set the value of the sctlr_el1 system register. sctlr_el1 is responsible for configuring different parameters of CPU when CPU operates at EL1. For example, it controls whether the cache is enabled and, what is most important for us, whether the MMU (Memory Management Unit) is turned on. sctlr_el1 is accessible from all exception levels higher or equal than EL1 (you can infer this from _el1 postfix) SCTLR_VALUE_MMU_DISABLED constant is defined here Individual bits of this value are defined like this: #define SCTLR_RESERVED (3 << 28) | (3 << 22) | (1 << 20) | (1 << 11) Some bits in the description of sctlr_el1 register are marked as RES1 . Those bits are reserved for future usage and should be initialized with 1 . #define SCTLR_EE_LITTLE_ENDIAN (0 << 25) Exception Endianness . This field controls endianess of explicit data access at EL1. We are going to configure the processor to work only with little-endian format. #define SCTLR_EOE_LITTLE_ENDIAN (0 << 24) Similar to previous field but this one controls endianess of explicit data access at EL0, instead of EL1. #define SCTLR_I_CACHE_DISABLED (0 << 12) Disable instruction cache. We are going to disable all caches for simplicity. You can find more information about data and instruction caches here . #define SCTLR_D_CACHE_DISABLED (0 << 2) Disable data cache. #define SCTLR_MMU_DISABLED (0 << 0) Disable MMU. MMU must be disabled until the lesson 6, where we are going to prepare page tables and start working with virtual memory. FYI - official doc","title":"SCTLR_EL1, System Control Register (EL1)"},{"location":"lesson02/rpi-os/#hcr_el2-hypervisor-configuration-el2","text":"ldr x0, =HCR_VALUE msr hcr_el2, x0 We are NOT going to implement our own hypervisor . Still we need to use this register. Among other settings, bit 31 (RW) controls the execution state at EL1, being AArch64 (1) or AArch32 (0). This register also controls at which EL we will handle IRQ. In sysregs.h we set HCR_VALUE to be (1<<31). Official doc","title":"HCR_EL2, Hypervisor Configuration (EL2)"},{"location":"lesson02/rpi-os/#scr_el3-secure-configuration-el3","text":"ldr x0, =SCR_VALUE msr scr_el3, x0 This register is responsible for configuring security settings. For example, it controls whether all lower levels are executed in \"secure\" or \"nonsecure\" world. It also controls execution state at EL2. Here we set that EL2 will execute at AArch64 state, and all lower exception levels will be \"non secure\". This register has no counterpart at EL2. Therefore, we don't have to set it on qemu emulation. Official doc","title":"SCR_EL3, Secure Configuration (EL3)"},{"location":"lesson02/rpi-os/#spsr_el3-saved-program-status-el3","text":"ldr x0, =SPSR_VALUE msr spsr_el3, x0 spsr_el3 contains CPU state, that will be restored after we execute eret instruction. What is CPU state? It consists of the following information: Condition Flags Those flags contains information about previously executed executions: whether the result was negative (N flag), zero (A flag), has unsigned overflow (C flag) or has signed overflow (V flag). Values of those flags can be used in conditional branch instructions. For example, b.eq instruction will jump to the provided label only if the result of the last comparison operation is equal to 0. The processor checks this by testing whether Z flag is set to 1. Interrupt disable bits Those bits allows to enable/disable different types of interrupts. EL & other information , required to fully restore the processor execution state after an exception is handled. Usually spsr_el3 is saved automatically by CPU hardware, when an exception is taken to EL3. Furthermore, this register is writable by our code, so we take advantage of this fact and manually prepare CPU state. SPSR_VALUE is prepared here and we initialize the following fields: #define SPSR_MASK_ALL (7 << 6) After we change EL to EL1 all types of interrupts will be masked (or disabled, which is the same). #define SPSR_EL1h (5 << 0) This indicates to which EL the eret instruction will take the CPU to. It's EL1. About EL1h: At EL1 we can either use our own dedicated stack pointer or use EL0 stack pointer. EL1h mode means that we are using EL1 dedicated stack pointer. Official doc","title":"SPSR_EL3, Saved Program Status (EL3)"},{"location":"lesson02/rpi-os/#elr_el3-exception-link-el3","text":"adr x0, el1_entry msr elr_el3, x0 eret elr_el3 holds the address, to which we are going to return after eret instruction will be executed. Here we set this address to the location of el1_entry label. Official doc","title":"ELR_EL3, Exception Link (EL3)"},{"location":"lesson02/rpi-os/#conclusion","text":"That is pretty much it: when we enter el1_entry function the execution should be already at EL1 mode. Our subsequent experiments will switch between EL1 (kernel) and EL0 (user) frequently. Go ahead and try it out!","title":"Conclusion"},{"location":"lesson03/exercises/","text":"Exercises Change the reset value of TVAL so that the timer fires roughly every 1 sec. How can you determine such a value and show your solution is correct? Trigger an exception at EL1, handle it at EL2, and return to EL1. Modify the kernel source to implement the behavior. In a paragraph, describe the exception you plan to trigger, what EL2 exception handlers you will implement, and how to demonstrate your implementation correctness. Deliverable A code tarball implementing (1) above. A code tarball implementing (3) above. A docx or PDF file answering (2) (4) above.","title":"Exercises"},{"location":"lesson03/exercises/#exercises","text":"Change the reset value of TVAL so that the timer fires roughly every 1 sec. How can you determine such a value and show your solution is correct? Trigger an exception at EL1, handle it at EL2, and return to EL1. Modify the kernel source to implement the behavior. In a paragraph, describe the exception you plan to trigger, what EL2 exception handlers you will implement, and how to demonstrate your implementation correctness.","title":"Exercises"},{"location":"lesson03/exercises/#deliverable","text":"A code tarball implementing (1) above. A code tarball implementing (3) above. A docx or PDF file answering (2) (4) above.","title":"Deliverable"},{"location":"lesson03/rpi-os/","text":"3: Interrupts Objectives We will build a baremetal program that prints out messages, as driven by periodic interrupts from a hardware timer. You will learn and experience with: Exception/interrupt vectors Handling interrupts Program hardware timers Terms \"Interrupts\" or \"irq\"? We use these two terms interchangeably. Many kernel documents use the latter. Background: interrupts & exceptions in ARM64 By their canonical definitions, interrupts are asynchronous while exceptions are synchronous. However in ARM64 lingo, exception is broadly defined; interrupts are a special kind of exceptions. x86 has its own lingo, calling exceptions as \"traps\". In this article, we use ARM's broad definition of exceptions unless stated otherwise. Exception types ARM64 defines 4 types of exceptions. We will focus the former two . Synchronous exception s Exceptions of this type are always caused by the currently executed instruction. For example, you can use str instruction to store some data at a non-existing memory location. In this case, a synchronous exception is generated. Synchronous exceptions also can be used to generate a \"software interrupt\". Software interrupt is a synchronous exception that is generated on purpose by svc instruction. We will use this technique in lesson 5 to implement system calls. Asynchronous exceptions (IRQ) Those are normal interrupts. They are always asynchronous, which means that they have nothing to do with the currently executed instruction. In contrast to synchronous exceptions, they are always not generated by the processor itself, but by external hardware. FIQ (Fast Interrupt Request) This type of exception is called \"fast interrupts\" and exist solely for the purpose of prioritizing exceptions. It is possible to configure some interrupts as \"normal\" and other as \"fast\". Fast interrupts will be signaled first and will be handled by a separate exception handler. Linux doesn't use fast interrupts and we also are not going to do so. SError (System Error) Like IRQ and FIQ , SError exceptions are asynchronous and are generated by external hardware. Unlike IRQ and FIQ , SError always indicates some error condition. Here you can find an example explaining when SError can be generated. Exception vectors An exception vector is a piece of code the CPU will execute when a specific exception happens. \" These would normally be branch instructions that direct the core to the full exception handler. \" (the ARM64 manual). In some other architectures an exception vector could be an address to jump to. Note the subtle difference. Each exception level (EL) has its own vector table. Here we focus on EL1 where the kernel executes. The kernel must provide exception handlers to be executed at EL1 in order to handle exceptions from EL0 (the user programs) or EL1 (its own execution). These should be handlers for each exception type above (SError, fiq, irq, and sync) and for each of the four execution states of the CPU: EL1t Exception happens when CPU is at EL1 while the stack pointer (SP) was set to be shared with EL0. This happens when SPSel register holds the value 0 . Recall that SPSel is part of the CPU's PSTATE. EL1h Exception happens at EL1 at the time when a dedicated SP was allocated for EL1. This happens when SPSel holds the value 1 . This is the mode that our kernel is are currently using. EL0_64 Exception is taken from EL0 executing in 64-bit mode. This experiment will not deal with EL0. Spoiler: EL0_64 corresponds to the exceptions that caused by 64-bit user programs. EL0_32 Exception is taken from EL0 executing in 32-bit mode. This experiment will not deal with EL0 or 32-bit mode. Spoiler: this corresponds to exceptions in 32-bit user programs. \"The t and h suffixes are based on the terminology of thread and handler , introduced in ARMv7-M.\" -- ARM In total, for EL1 the kernel needs to define 16 exception handlers (4 types X 4 execution states) ARM64 vector table Each exception vector (or handler) is a continuous sequence of instructions responsible for handling a particular exception. Each exception vector can occupy 0x80 bytes maximum. This is not much, but nobody prevents us from jumping to some other memory location from an exception vector. The vector table is an array of exception vectors. Note each EL has its own table as described above. Here is a short, good reference from Arm. Code Walkthrough Exception vectors, tables, etc. Everything related to exception handling is defined in entry.S . The code mimics what the ARM64 Linux kernel does. Why named \"entry\"? Because in a full-fledged kernel, exception/irq handlers are where user programs enter the kernel for execution. Although this experiment is not building such a kernel, we follow the naming convention. The first macro ventry and it is used to create entries in the vector table. .macro ventry label .align 7 b \\label .endm As suggested above: for code clarity, we are not going to handle exceptions right inside the exception vector. Instead, we make each vector a branch instruction ( b \\label ) that jumps to a label provided for the macro as label argument. We need .align 7 because all exception vectors should be spaced at 0x80 bytes (2^7) one from another. A useful assembly trick. The vector table is defined here and it consists of 16 ventry definitions. Making CPU aware of the vector table Ok, now we have prepared the vector table, but the processor doesn't know where it is located and therefore can't use it. In order for the exception handling to work, we must set vbar_el1 (Vector Base Address Register) to the vector table address. This is done here . .globl irq_vector_init irq_vector_init: adr x0, vectors msr vbar_el1, x0 ret A simple handler for handling unexpected exceptions In this experiment we are only interested in handling IRQ from EL1h . Yet, our kernel defines all 16 handlers for EL1. This is for debugging ease: we want to print out meaningful message in case our kernel triggers some other exceptions due to our programming mistakes. Note again: all these handlers are to be executed at EL1. The exceptions come from either EL0 or EL1. We name all the handlers that are NOT supposed to be trigged with a invalid postfix. We implement these handlers using a handle_invalid_entry macro: .macro handle_invalid_entry type kernel_entry mov x0, #\\type mrs x1, esr_el1 mrs x2, elr_el1 bl show_invalid_entry_message b err_hang .endm The first line invokes a macro kernel_entry which is the first few instructions the kernel should execute in handling an exception/interrupt (recall the term \"entry\"). We will discuss it below. Then we call show_invalid_entry_message and prepare 3 arguments for it. The arguments are passed in 3 registers: x0, x1, and x2. x0: the exception type. The value comes from the argument to this macro. It can take one of these values defined by our kernel code. It tells us exactly which exception handler has been executed. x1: information about what causes the exception. The value comes from esr_el1 register. ESR stands for Exception Syndrome Register. EL1 implies \"when an exception is taken to EL1\", i.e. when the exception is handled at EL1. Note: in this experiment our kernel runs at EL1 and when an interrupt happens it is handled at EL1. Read the ref again. x2: the address of the instruction being executed when the exception happens. The value comes from the elr_el1 as described earlier. For synchronous exceptions, this is the instruction that causes the exception; for irqs (asynchronous), this is the instruction completed right before irq happens. Again, the postfix EL1 indicates that \" when taking an exception to EL1, (this reg) holds the address to return to. \" The code next invokes show_invalid_entry_message function, which prints textual information to UART. Returning from that function, the code executes in an infinite loop as we have nothing else to do. kernel_entry & exit To handle valid exceptions (timer interrupts in our case), the kernel needs to save & restore the context of the \"normal\" execution, i.e. switching from the normal execution to the exception handler, executing it, and resuming the execution being interrupted. In other words, after the exception handler, we want all general purpose registers to have the same values as they had before the exception was generated. Why does NOT the above handler handle_invalid_entry save registers? Because it ends with an infinite loop and never intends to resume the interrupted execution. el1_irq: kernel_entry bl handle_irq kernel_exit Back to kernel_entry . This is the first thing to do in handling an exception: saving the processor state, notably registers x0 - x30, to the stack. To do so, it first subtracts from sp the size of total stored registers (#S_FRAME_SIZE) and then fills the stack space. According to kernel_entry , there is kernel_exit to be called as the last thing of an exception handler. kernel_exit restores the CPU state by copying back the values of x0 - x30. The order exactly mirrors that of kernel_entry otherwise we will see wrong register values. Finally kernel_exit executes eret , which returns to the normal execution. Note: general purpose registers are not the only thing to be saved for kernel_entry/exit . Doing so is enough for our simple kernel for now. More on them in subsequent experiments. Working with interrupts Configuring the interrupt controller Interrupts are generated by IO devices, go through the irq controller, and eventually arrive the CPU. The CPU can program the irq controller to enable/disable specific interrupt sources. By disabling an irq source, the CPU will not lose any irq from that device, but just defer receiving irq until the CPU re-enables the irq source. The CPU can also read from the irq controller which IO devices have pending interrupts, meaning that the IO devices need attention. Bcm2837, the SoC for Rpi3, has its own interrupt controller described on page 109 of BCM2837 ARM Peripherals manual . Because of the hardware quirks (e.g. many irqs are routed from GPU to CPU), the interrupt controller organizes irq sources into three groups and has registers for controlling/checking individual groups. Be aware of their weird naming: these irq groups are called \"Basic\" (irqs routed to the ARM CPU), \"1\", and \"2\" (irqs routed from GPU to CPU). For example, IRQ basic pending , IRQ pending 1 , IRQ pending 2 . The SoC manual has more dirty details. We are only interested in timer interrupts. The SoC manual, page 113 states that irq #1 and #3 are from the system timer. These irq sources belong to the irq group 1, which can be enabled using ENABLE_IRQS_1 . So here is the function that enables system timer IRQ at #1. void enable_interrupt_controller() { put32(ENABLE_IRQS_1, SYSTEM_TIMER_IRQ_1); } Masking/unmasking interrupts From time to time, the kernel must mask/unmask ALL interrupts, so that some critical code regions will never be interrupted. For example, what happens if an interrupt occurs right in the middle of kernel_entry macro? The CPU state would be corrupted. Upon entry to ANY exception/interrupt, the processor automatically masks all interrupts so that the kernel can save the CPU state atomically. The kernel then unmasks exceptions (often interrupts) it wants to handle during the execution of the interrupt handler. Right before exiting the exception handling ( eret ), the kernel masks all interrupts again for atomic CPU state restore. Note: it is perfectly legal to have nested interrupts, i.e. handling another interrupt in the middle of an interrupt handler. Nested interrupts are NOT common: for simple designs, many kernels intentionally keep interrupt handlers very short so they can mask interrupts throughout an interrupt handler without delaying future interrupts too much. However, handling interrupts during exception handlers is VERY common. Syscalls are executed as exception handlers, during which the kernel must be responsive to interrupts. The following two functions mask and unmask interrupts. .globl enable_irq enable_irq: msr daifclr, #2 ret .globl disable_irq disable_irq: msr daifset, #2 ret ARM processor state (PSTATE) has 4 bits holding mask status for different types of interrupts. D Masks debug exceptions. These are a special type of synchronous exceptions. For obvious reasons, it is not possible to mask all synchronous exceptions, but it is convenient to have a separate flag that can mask debug exceptions. A Masks SErrors . It is called A because SErrors sometimes are called asynchronous aborts. I Masks IRQs F Masks FIQs Now you can probably guess why registers that are responsible for changing interrupt mask status are called daifclr and daifset . Those registers set and clear interrupt mask status bits in the processor state. Why do we use constant value 2 in both of the functions? This is because we only want to set and clear the second ( I ) bit. The IRQ handler We have a single, common exception handler for handling all IRQs . This handler is defined here . void handle_irq(void) { unsigned int irq = get32(IRQ_PENDING_1); switch (irq) { case (SYSTEM_TIMER_IRQ_1): handle_timer_irq(); break; default: printf(\"Unknown pending irq: %x\\r\\n\", irq); } } In the handler, we need a way to figure out what IO device generated the interrupt. Interrupt controller can help us with this job: it has IRQ_PENDING_1 register that holds interrupt status for interrupts 0 - 31 . Using this register we can check whether the current interrupt was generated by the timer or by some other device and call device specific interrupt handler. Note, multiple interrupts can be pending at the same time. That's why each device specific interrupt handler must acknowledge that it completed handling the interrupt and only after that interrupt pending bit in IRQ_PENDING_1 will be cleared. Because of the same reason, for a production kernel you would probably want to wrap switch construct in the interrupt handler in a loop: in this way, you will be able to handle multiple interrupts during a single handler execution. Arm's generic hardware timer We use the Arm generic timer, which is part of Arm64 core design (i.e. not defined by SoC). This is nice, as the generic timers exist for all Armv8 CPUs. Your experiences will apply to other Armv8 SoCs as well. Arm's official webpage (ARM062-1010708621-30) describes the use of generic timers. The following figure shows the generic timer hardware. In a nutshell, a global, chip-level hardware counter (i.e. \"System Counter\") drives per-core timer instances. As hardware boots, System Counter keeps incrementing, i.e. free running. Software can read the current System Counter. But System Counter alone does not generate interrupts. Software must program the timers so that they interrupt corresponding CPU cores at specific time intervals. Note: PE means CPU cores. As our kernel only deals with one core, we focus on one timer instance. How should the kernel program the timer? The timer provides two core registers (among others) as two alternative ways for programming the same timer. They are intuitive: CVAL, a 64-bit comparator. Roughly, this sets a \"threshold\" for System Counter: Example: The kernel writes a value X to CVAL. When System Counter exceeds X, the timer generates an interrupt. TVAL, a 32-bit timer value. Roughly, this sets a \"delta\" for System Counter: Example: The kernel writes a value X to TVAL. The hardware updates CVAL += the Current System Counter + TVAL. The timer generates an interrupt according to the new CVAL. The above brief description would suffice in our kernel experiment. Beyond them, TVAL has another less intuitive, \"countdown\" function (not used in this experiment but good to know). Since the last write by software, TVAL decrements as System Counter increments. The moment TVAL counts down to 0 is when an interrupt fires. After that, TVAL will keep counting down to a minus value. To summarize: If software needs a timer event in X ticks of the clock, the software can write X to TVAL periodically. Alternatively, if software wants an event when the system count reaches Y, software can write Y to CVAL. If software wants to know the remaining ticks until the next interrupt, the software reads from TVAL. Initialize timer (timer.S) By programming the timer device, We turn on the timer and allow it to generate interrupts. gen_timer_init: mov x0, #1 msr CNTP_CTL_EL0, x0 ret This writes 1 to the control register ( CNTP_CTL_EL0 ) of the EL1 physical timer . See here for the register definition. How to interpret the register name \"CNTP_CTL_EL0\": CTL indicates this is a control register; CNTP_XXX_EL0 indicates that this is for the EL1 physical timer. Why _EL0? I guess it means that the timer is accessible to both EL1 and EL0. See the table below. Register Purpose <timer>_CTL_EL<x> Control register <timer>_CVAL_EL<x> Comparator value <timer>_TVAL_EL<x> Timer value Timer Register prefix EL<x> EL1 physical timer CNTP EL0 EL1 virtual time CNTV EL0 Non-secure EL2 physical timer CNTHP EL2 Non-secure EL2 virtual timer CNTHV EL2 EL3 physical timer CNTPS EL1 Secure EL2 physical timer CNTHPS EL2 Secure EL2 virtual timer CNTHVS EL2 Turn on timer interrupt at the CPU core We have to deal with yet another Rpi3 quirk. The Arm generic timer IRQs are wired to a per-core interrupt controller/register. For core 0, this is TIMER_INT_CTRL_0 at 0x40000040; bit 1 is for physical timer at EL1 (CNTP). This register is documented in the manual of BCM2836 (search for \"Core timers interrupts\"). Note the manual is NOT for the BCM2837 SoC used by Rpi3. I have no idea how community figured this out. void enable_interrupt_controller() { // Enables Core 0 Timers interrupt control for the generic timer put32(TIMER_INT_CTRL_0, TIMER_INT_CTRL_0_VALUE); } To summarize : we have to program three places in order to receive the timer interrupts: the timer device, the per-core interrupt controller, and the core itself (DAIF). Handing timer interrupts The kernel gets an irq. The kernel check if it comes from the timer; if so, the kernel sets the timer for firing the next interrupt. void handle_irq(void) { // Each Core has its own pending local intrrupts register unsigned int irq = get32(INT_SOURCE_0); switch (irq) { case (GENERIC_TIMER_INTERRUPT): handle_generic_timer_irq(); break; ... The EL1h exception handler invokes the above function. The function reads INT_SOURCE_0 (0x4000:0060), search for \"Core interrupt sources\" in the BCM2836 manual ), where bit 1 is for our CNTP timer. Reset timer (timer.S) The kernel writes a delta value (1<<24) to TVAL, requesting an interrupt to fire after 1<<24 ticks. gen_timer_reset: mov x0, #1 lsl x0, x0, #24 msr CNTP_TVAL_EL0, x0 ret Timers on Rpi3 There are other timers on Rpi3 which you may see from various online blogs/tutorials/forums. The information can be very confusing. The naming of timers does NOT help. I list them below together with Arm generic timers described above. I suggest you stay away from other timers because the experience will not be as useful. Name Implemented by IRQ QEMU support? (v5.0 ) Phys Addr Document System Timer Broadcom (?) Global. In GPU irq space Implemented as bcm2835_systmr. However free running and cannot generate irq . 3f003000 BCM2837 ARM timer Arm ip (sp804) Global. In Arm core's private irq space (\"Basic irqs\") Unimplemented. See QEMU code bcm2835_peripherals.c 3f00b400 BCM2836 Local timer Broadcom (?) Per core Partially implemented . Can generate trigger irq but readback seems unsupported. 40000034 BCM2836 Arm generic timer Arm, as part of armv8 Per core Implemented 40000040 Armv8 doc + BCM2836 for IRQ routing FYI: Programming the Rpi3's system timer (not used in this experiment) Raspberry Pi system timer is a very simple device. It has a counter that increases its value by 1 after each clock tick. It also has 4 interrupt lines that connect to the interrupt controller (so it can generate 4 different interrupts) and 4 corresponding compare registers. When the value of the counter becomes equal to the value stored in one of the compare registers the corresponding interrupt is fired. That's why, before we will be able to use system timer interrupts, we need to initialize one of the compare registers with a non-zero value, the larger the value is - the later an interrupt will be generated. This is done in timer_init function. const unsigned int interval = 200000; unsigned int curVal = 0; void timer_init ( void ) { curVal = get32(TIMER_CLO); curVal += interval; put32(TIMER_C1, curVal); } The first line reads current counter value, the second line increases it and the third line sets the value of the compare register for the interrupt number 1. By manipulating interval value you can adjust how soon the first timer interrupt will be generated. Finally, we got to the timer interrupt handler. It is actually very simple. void handle_timer_irq( void ) { curVal += interval; put32(TIMER_C1, curVal); put32(TIMER_CS, TIMER_CS_M1); printf(\"Timer iterrupt received\\n\\r\"); } Here we first update compare register so that that next interrupt will be generated after the same time interval. Next, we acknowledge the interrupt by writing 1 to the TIMER_CS register. In the documentation TIMER_CS is called \"Timer Control/Status\" register. Bits [0:3] of this register can be used to acknowledge interrupts coming from one of the 4 available interrupt lines. Hacking tips-- observe interrupts with QEMU** qemu-system-aarch64 -M raspi3 -kernel ./kernel8.img -serial null -serial stdio -d int -D test.log Explanation: -d int ---> enable interrupt dedug -D test.log ----> put debug msg to a file test.log Conclusion The last thing that you might want to take a look at is the kernel_main function where all previously discussed functionality is orchestrated. After you compile and run the sample it should print \"Timer interrupt received\" message after an interrupt is taken. Please, try to do it by yourself and don't forget to carefully examine the code and experiment with it.","title":"3: Interrupts"},{"location":"lesson03/rpi-os/#3-interrupts","text":"","title":"3: Interrupts"},{"location":"lesson03/rpi-os/#objectives","text":"We will build a baremetal program that prints out messages, as driven by periodic interrupts from a hardware timer. You will learn and experience with: Exception/interrupt vectors Handling interrupts Program hardware timers","title":"Objectives"},{"location":"lesson03/rpi-os/#terms","text":"\"Interrupts\" or \"irq\"? We use these two terms interchangeably. Many kernel documents use the latter.","title":"Terms"},{"location":"lesson03/rpi-os/#background-interrupts-exceptions-in-arm64","text":"By their canonical definitions, interrupts are asynchronous while exceptions are synchronous. However in ARM64 lingo, exception is broadly defined; interrupts are a special kind of exceptions. x86 has its own lingo, calling exceptions as \"traps\". In this article, we use ARM's broad definition of exceptions unless stated otherwise.","title":"Background: interrupts &amp; exceptions in ARM64"},{"location":"lesson03/rpi-os/#exception-types","text":"ARM64 defines 4 types of exceptions. We will focus the former two . Synchronous exception s Exceptions of this type are always caused by the currently executed instruction. For example, you can use str instruction to store some data at a non-existing memory location. In this case, a synchronous exception is generated. Synchronous exceptions also can be used to generate a \"software interrupt\". Software interrupt is a synchronous exception that is generated on purpose by svc instruction. We will use this technique in lesson 5 to implement system calls. Asynchronous exceptions (IRQ) Those are normal interrupts. They are always asynchronous, which means that they have nothing to do with the currently executed instruction. In contrast to synchronous exceptions, they are always not generated by the processor itself, but by external hardware. FIQ (Fast Interrupt Request) This type of exception is called \"fast interrupts\" and exist solely for the purpose of prioritizing exceptions. It is possible to configure some interrupts as \"normal\" and other as \"fast\". Fast interrupts will be signaled first and will be handled by a separate exception handler. Linux doesn't use fast interrupts and we also are not going to do so. SError (System Error) Like IRQ and FIQ , SError exceptions are asynchronous and are generated by external hardware. Unlike IRQ and FIQ , SError always indicates some error condition. Here you can find an example explaining when SError can be generated.","title":"Exception types"},{"location":"lesson03/rpi-os/#exception-vectors","text":"An exception vector is a piece of code the CPU will execute when a specific exception happens. \" These would normally be branch instructions that direct the core to the full exception handler. \" (the ARM64 manual). In some other architectures an exception vector could be an address to jump to. Note the subtle difference. Each exception level (EL) has its own vector table. Here we focus on EL1 where the kernel executes. The kernel must provide exception handlers to be executed at EL1 in order to handle exceptions from EL0 (the user programs) or EL1 (its own execution). These should be handlers for each exception type above (SError, fiq, irq, and sync) and for each of the four execution states of the CPU: EL1t Exception happens when CPU is at EL1 while the stack pointer (SP) was set to be shared with EL0. This happens when SPSel register holds the value 0 . Recall that SPSel is part of the CPU's PSTATE. EL1h Exception happens at EL1 at the time when a dedicated SP was allocated for EL1. This happens when SPSel holds the value 1 . This is the mode that our kernel is are currently using. EL0_64 Exception is taken from EL0 executing in 64-bit mode. This experiment will not deal with EL0. Spoiler: EL0_64 corresponds to the exceptions that caused by 64-bit user programs. EL0_32 Exception is taken from EL0 executing in 32-bit mode. This experiment will not deal with EL0 or 32-bit mode. Spoiler: this corresponds to exceptions in 32-bit user programs. \"The t and h suffixes are based on the terminology of thread and handler , introduced in ARMv7-M.\" -- ARM In total, for EL1 the kernel needs to define 16 exception handlers (4 types X 4 execution states)","title":"Exception vectors"},{"location":"lesson03/rpi-os/#arm64-vector-table","text":"Each exception vector (or handler) is a continuous sequence of instructions responsible for handling a particular exception. Each exception vector can occupy 0x80 bytes maximum. This is not much, but nobody prevents us from jumping to some other memory location from an exception vector. The vector table is an array of exception vectors. Note each EL has its own table as described above. Here is a short, good reference from Arm.","title":"ARM64 vector table"},{"location":"lesson03/rpi-os/#code-walkthrough","text":"","title":"Code Walkthrough"},{"location":"lesson03/rpi-os/#exception-vectors-tables-etc","text":"Everything related to exception handling is defined in entry.S . The code mimics what the ARM64 Linux kernel does. Why named \"entry\"? Because in a full-fledged kernel, exception/irq handlers are where user programs enter the kernel for execution. Although this experiment is not building such a kernel, we follow the naming convention. The first macro ventry and it is used to create entries in the vector table. .macro ventry label .align 7 b \\label .endm As suggested above: for code clarity, we are not going to handle exceptions right inside the exception vector. Instead, we make each vector a branch instruction ( b \\label ) that jumps to a label provided for the macro as label argument. We need .align 7 because all exception vectors should be spaced at 0x80 bytes (2^7) one from another. A useful assembly trick. The vector table is defined here and it consists of 16 ventry definitions.","title":"Exception vectors, tables, etc."},{"location":"lesson03/rpi-os/#making-cpu-aware-of-the-vector-table","text":"Ok, now we have prepared the vector table, but the processor doesn't know where it is located and therefore can't use it. In order for the exception handling to work, we must set vbar_el1 (Vector Base Address Register) to the vector table address. This is done here . .globl irq_vector_init irq_vector_init: adr x0, vectors msr vbar_el1, x0 ret","title":"Making CPU aware of the vector table"},{"location":"lesson03/rpi-os/#a-simple-handler-for-handling-unexpected-exceptions","text":"In this experiment we are only interested in handling IRQ from EL1h . Yet, our kernel defines all 16 handlers for EL1. This is for debugging ease: we want to print out meaningful message in case our kernel triggers some other exceptions due to our programming mistakes. Note again: all these handlers are to be executed at EL1. The exceptions come from either EL0 or EL1. We name all the handlers that are NOT supposed to be trigged with a invalid postfix. We implement these handlers using a handle_invalid_entry macro: .macro handle_invalid_entry type kernel_entry mov x0, #\\type mrs x1, esr_el1 mrs x2, elr_el1 bl show_invalid_entry_message b err_hang .endm The first line invokes a macro kernel_entry which is the first few instructions the kernel should execute in handling an exception/interrupt (recall the term \"entry\"). We will discuss it below. Then we call show_invalid_entry_message and prepare 3 arguments for it. The arguments are passed in 3 registers: x0, x1, and x2. x0: the exception type. The value comes from the argument to this macro. It can take one of these values defined by our kernel code. It tells us exactly which exception handler has been executed. x1: information about what causes the exception. The value comes from esr_el1 register. ESR stands for Exception Syndrome Register. EL1 implies \"when an exception is taken to EL1\", i.e. when the exception is handled at EL1. Note: in this experiment our kernel runs at EL1 and when an interrupt happens it is handled at EL1. Read the ref again. x2: the address of the instruction being executed when the exception happens. The value comes from the elr_el1 as described earlier. For synchronous exceptions, this is the instruction that causes the exception; for irqs (asynchronous), this is the instruction completed right before irq happens. Again, the postfix EL1 indicates that \" when taking an exception to EL1, (this reg) holds the address to return to. \" The code next invokes show_invalid_entry_message function, which prints textual information to UART. Returning from that function, the code executes in an infinite loop as we have nothing else to do.","title":"A simple handler for handling unexpected exceptions"},{"location":"lesson03/rpi-os/#kernel_entry-exit","text":"To handle valid exceptions (timer interrupts in our case), the kernel needs to save & restore the context of the \"normal\" execution, i.e. switching from the normal execution to the exception handler, executing it, and resuming the execution being interrupted. In other words, after the exception handler, we want all general purpose registers to have the same values as they had before the exception was generated. Why does NOT the above handler handle_invalid_entry save registers? Because it ends with an infinite loop and never intends to resume the interrupted execution. el1_irq: kernel_entry bl handle_irq kernel_exit Back to kernel_entry . This is the first thing to do in handling an exception: saving the processor state, notably registers x0 - x30, to the stack. To do so, it first subtracts from sp the size of total stored registers (#S_FRAME_SIZE) and then fills the stack space. According to kernel_entry , there is kernel_exit to be called as the last thing of an exception handler. kernel_exit restores the CPU state by copying back the values of x0 - x30. The order exactly mirrors that of kernel_entry otherwise we will see wrong register values. Finally kernel_exit executes eret , which returns to the normal execution. Note: general purpose registers are not the only thing to be saved for kernel_entry/exit . Doing so is enough for our simple kernel for now. More on them in subsequent experiments.","title":"kernel_entry &amp; exit"},{"location":"lesson03/rpi-os/#working-with-interrupts","text":"","title":"Working with interrupts"},{"location":"lesson03/rpi-os/#configuring-the-interrupt-controller","text":"Interrupts are generated by IO devices, go through the irq controller, and eventually arrive the CPU. The CPU can program the irq controller to enable/disable specific interrupt sources. By disabling an irq source, the CPU will not lose any irq from that device, but just defer receiving irq until the CPU re-enables the irq source. The CPU can also read from the irq controller which IO devices have pending interrupts, meaning that the IO devices need attention. Bcm2837, the SoC for Rpi3, has its own interrupt controller described on page 109 of BCM2837 ARM Peripherals manual . Because of the hardware quirks (e.g. many irqs are routed from GPU to CPU), the interrupt controller organizes irq sources into three groups and has registers for controlling/checking individual groups. Be aware of their weird naming: these irq groups are called \"Basic\" (irqs routed to the ARM CPU), \"1\", and \"2\" (irqs routed from GPU to CPU). For example, IRQ basic pending , IRQ pending 1 , IRQ pending 2 . The SoC manual has more dirty details. We are only interested in timer interrupts. The SoC manual, page 113 states that irq #1 and #3 are from the system timer. These irq sources belong to the irq group 1, which can be enabled using ENABLE_IRQS_1 . So here is the function that enables system timer IRQ at #1. void enable_interrupt_controller() { put32(ENABLE_IRQS_1, SYSTEM_TIMER_IRQ_1); }","title":"Configuring the interrupt controller"},{"location":"lesson03/rpi-os/#maskingunmasking-interrupts","text":"From time to time, the kernel must mask/unmask ALL interrupts, so that some critical code regions will never be interrupted. For example, what happens if an interrupt occurs right in the middle of kernel_entry macro? The CPU state would be corrupted. Upon entry to ANY exception/interrupt, the processor automatically masks all interrupts so that the kernel can save the CPU state atomically. The kernel then unmasks exceptions (often interrupts) it wants to handle during the execution of the interrupt handler. Right before exiting the exception handling ( eret ), the kernel masks all interrupts again for atomic CPU state restore. Note: it is perfectly legal to have nested interrupts, i.e. handling another interrupt in the middle of an interrupt handler. Nested interrupts are NOT common: for simple designs, many kernels intentionally keep interrupt handlers very short so they can mask interrupts throughout an interrupt handler without delaying future interrupts too much. However, handling interrupts during exception handlers is VERY common. Syscalls are executed as exception handlers, during which the kernel must be responsive to interrupts. The following two functions mask and unmask interrupts. .globl enable_irq enable_irq: msr daifclr, #2 ret .globl disable_irq disable_irq: msr daifset, #2 ret ARM processor state (PSTATE) has 4 bits holding mask status for different types of interrupts. D Masks debug exceptions. These are a special type of synchronous exceptions. For obvious reasons, it is not possible to mask all synchronous exceptions, but it is convenient to have a separate flag that can mask debug exceptions. A Masks SErrors . It is called A because SErrors sometimes are called asynchronous aborts. I Masks IRQs F Masks FIQs Now you can probably guess why registers that are responsible for changing interrupt mask status are called daifclr and daifset . Those registers set and clear interrupt mask status bits in the processor state. Why do we use constant value 2 in both of the functions? This is because we only want to set and clear the second ( I ) bit.","title":"Masking/unmasking interrupts"},{"location":"lesson03/rpi-os/#the-irq-handler","text":"We have a single, common exception handler for handling all IRQs . This handler is defined here . void handle_irq(void) { unsigned int irq = get32(IRQ_PENDING_1); switch (irq) { case (SYSTEM_TIMER_IRQ_1): handle_timer_irq(); break; default: printf(\"Unknown pending irq: %x\\r\\n\", irq); } } In the handler, we need a way to figure out what IO device generated the interrupt. Interrupt controller can help us with this job: it has IRQ_PENDING_1 register that holds interrupt status for interrupts 0 - 31 . Using this register we can check whether the current interrupt was generated by the timer or by some other device and call device specific interrupt handler. Note, multiple interrupts can be pending at the same time. That's why each device specific interrupt handler must acknowledge that it completed handling the interrupt and only after that interrupt pending bit in IRQ_PENDING_1 will be cleared. Because of the same reason, for a production kernel you would probably want to wrap switch construct in the interrupt handler in a loop: in this way, you will be able to handle multiple interrupts during a single handler execution.","title":"The IRQ handler"},{"location":"lesson03/rpi-os/#arms-generic-hardware-timer","text":"We use the Arm generic timer, which is part of Arm64 core design (i.e. not defined by SoC). This is nice, as the generic timers exist for all Armv8 CPUs. Your experiences will apply to other Armv8 SoCs as well. Arm's official webpage (ARM062-1010708621-30) describes the use of generic timers. The following figure shows the generic timer hardware. In a nutshell, a global, chip-level hardware counter (i.e. \"System Counter\") drives per-core timer instances. As hardware boots, System Counter keeps incrementing, i.e. free running. Software can read the current System Counter. But System Counter alone does not generate interrupts. Software must program the timers so that they interrupt corresponding CPU cores at specific time intervals. Note: PE means CPU cores. As our kernel only deals with one core, we focus on one timer instance. How should the kernel program the timer? The timer provides two core registers (among others) as two alternative ways for programming the same timer. They are intuitive: CVAL, a 64-bit comparator. Roughly, this sets a \"threshold\" for System Counter: Example: The kernel writes a value X to CVAL. When System Counter exceeds X, the timer generates an interrupt. TVAL, a 32-bit timer value. Roughly, this sets a \"delta\" for System Counter: Example: The kernel writes a value X to TVAL. The hardware updates CVAL += the Current System Counter + TVAL. The timer generates an interrupt according to the new CVAL. The above brief description would suffice in our kernel experiment. Beyond them, TVAL has another less intuitive, \"countdown\" function (not used in this experiment but good to know). Since the last write by software, TVAL decrements as System Counter increments. The moment TVAL counts down to 0 is when an interrupt fires. After that, TVAL will keep counting down to a minus value. To summarize: If software needs a timer event in X ticks of the clock, the software can write X to TVAL periodically. Alternatively, if software wants an event when the system count reaches Y, software can write Y to CVAL. If software wants to know the remaining ticks until the next interrupt, the software reads from TVAL.","title":"Arm's generic hardware timer"},{"location":"lesson03/rpi-os/#initialize-timer-timers","text":"By programming the timer device, We turn on the timer and allow it to generate interrupts. gen_timer_init: mov x0, #1 msr CNTP_CTL_EL0, x0 ret This writes 1 to the control register ( CNTP_CTL_EL0 ) of the EL1 physical timer . See here for the register definition. How to interpret the register name \"CNTP_CTL_EL0\": CTL indicates this is a control register; CNTP_XXX_EL0 indicates that this is for the EL1 physical timer. Why _EL0? I guess it means that the timer is accessible to both EL1 and EL0. See the table below. Register Purpose <timer>_CTL_EL<x> Control register <timer>_CVAL_EL<x> Comparator value <timer>_TVAL_EL<x> Timer value Timer Register prefix EL<x> EL1 physical timer CNTP EL0 EL1 virtual time CNTV EL0 Non-secure EL2 physical timer CNTHP EL2 Non-secure EL2 virtual timer CNTHV EL2 EL3 physical timer CNTPS EL1 Secure EL2 physical timer CNTHPS EL2 Secure EL2 virtual timer CNTHVS EL2","title":"Initialize timer (timer.S)"},{"location":"lesson03/rpi-os/#turn-on-timer-interrupt-at-the-cpu-core","text":"We have to deal with yet another Rpi3 quirk. The Arm generic timer IRQs are wired to a per-core interrupt controller/register. For core 0, this is TIMER_INT_CTRL_0 at 0x40000040; bit 1 is for physical timer at EL1 (CNTP). This register is documented in the manual of BCM2836 (search for \"Core timers interrupts\"). Note the manual is NOT for the BCM2837 SoC used by Rpi3. I have no idea how community figured this out. void enable_interrupt_controller() { // Enables Core 0 Timers interrupt control for the generic timer put32(TIMER_INT_CTRL_0, TIMER_INT_CTRL_0_VALUE); } To summarize : we have to program three places in order to receive the timer interrupts: the timer device, the per-core interrupt controller, and the core itself (DAIF).","title":"Turn on timer interrupt at the CPU core"},{"location":"lesson03/rpi-os/#handing-timer-interrupts","text":"The kernel gets an irq. The kernel check if it comes from the timer; if so, the kernel sets the timer for firing the next interrupt. void handle_irq(void) { // Each Core has its own pending local intrrupts register unsigned int irq = get32(INT_SOURCE_0); switch (irq) { case (GENERIC_TIMER_INTERRUPT): handle_generic_timer_irq(); break; ... The EL1h exception handler invokes the above function. The function reads INT_SOURCE_0 (0x4000:0060), search for \"Core interrupt sources\" in the BCM2836 manual ), where bit 1 is for our CNTP timer.","title":"Handing timer interrupts"},{"location":"lesson03/rpi-os/#reset-timer-timers","text":"The kernel writes a delta value (1<<24) to TVAL, requesting an interrupt to fire after 1<<24 ticks. gen_timer_reset: mov x0, #1 lsl x0, x0, #24 msr CNTP_TVAL_EL0, x0 ret","title":"Reset timer (timer.S)"},{"location":"lesson03/rpi-os/#timers-on-rpi3","text":"There are other timers on Rpi3 which you may see from various online blogs/tutorials/forums. The information can be very confusing. The naming of timers does NOT help. I list them below together with Arm generic timers described above. I suggest you stay away from other timers because the experience will not be as useful. Name Implemented by IRQ QEMU support? (v5.0 ) Phys Addr Document System Timer Broadcom (?) Global. In GPU irq space Implemented as bcm2835_systmr. However free running and cannot generate irq . 3f003000 BCM2837 ARM timer Arm ip (sp804) Global. In Arm core's private irq space (\"Basic irqs\") Unimplemented. See QEMU code bcm2835_peripherals.c 3f00b400 BCM2836 Local timer Broadcom (?) Per core Partially implemented . Can generate trigger irq but readback seems unsupported. 40000034 BCM2836 Arm generic timer Arm, as part of armv8 Per core Implemented 40000040 Armv8 doc + BCM2836 for IRQ routing","title":"Timers on Rpi3"},{"location":"lesson03/rpi-os/#fyi-programming-the-rpi3s-system-timer-not-used-in-this-experiment","text":"Raspberry Pi system timer is a very simple device. It has a counter that increases its value by 1 after each clock tick. It also has 4 interrupt lines that connect to the interrupt controller (so it can generate 4 different interrupts) and 4 corresponding compare registers. When the value of the counter becomes equal to the value stored in one of the compare registers the corresponding interrupt is fired. That's why, before we will be able to use system timer interrupts, we need to initialize one of the compare registers with a non-zero value, the larger the value is - the later an interrupt will be generated. This is done in timer_init function. const unsigned int interval = 200000; unsigned int curVal = 0; void timer_init ( void ) { curVal = get32(TIMER_CLO); curVal += interval; put32(TIMER_C1, curVal); } The first line reads current counter value, the second line increases it and the third line sets the value of the compare register for the interrupt number 1. By manipulating interval value you can adjust how soon the first timer interrupt will be generated. Finally, we got to the timer interrupt handler. It is actually very simple. void handle_timer_irq( void ) { curVal += interval; put32(TIMER_C1, curVal); put32(TIMER_CS, TIMER_CS_M1); printf(\"Timer iterrupt received\\n\\r\"); } Here we first update compare register so that that next interrupt will be generated after the same time interval. Next, we acknowledge the interrupt by writing 1 to the TIMER_CS register. In the documentation TIMER_CS is called \"Timer Control/Status\" register. Bits [0:3] of this register can be used to acknowledge interrupts coming from one of the 4 available interrupt lines.","title":"FYI: Programming the Rpi3's system timer (not used in this experiment)"},{"location":"lesson03/rpi-os/#hacking-tips-observe-interrupts-with-qemu","text":"qemu-system-aarch64 -M raspi3 -kernel ./kernel8.img -serial null -serial stdio -d int -D test.log Explanation: -d int ---> enable interrupt dedug -D test.log ----> put debug msg to a file test.log","title":"Hacking tips-- observe interrupts with QEMU**"},{"location":"lesson03/rpi-os/#conclusion","text":"The last thing that you might want to take a look at is the kernel_main function where all previously discussed functionality is orchestrated. After you compile and run the sample it should print \"Timer interrupt received\" message after an interrupt is taken. Please, try to do it by yourself and don't forget to carefully examine the code and experiment with it.","title":"Conclusion"},{"location":"lesson03/linux/interrupt_controllers/","text":"3.3: Interrupt controllers In this chapter, we are going to talk a lot about Linux drivers and how they handle interrupts. We will start with driver initialization code and then take a look at how interrupts are processed after handle_arch_irq function. Using device tree to find out needed devices and drivers When implementing interrupts in the RPi OS we have been working with 2 devices: system timer and interrupt controller. Now our goal will be to understand how the same devices work in Linux. The first thing we need to do is to find drivers that are responsible for working with mentioned devices. And in order to find needed drivers we can use bcm2837-rpi-3-b.dts device tree file. This is the top level device tree file that is specific for Raspberry Pi 3 Model B, it includes other more common device tree files, that are shared between different versions of Raspberry Pi. If you follow the chain of includes and search for timer and interrupt-controller you can find 4 devices. Local interrupt controller Local timer Global interrupt controller. It is defined here and modified here . System timer Stop, but why do we have 4 devices instead of 2? This requires some explanation, and we will tackle this question in the next section. Local vs global interrupt controllers When you think about interrupt handling in multiprocessor systems, one question you should ask yourself is which core should be responsible for processing a particular interrupt? When an interrupt occurs, are all 4 cores interrupted, or only a single one? Is it possible to route a particular interrupt to a specific core? Another question you may wonder is how one processor can notify another processor if he needs to pass some information to it? The local interrupt controller is a device that can help you in answering all those questions. It is responsible for the following tasks. Configuring which core should receive a specific interrupt. Sending interrupts between cores. Such interrupts are called \"mailboxes\" and allow cores to communicate one with each other. Handling interrupts from local timer and performance monitors interrupts (PMU). The behavior of a local interrupt controller as well as a local timer is documented in BCM2836 ARM-local peripherals manual. I already mentioned local timer several times. Now you probably wonder why do we need two independent timers in the system? I guess that the primary use-case for using the local timer is when you want to configure all 4 cores to receive timer interrupts simultaneously. If you use system timer you can only route interrupts to a single core. When working with the RPi OS we didn't work with either local interrupt controller or local timer. That is because by default local interrupt controller is configured in such a way that all external interrupts are sent to the first core, which is exactly what we need. We haven't used local timer because we use system timer instead. Local interrupt controller Accordingly to the bcm2837.dtsi the global interrupt controller is a child of the local one. Thus it makes sense to start our exploration with the local controller. If we need to find a driver that works with a particular device, we should use compatible property. Searching for the value of this property you can easily find that there is a single driver that is compatible with RPi local interrupt controller - here is the corresponding definition . IRQCHIP_DECLARE(bcm2836_arm_irqchip_l1_intc, \"brcm,bcm2836-l1-intc\", bcm2836_arm_irqchip_l1_intc_of_init); Now you can probably guess what is the procedure of a driver initialization: the kernel walks through all device definitions in the device tree and for each definition it looks for a matching driver using \"compatible\" property. If the driver is found, then its initialization function is called. Initialization function is provided during device registration, and in our case this function is bcm2836_arm_irqchip_l1_intc_of_init . static int __init bcm2836_arm_irqchip_l1_intc_of_init(struct device_node *node, struct device_node *parent) { intc.base = of_iomap(node, 0); if (!intc.base) { panic(\"%pOF: unable to map local interrupt registers\\n\", node); } bcm2835_init_local_timer_frequency(); intc.domain = irq_domain_add_linear(node, LAST_IRQ + 1, &bcm2836_arm_irqchip_intc_ops, NULL); if (!intc.domain) panic(\"%pOF: unable to create IRQ domain\\n\", node); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_CNTPSIRQ, &bcm2836_arm_irqchip_timer); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_CNTPNSIRQ, &bcm2836_arm_irqchip_timer); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_CNTHPIRQ, &bcm2836_arm_irqchip_timer); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_CNTVIRQ, &bcm2836_arm_irqchip_timer); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_GPU_FAST, &bcm2836_arm_irqchip_gpu); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_PMU_FAST, &bcm2836_arm_irqchip_pmu); bcm2836_arm_irqchip_smp_init(); set_handle_irq(bcm2836_arm_irqchip_handle_irq); return 0; } The initialization function takes 2 parameters: 'node' and 'parent', both of them are of the type struct device_node . node represents the current node in the device tree, and in our case it points here parent is a parent node in the device tree hierarchy, and for the local interrupt controller it points to soc element ( soc stands for \"system on chip\" and it is the simplest possible bus which maps all device registers directly to main memory.). node can be used to read various properties from the current device tree node. For example, the first line of the bcm2836_arm_irqchip_l1_intc_of_init function reads the device base address from reg property. However, the process is more complicated than that, because when this function is executed MMU is already enabled, and before we will be able to access some region of physical memory we must map this region to some virtual address. This is exactly what of_iomap function is doing: it reads reg property of the provided node and maps the whole memory region, described by reg property, to some virtual memory region. Next local timer frequency is initialized in bcm2835_init_local_timer_frequency function. There is nothing specific about this function: it just uses some of the registers, described in BCM2836 ARM-local peripherals manual, to initialize local timer. Next line requires some explanations. intc.domain = irq_domain_add_linear(node, LAST_IRQ + 1, &bcm2836_arm_irqchip_intc_ops, NULL); Linux assigns a unique integer number to each interrupt, you can think about this number as a unique interrupt ID. This ID is used each time you want to do something with an interrupt (for example, assign a handler, or assign which CPU should handle it). Each interrupt also has a hardware interrupt number. This is usually a number that tells which interrupt line was triggered. BCM2837 ARM Peripherals manual has the peripheral interrupt table at page 113 - you can think about an index in this table as a hardware interrupt number. So obviously we need some mechanism to map Linux irq numbers to hardware irq number and vice versa. If there is only one interrupt controller it would be possible to use one to one mapping but in general case a more sophisticated mechanism need to be used. In Linux struct irq_domain implements such mapping. Each interrupt controller driver should create its own irq domain and register all interrupts that it can handle with this domain. Registration function returns Linux irq number that later is used to work with the interrupt. Next 6 lines are responsible for registering each supported interrupt with the irq domain. bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_CNTPSIRQ, &bcm2836_arm_irqchip_timer); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_CNTPNSIRQ, &bcm2836_arm_irqchip_timer); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_CNTHPIRQ, &bcm2836_arm_irqchip_timer); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_CNTVIRQ, &bcm2836_arm_irqchip_timer); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_GPU_FAST, &bcm2836_arm_irqchip_gpu); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_PMU_FAST, &bcm2836_arm_irqchip_pmu); Accordingly to BCM2836 ARM-local peripherals manual local interrupt controller handles 10 different interrupts: 0 - 3 are interrupts from local timer, 4 - 7 are mailbox interrupts, which are used in interprocess communication, 8 corresponds to all interrupts generated by the global interrupt controller and interrupt 9 is a performance monitor interrupt. Here you can see that the driver defines a set of constants that holds hardware irq number per each interrupt. The registration code above registers all interrupts, except mailbox interrupts, which are registered separately. In order to understand the registration code better lets examine bcm2836_arm_irqchip_register_irq function. static void bcm2836_arm_irqchip_register_irq(int hwirq, struct irq_chip *chip) { int irq = irq_create_mapping(intc.domain, hwirq); irq_set_percpu_devid(irq); irq_set_chip_and_handler(irq, chip, handle_percpu_devid_irq); irq_set_status_flags(irq, IRQ_NOAUTOEN); } The first line here performs actual interrupt registration. irq_create_mapping takes hardware interrupt number as an input and returns Linux irq number. irq_set_percpu_devid configures interrupt as \"per CPU\", so that it will be handled only on the current CPU. This makes perfect sense because all interrupts that we are discussing now are local and they all can be handled only on the current CPU. irq_set_chip_and_handler , as its name suggest, sets irq chip and irq handler. Irq chip is a special struct, which needs to be created by the driver, that has methods for masking and unmasking a particular interrupt. The driver that we are examining right now defines 3 different irq chips: timer chip, PMU chip and GPU chip, which controls all interrupts generated by the external peripheral devices. Handler is a function that is responsible for processing an interrupt. In this case, the handler is set to generic handle_percpu_devid_irq function. This handler later will be rewritten by the global interrupt controller driver. irq_set_status_flags in this particular case sets a flag, indicating that the current interrupt should be enabled manually and should not be enabled by default. Going back to the bcm2836_arm_irqchip_l1_intc_of_init function, there are only 2 calls left. The first one is bcm2836_arm_irqchip_smp_init . Here mailbox interrupts are enabled, allowing processors cores to communicate with each other. The last function call is extremely important - this is the place where low-level exception handling code is connected to the driver. set_handle_irq(bcm2836_arm_irqchip_handle_irq); set_handle_irq is defined in architecture specific code and we already encountered this function. From the line above we can understand that bcm2836_arm_irqchip_handle_irq will be called by the low-level exception code. The function itself is listed below. static void __exception_irq_entry bcm2836_arm_irqchip_handle_irq(struct pt_regs *regs) { int cpu = smp_processor_id(); u32 stat; stat = readl_relaxed(intc.base + LOCAL_IRQ_PENDING0 + 4 * cpu); if (stat & BIT(LOCAL_IRQ_MAILBOX0)) { #ifdef CONFIG_SMP void __iomem *mailbox0 = (intc.base + LOCAL_MAILBOX0_CLR0 + 16 * cpu); u32 mbox_val = readl(mailbox0); u32 ipi = ffs(mbox_val) - 1; writel(1 << ipi, mailbox0); handle_IPI(ipi, regs); #endif } else if (stat) { u32 hwirq = ffs(stat) - 1; handle_domain_irq(intc.domain, hwirq, regs); } } This function reads LOCAL_IRQ_PENDING register to figure out what interrupts are currently pending. There are 4 LOCAL_IRQ_PENDING registers, each corresponding to its own processor core, that's why current processor index is used to select the right one. Mailbox interrupts and all other interrupts are processed in 2 different clauses of an if statement. The interaction between different cores of a multiprocessor system is out of scope for our current discussion, so we are going to skip mailbox interrupt handling part. Now we have only the following 2 lines left unexplained. u32 hwirq = ffs(stat) - 1; handle_domain_irq(intc.domain, hwirq, regs); This is were interrupt is passed to the next handler. First of all hardware irq number is calculated. ffs (Find first bit) function is used to do this. After hardware irq number is calculated handle_domain_irq function is called. This function uses irq domain to translate hardware irq number to Linux irq number, then checks irq configuration (it is stored in irq_desc struct) and calls an interrupt handler. We've seen that the handler was set to handle_percpu_devid_irq . However, this handler will be overwritten by the child interrupt controller later. Now, let's examine how this happens. Generic interrupt controller We have already seen how to use device tree and compatible property to find the driver corresponding to some device, so I am going to skip this part and jump straight to the generic interrupt controller driver source code. You can find it in irq-bcm2835.c file. As usual, we are going to start our exploration with the initialization function. It is called armctrl_of_init . static int __init armctrl_of_init(struct device_node *node, struct device_node *parent, bool is_2836) { void __iomem *base; int irq, b, i; base = of_iomap(node, 0); if (!base) panic(\"%pOF: unable to map IC registers\\n\", node); intc.domain = irq_domain_add_linear(node, MAKE_HWIRQ(NR_BANKS, 0), &armctrl_ops, NULL); if (!intc.domain) panic(\"%pOF: unable to create IRQ domain\\n\", node); for (b = 0; b < NR_BANKS; b++) { intc.pending[b] = base + reg_pending[b]; intc.enable[b] = base + reg_enable[b]; intc.disable[b] = base + reg_disable[b]; for (i = 0; i < bank_irqs[b]; i++) { irq = irq_create_mapping(intc.domain, MAKE_HWIRQ(b, i)); BUG_ON(irq <= 0); irq_set_chip_and_handler(irq, &armctrl_chip, handle_level_irq); irq_set_probe(irq); } } if (is_2836) { int parent_irq = irq_of_parse_and_map(node, 0); if (!parent_irq) { panic(\"%pOF: unable to get parent interrupt.\\n\", node); } irq_set_chained_handler(parent_irq, bcm2836_chained_handle_irq); } else { set_handle_irq(bcm2835_handle_irq); } return 0; } Now, let's investigate this function in more details. void __iomem *base; int irq, b, i; base = of_iomap(node, 0); if (!base) panic(\"%pOF: unable to map IC registers\\n\", node); intc.domain = irq_domain_add_linear(node, MAKE_HWIRQ(NR_BANKS, 0), &armctrl_ops, NULL); if (!intc.domain) panic(\"%pOF: unable to create IRQ domain\\n\", node); The function starts with the code that reads device base address from the device three and initializes the irq domain. This part should be already familiar to you because we have seen similar code in the local irq controller driver. for (b = 0; b < NR_BANKS; b++) { intc.pending[b] = base + reg_pending[b]; intc.enable[b] = base + reg_enable[b]; intc.disable[b] = base + reg_disable[b]; Next, there is a loop that iterates over all irq banks. We already briefly touched irq banks in the first chapter of this lesson. The interrupt controller has 3 irq banks, which are controlled by ENABLE_IRQS_1 , ENABLE_IRQS_2 and ENABLE_BASIC_IRQS registers. Each of the banks has its own enable, disable and pending registers. Enable and disable registers can be used to either enable or disable individual interrupts that belong to a particular bank. Pending register is used to determine what interrupts are waiting to be processed. for (i = 0; i < bank_irqs[b]; i++) { irq = irq_create_mapping(intc.domain, MAKE_HWIRQ(b, i)); BUG_ON(irq <= 0); irq_set_chip_and_handler(irq, &armctrl_chip, handle_level_irq); irq_set_probe(irq); } Next, there is a nested loop that is responsible for registering each supported interrupt and setting irq chip and handler. We already saw how the same functions are used in the local interrupt controller driver. However, I would like to highlight a few important things. MAKE_HWIRQ macro is used to calculate hardware irq number. It is calculated based on bank index and irq index inside the bank. handle_level_irq is a common handler that is used for interrupts of the level type. Interrupts of such type keep interrupt line set to \"high\" until the interrupt is acknowledged. There are also edge type interrupts that works in a different way. irq_set_probe function just unsets IRQ_NOPROBE interrupt flag, effectively disabling interrupt auto-probing. Interrupt auto-probing is a process that allows different drivers to discover which interrupt line their devices are connected to. This is not needed for Raspberry Pi, because this information is encoded in the device tree, however, for some devices, this might be useful. Please, refer to this comment to understand how auto-probing works in the Linux kernel. Next piece of code is different for BCM2836 and BCM2835 interrupt controllers (the first one corresponds to the RPi models 2 and 3, and the second one to RPi Model 1). If we are dealing with BCM2836 the following code is executed. int parent_irq = irq_of_parse_and_map(node, 0); if (!parent_irq) { panic(\"%pOF: unable to get parent interrupt.\\n\", node); } irq_set_chained_handler(parent_irq, bcm2836_chained_handle_irq); Device tree indicates that local interrupt controller is a parent of the global interrupt controller. Another device tree property tells us that global interrupt controller is connected to the interupt line number 8 of the local controller, this means that our parent irq is the one with hardware irq number 8. Those 2 properties allow Linux kernel to find out parent interrupt number (this is Linux interrupt number, not hardware number). Finally irq_set_chained_handler function replaces the handler of the parent irq with bcm2836_chained_handle_irq function. bcm2836_chained_handle_irq is very simple. Its code is listed below. static void bcm2836_chained_handle_irq(struct irq_desc *desc) { u32 hwirq; while ((hwirq = get_next_armctrl_hwirq()) != ~0) generic_handle_irq(irq_linear_revmap(intc.domain, hwirq)); } You can think about this code as an advanced version of what we did here for the RPi OS. get_next_armctrl_hwirq uses all 3 pending registers to figure out which interrupt was fired. irq_linear_revmap uses irq domain to translate hardware irq number into Linux irq number and generic_handle_irq just executes irq handler. Irq handler was set in the initialization function and it points to handle_level_irq that eventually executes all irq actions associated with the interrupt (this is actually done here .). For now, the list of irq actions is empty for all supported interrupts - a driver that is interested in handling some interrupt should add an action to the appropriate list. In the next chapter, we are going to see how this is done using system timer as an example. Previous Page 3.2 Interrupt handling: Low-level exception handling in Linux Next Page 3.4 Interrupt handling: Timers","title":"Interrupt controllers"},{"location":"lesson03/linux/interrupt_controllers/#33-interrupt-controllers","text":"In this chapter, we are going to talk a lot about Linux drivers and how they handle interrupts. We will start with driver initialization code and then take a look at how interrupts are processed after handle_arch_irq function.","title":"3.3: Interrupt controllers"},{"location":"lesson03/linux/interrupt_controllers/#using-device-tree-to-find-out-needed-devices-and-drivers","text":"When implementing interrupts in the RPi OS we have been working with 2 devices: system timer and interrupt controller. Now our goal will be to understand how the same devices work in Linux. The first thing we need to do is to find drivers that are responsible for working with mentioned devices. And in order to find needed drivers we can use bcm2837-rpi-3-b.dts device tree file. This is the top level device tree file that is specific for Raspberry Pi 3 Model B, it includes other more common device tree files, that are shared between different versions of Raspberry Pi. If you follow the chain of includes and search for timer and interrupt-controller you can find 4 devices. Local interrupt controller Local timer Global interrupt controller. It is defined here and modified here . System timer Stop, but why do we have 4 devices instead of 2? This requires some explanation, and we will tackle this question in the next section.","title":"Using device tree to find out needed devices and drivers"},{"location":"lesson03/linux/interrupt_controllers/#local-vs-global-interrupt-controllers","text":"When you think about interrupt handling in multiprocessor systems, one question you should ask yourself is which core should be responsible for processing a particular interrupt? When an interrupt occurs, are all 4 cores interrupted, or only a single one? Is it possible to route a particular interrupt to a specific core? Another question you may wonder is how one processor can notify another processor if he needs to pass some information to it? The local interrupt controller is a device that can help you in answering all those questions. It is responsible for the following tasks. Configuring which core should receive a specific interrupt. Sending interrupts between cores. Such interrupts are called \"mailboxes\" and allow cores to communicate one with each other. Handling interrupts from local timer and performance monitors interrupts (PMU). The behavior of a local interrupt controller as well as a local timer is documented in BCM2836 ARM-local peripherals manual. I already mentioned local timer several times. Now you probably wonder why do we need two independent timers in the system? I guess that the primary use-case for using the local timer is when you want to configure all 4 cores to receive timer interrupts simultaneously. If you use system timer you can only route interrupts to a single core. When working with the RPi OS we didn't work with either local interrupt controller or local timer. That is because by default local interrupt controller is configured in such a way that all external interrupts are sent to the first core, which is exactly what we need. We haven't used local timer because we use system timer instead.","title":"Local vs global interrupt controllers"},{"location":"lesson03/linux/interrupt_controllers/#local-interrupt-controller","text":"Accordingly to the bcm2837.dtsi the global interrupt controller is a child of the local one. Thus it makes sense to start our exploration with the local controller. If we need to find a driver that works with a particular device, we should use compatible property. Searching for the value of this property you can easily find that there is a single driver that is compatible with RPi local interrupt controller - here is the corresponding definition . IRQCHIP_DECLARE(bcm2836_arm_irqchip_l1_intc, \"brcm,bcm2836-l1-intc\", bcm2836_arm_irqchip_l1_intc_of_init); Now you can probably guess what is the procedure of a driver initialization: the kernel walks through all device definitions in the device tree and for each definition it looks for a matching driver using \"compatible\" property. If the driver is found, then its initialization function is called. Initialization function is provided during device registration, and in our case this function is bcm2836_arm_irqchip_l1_intc_of_init . static int __init bcm2836_arm_irqchip_l1_intc_of_init(struct device_node *node, struct device_node *parent) { intc.base = of_iomap(node, 0); if (!intc.base) { panic(\"%pOF: unable to map local interrupt registers\\n\", node); } bcm2835_init_local_timer_frequency(); intc.domain = irq_domain_add_linear(node, LAST_IRQ + 1, &bcm2836_arm_irqchip_intc_ops, NULL); if (!intc.domain) panic(\"%pOF: unable to create IRQ domain\\n\", node); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_CNTPSIRQ, &bcm2836_arm_irqchip_timer); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_CNTPNSIRQ, &bcm2836_arm_irqchip_timer); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_CNTHPIRQ, &bcm2836_arm_irqchip_timer); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_CNTVIRQ, &bcm2836_arm_irqchip_timer); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_GPU_FAST, &bcm2836_arm_irqchip_gpu); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_PMU_FAST, &bcm2836_arm_irqchip_pmu); bcm2836_arm_irqchip_smp_init(); set_handle_irq(bcm2836_arm_irqchip_handle_irq); return 0; } The initialization function takes 2 parameters: 'node' and 'parent', both of them are of the type struct device_node . node represents the current node in the device tree, and in our case it points here parent is a parent node in the device tree hierarchy, and for the local interrupt controller it points to soc element ( soc stands for \"system on chip\" and it is the simplest possible bus which maps all device registers directly to main memory.). node can be used to read various properties from the current device tree node. For example, the first line of the bcm2836_arm_irqchip_l1_intc_of_init function reads the device base address from reg property. However, the process is more complicated than that, because when this function is executed MMU is already enabled, and before we will be able to access some region of physical memory we must map this region to some virtual address. This is exactly what of_iomap function is doing: it reads reg property of the provided node and maps the whole memory region, described by reg property, to some virtual memory region. Next local timer frequency is initialized in bcm2835_init_local_timer_frequency function. There is nothing specific about this function: it just uses some of the registers, described in BCM2836 ARM-local peripherals manual, to initialize local timer. Next line requires some explanations. intc.domain = irq_domain_add_linear(node, LAST_IRQ + 1, &bcm2836_arm_irqchip_intc_ops, NULL); Linux assigns a unique integer number to each interrupt, you can think about this number as a unique interrupt ID. This ID is used each time you want to do something with an interrupt (for example, assign a handler, or assign which CPU should handle it). Each interrupt also has a hardware interrupt number. This is usually a number that tells which interrupt line was triggered. BCM2837 ARM Peripherals manual has the peripheral interrupt table at page 113 - you can think about an index in this table as a hardware interrupt number. So obviously we need some mechanism to map Linux irq numbers to hardware irq number and vice versa. If there is only one interrupt controller it would be possible to use one to one mapping but in general case a more sophisticated mechanism need to be used. In Linux struct irq_domain implements such mapping. Each interrupt controller driver should create its own irq domain and register all interrupts that it can handle with this domain. Registration function returns Linux irq number that later is used to work with the interrupt. Next 6 lines are responsible for registering each supported interrupt with the irq domain. bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_CNTPSIRQ, &bcm2836_arm_irqchip_timer); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_CNTPNSIRQ, &bcm2836_arm_irqchip_timer); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_CNTHPIRQ, &bcm2836_arm_irqchip_timer); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_CNTVIRQ, &bcm2836_arm_irqchip_timer); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_GPU_FAST, &bcm2836_arm_irqchip_gpu); bcm2836_arm_irqchip_register_irq(LOCAL_IRQ_PMU_FAST, &bcm2836_arm_irqchip_pmu); Accordingly to BCM2836 ARM-local peripherals manual local interrupt controller handles 10 different interrupts: 0 - 3 are interrupts from local timer, 4 - 7 are mailbox interrupts, which are used in interprocess communication, 8 corresponds to all interrupts generated by the global interrupt controller and interrupt 9 is a performance monitor interrupt. Here you can see that the driver defines a set of constants that holds hardware irq number per each interrupt. The registration code above registers all interrupts, except mailbox interrupts, which are registered separately. In order to understand the registration code better lets examine bcm2836_arm_irqchip_register_irq function. static void bcm2836_arm_irqchip_register_irq(int hwirq, struct irq_chip *chip) { int irq = irq_create_mapping(intc.domain, hwirq); irq_set_percpu_devid(irq); irq_set_chip_and_handler(irq, chip, handle_percpu_devid_irq); irq_set_status_flags(irq, IRQ_NOAUTOEN); } The first line here performs actual interrupt registration. irq_create_mapping takes hardware interrupt number as an input and returns Linux irq number. irq_set_percpu_devid configures interrupt as \"per CPU\", so that it will be handled only on the current CPU. This makes perfect sense because all interrupts that we are discussing now are local and they all can be handled only on the current CPU. irq_set_chip_and_handler , as its name suggest, sets irq chip and irq handler. Irq chip is a special struct, which needs to be created by the driver, that has methods for masking and unmasking a particular interrupt. The driver that we are examining right now defines 3 different irq chips: timer chip, PMU chip and GPU chip, which controls all interrupts generated by the external peripheral devices. Handler is a function that is responsible for processing an interrupt. In this case, the handler is set to generic handle_percpu_devid_irq function. This handler later will be rewritten by the global interrupt controller driver. irq_set_status_flags in this particular case sets a flag, indicating that the current interrupt should be enabled manually and should not be enabled by default. Going back to the bcm2836_arm_irqchip_l1_intc_of_init function, there are only 2 calls left. The first one is bcm2836_arm_irqchip_smp_init . Here mailbox interrupts are enabled, allowing processors cores to communicate with each other. The last function call is extremely important - this is the place where low-level exception handling code is connected to the driver. set_handle_irq(bcm2836_arm_irqchip_handle_irq); set_handle_irq is defined in architecture specific code and we already encountered this function. From the line above we can understand that bcm2836_arm_irqchip_handle_irq will be called by the low-level exception code. The function itself is listed below. static void __exception_irq_entry bcm2836_arm_irqchip_handle_irq(struct pt_regs *regs) { int cpu = smp_processor_id(); u32 stat; stat = readl_relaxed(intc.base + LOCAL_IRQ_PENDING0 + 4 * cpu); if (stat & BIT(LOCAL_IRQ_MAILBOX0)) { #ifdef CONFIG_SMP void __iomem *mailbox0 = (intc.base + LOCAL_MAILBOX0_CLR0 + 16 * cpu); u32 mbox_val = readl(mailbox0); u32 ipi = ffs(mbox_val) - 1; writel(1 << ipi, mailbox0); handle_IPI(ipi, regs); #endif } else if (stat) { u32 hwirq = ffs(stat) - 1; handle_domain_irq(intc.domain, hwirq, regs); } } This function reads LOCAL_IRQ_PENDING register to figure out what interrupts are currently pending. There are 4 LOCAL_IRQ_PENDING registers, each corresponding to its own processor core, that's why current processor index is used to select the right one. Mailbox interrupts and all other interrupts are processed in 2 different clauses of an if statement. The interaction between different cores of a multiprocessor system is out of scope for our current discussion, so we are going to skip mailbox interrupt handling part. Now we have only the following 2 lines left unexplained. u32 hwirq = ffs(stat) - 1; handle_domain_irq(intc.domain, hwirq, regs); This is were interrupt is passed to the next handler. First of all hardware irq number is calculated. ffs (Find first bit) function is used to do this. After hardware irq number is calculated handle_domain_irq function is called. This function uses irq domain to translate hardware irq number to Linux irq number, then checks irq configuration (it is stored in irq_desc struct) and calls an interrupt handler. We've seen that the handler was set to handle_percpu_devid_irq . However, this handler will be overwritten by the child interrupt controller later. Now, let's examine how this happens.","title":"Local interrupt controller"},{"location":"lesson03/linux/interrupt_controllers/#generic-interrupt-controller","text":"We have already seen how to use device tree and compatible property to find the driver corresponding to some device, so I am going to skip this part and jump straight to the generic interrupt controller driver source code. You can find it in irq-bcm2835.c file. As usual, we are going to start our exploration with the initialization function. It is called armctrl_of_init . static int __init armctrl_of_init(struct device_node *node, struct device_node *parent, bool is_2836) { void __iomem *base; int irq, b, i; base = of_iomap(node, 0); if (!base) panic(\"%pOF: unable to map IC registers\\n\", node); intc.domain = irq_domain_add_linear(node, MAKE_HWIRQ(NR_BANKS, 0), &armctrl_ops, NULL); if (!intc.domain) panic(\"%pOF: unable to create IRQ domain\\n\", node); for (b = 0; b < NR_BANKS; b++) { intc.pending[b] = base + reg_pending[b]; intc.enable[b] = base + reg_enable[b]; intc.disable[b] = base + reg_disable[b]; for (i = 0; i < bank_irqs[b]; i++) { irq = irq_create_mapping(intc.domain, MAKE_HWIRQ(b, i)); BUG_ON(irq <= 0); irq_set_chip_and_handler(irq, &armctrl_chip, handle_level_irq); irq_set_probe(irq); } } if (is_2836) { int parent_irq = irq_of_parse_and_map(node, 0); if (!parent_irq) { panic(\"%pOF: unable to get parent interrupt.\\n\", node); } irq_set_chained_handler(parent_irq, bcm2836_chained_handle_irq); } else { set_handle_irq(bcm2835_handle_irq); } return 0; } Now, let's investigate this function in more details. void __iomem *base; int irq, b, i; base = of_iomap(node, 0); if (!base) panic(\"%pOF: unable to map IC registers\\n\", node); intc.domain = irq_domain_add_linear(node, MAKE_HWIRQ(NR_BANKS, 0), &armctrl_ops, NULL); if (!intc.domain) panic(\"%pOF: unable to create IRQ domain\\n\", node); The function starts with the code that reads device base address from the device three and initializes the irq domain. This part should be already familiar to you because we have seen similar code in the local irq controller driver. for (b = 0; b < NR_BANKS; b++) { intc.pending[b] = base + reg_pending[b]; intc.enable[b] = base + reg_enable[b]; intc.disable[b] = base + reg_disable[b]; Next, there is a loop that iterates over all irq banks. We already briefly touched irq banks in the first chapter of this lesson. The interrupt controller has 3 irq banks, which are controlled by ENABLE_IRQS_1 , ENABLE_IRQS_2 and ENABLE_BASIC_IRQS registers. Each of the banks has its own enable, disable and pending registers. Enable and disable registers can be used to either enable or disable individual interrupts that belong to a particular bank. Pending register is used to determine what interrupts are waiting to be processed. for (i = 0; i < bank_irqs[b]; i++) { irq = irq_create_mapping(intc.domain, MAKE_HWIRQ(b, i)); BUG_ON(irq <= 0); irq_set_chip_and_handler(irq, &armctrl_chip, handle_level_irq); irq_set_probe(irq); } Next, there is a nested loop that is responsible for registering each supported interrupt and setting irq chip and handler. We already saw how the same functions are used in the local interrupt controller driver. However, I would like to highlight a few important things. MAKE_HWIRQ macro is used to calculate hardware irq number. It is calculated based on bank index and irq index inside the bank. handle_level_irq is a common handler that is used for interrupts of the level type. Interrupts of such type keep interrupt line set to \"high\" until the interrupt is acknowledged. There are also edge type interrupts that works in a different way. irq_set_probe function just unsets IRQ_NOPROBE interrupt flag, effectively disabling interrupt auto-probing. Interrupt auto-probing is a process that allows different drivers to discover which interrupt line their devices are connected to. This is not needed for Raspberry Pi, because this information is encoded in the device tree, however, for some devices, this might be useful. Please, refer to this comment to understand how auto-probing works in the Linux kernel. Next piece of code is different for BCM2836 and BCM2835 interrupt controllers (the first one corresponds to the RPi models 2 and 3, and the second one to RPi Model 1). If we are dealing with BCM2836 the following code is executed. int parent_irq = irq_of_parse_and_map(node, 0); if (!parent_irq) { panic(\"%pOF: unable to get parent interrupt.\\n\", node); } irq_set_chained_handler(parent_irq, bcm2836_chained_handle_irq); Device tree indicates that local interrupt controller is a parent of the global interrupt controller. Another device tree property tells us that global interrupt controller is connected to the interupt line number 8 of the local controller, this means that our parent irq is the one with hardware irq number 8. Those 2 properties allow Linux kernel to find out parent interrupt number (this is Linux interrupt number, not hardware number). Finally irq_set_chained_handler function replaces the handler of the parent irq with bcm2836_chained_handle_irq function. bcm2836_chained_handle_irq is very simple. Its code is listed below. static void bcm2836_chained_handle_irq(struct irq_desc *desc) { u32 hwirq; while ((hwirq = get_next_armctrl_hwirq()) != ~0) generic_handle_irq(irq_linear_revmap(intc.domain, hwirq)); } You can think about this code as an advanced version of what we did here for the RPi OS. get_next_armctrl_hwirq uses all 3 pending registers to figure out which interrupt was fired. irq_linear_revmap uses irq domain to translate hardware irq number into Linux irq number and generic_handle_irq just executes irq handler. Irq handler was set in the initialization function and it points to handle_level_irq that eventually executes all irq actions associated with the interrupt (this is actually done here .). For now, the list of irq actions is empty for all supported interrupts - a driver that is interested in handling some interrupt should add an action to the appropriate list. In the next chapter, we are going to see how this is done using system timer as an example.","title":"Generic interrupt controller"},{"location":"lesson03/linux/interrupt_controllers/#previous-page","text":"3.2 Interrupt handling: Low-level exception handling in Linux","title":"Previous Page"},{"location":"lesson03/linux/interrupt_controllers/#next-page","text":"3.4 Interrupt handling: Timers","title":"Next Page"},{"location":"lesson03/linux/low_level-exception_handling/","text":"3.2: Low-level exception handling in Linux Given huge Linux kernel source code, what is a good way to find the code that is responsible for interrupt handling? I can suggest one idea. Vector table base address should be stored in the 'vbar_el1' register, so, if you search for vbar_el1 , you should be able to figure out where exactly the vector table is initialized. Indeed, the search gives us just a few usages, one of which belongs to already familiar to us head.S . This code is inside __primary_switched function. This function is executed after the MMU is switched on. The code looks like the following. adr_l x8, vectors // load VBAR_EL1 with virtual msr vbar_el1, x8 // vector table address From this code, we can infer that the vector table is called vectors and you should be able to easily find its definition . /* * Exception vectors. */ .pushsection \".entry.text\", \"ax\" .align 11 ENTRY(vectors) kernel_ventry el1_sync_invalid // Synchronous EL1t kernel_ventry el1_irq_invalid // IRQ EL1t kernel_ventry el1_fiq_invalid // FIQ EL1t kernel_ventry el1_error_invalid // Error EL1t kernel_ventry el1_sync // Synchronous EL1h kernel_ventry el1_irq // IRQ EL1h kernel_ventry el1_fiq_invalid // FIQ EL1h kernel_ventry el1_error_invalid // Error EL1h kernel_ventry el0_sync // Synchronous 64-bit EL0 kernel_ventry el0_irq // IRQ 64-bit EL0 kernel_ventry el0_fiq_invalid // FIQ 64-bit EL0 kernel_ventry el0_error_invalid // Error 64-bit EL0 #ifdef CONFIG_COMPAT kernel_ventry el0_sync_compat // Synchronous 32-bit EL0 kernel_ventry el0_irq_compat // IRQ 32-bit EL0 kernel_ventry el0_fiq_invalid_compat // FIQ 32-bit EL0 kernel_ventry el0_error_invalid_compat // Error 32-bit EL0 #else kernel_ventry el0_sync_invalid // Synchronous 32-bit EL0 kernel_ventry el0_irq_invalid // IRQ 32-bit EL0 kernel_ventry el0_fiq_invalid // FIQ 32-bit EL0 kernel_ventry el0_error_invalid // Error 32-bit EL0 #endif END(vectors) Looks familiar, isn't it? And indeed, I've copied most of this code and just simplified it a little bit. kernel_ventry macro is almost the same as ventry , defined in the RPi OS. One difference, though, is that kernel_ventry also is responsible for checking whether a kernel stack overflow has occurred. This functionality is enabled if CONFIG_VMAP_STACK is set and it is a part of the kernel feature that is called Virtually mapped kernel stacks . I'm not going to explain it in details here, however, if you are interested, I can recommend you to read this article. kernel_entry kernel_entry macro should also be familiar to you. It is used exactly in the same way as the corresonding macro in the RPI OS. Original (Linux) version, however, is a lot more complicated. The code is listed below. .macro kernel_entry, el, regsize = 64 .if \\regsize == 32 mov w0, w0 // zero upper 32 bits of x0 .endif stp x0, x1, [sp, #16 * 0] stp x2, x3, [sp, #16 * 1] stp x4, x5, [sp, #16 * 2] stp x6, x7, [sp, #16 * 3] stp x8, x9, [sp, #16 * 4] stp x10, x11, [sp, #16 * 5] stp x12, x13, [sp, #16 * 6] stp x14, x15, [sp, #16 * 7] stp x16, x17, [sp, #16 * 8] stp x18, x19, [sp, #16 * 9] stp x20, x21, [sp, #16 * 10] stp x22, x23, [sp, #16 * 11] stp x24, x25, [sp, #16 * 12] stp x26, x27, [sp, #16 * 13] stp x28, x29, [sp, #16 * 14] .if \\el == 0 mrs x21, sp_el0 ldr_this_cpu tsk, __entry_task, x20 // Ensure MDSCR_EL1.SS is clear, ldr x19, [tsk, #TSK_TI_FLAGS] // since we can unmask debug disable_step_tsk x19, x20 // exceptions when scheduling. mov x29, xzr // fp pointed to user-space .else add x21, sp, #S_FRAME_SIZE get_thread_info tsk /* Save the task's original addr_limit and set USER_DS (TASK_SIZE_64) */ ldr x20, [tsk, #TSK_TI_ADDR_LIMIT] str x20, [sp, #S_ORIG_ADDR_LIMIT] mov x20, #TASK_SIZE_64 str x20, [tsk, #TSK_TI_ADDR_LIMIT] /* No need to reset PSTATE.UAO, hardware's already set it to 0 for us */ .endif /* \\el == 0 */ mrs x22, elr_el1 mrs x23, spsr_el1 stp lr, x21, [sp, #S_LR] /* * In order to be able to dump the contents of struct pt_regs at the * time the exception was taken (in case we attempt to walk the call * stack later), chain it together with the stack frames. */ .if \\el == 0 stp xzr, xzr, [sp, #S_STACKFRAME] .else stp x29, x22, [sp, #S_STACKFRAME] .endif add x29, sp, #S_STACKFRAME #ifdef CONFIG_ARM64_SW_TTBR0_PAN /* * Set the TTBR0 PAN bit in SPSR. When the exception is taken from * EL0, there is no need to check the state of TTBR0_EL1 since * accesses are always enabled. * Note that the meaning of this bit differs from the ARMv8.1 PAN * feature as all TTBR0_EL1 accesses are disabled, not just those to * user mappings. */ alternative_if ARM64_HAS_PAN b 1f // skip TTBR0 PAN alternative_else_nop_endif .if \\el != 0 mrs x21, ttbr0_el1 tst x21, #0xffff << 48 // Check for the reserved ASID orr x23, x23, #PSR_PAN_BIT // Set the emulated PAN in the saved SPSR b.eq 1f // TTBR0 access already disabled and x23, x23, #~PSR_PAN_BIT // Clear the emulated PAN in the saved SPSR .endif __uaccess_ttbr0_disable x21 1: #endif stp x22, x23, [sp, #S_PC] /* Not in a syscall by default (el0_svc overwrites for real syscall) */ .if \\el == 0 mov w21, #NO_SYSCALL str w21, [sp, #S_SYSCALLNO] .endif /* * Set sp_el0 to current thread_info. */ .if \\el == 0 msr sp_el0, tsk .endif /* * Registers that may be useful after this macro is invoked: * * x21 - aborted SP * x22 - aborted PC * x23 - aborted PSTATE */ .endm Now we are going to explore the kernel_entry macro in details. .macro kernel_entry, el, regsize = 64 The macro accepts 2 parameters: el and regsize . el can be either 0 or 1 depending on whether an exception was generated at EL0 or EL1. regsize is 32 if we came from 32-bit EL0 or 64 otherwise. .if \\regsize == 32 mov w0, w0 // zero upper 32 bits of x0 .endif In 32-bit mode, we use 32-bit general purpose registers ( w0 instead of x0 ). w0 is architecturally mapped to the lower part of x0 . The provided code snippet zeroes upper 32 bits of the x0 register by writing w0 to itself. stp x0, x1, [sp, #16 * 0] stp x2, x3, [sp, #16 * 1] stp x4, x5, [sp, #16 * 2] stp x6, x7, [sp, #16 * 3] stp x8, x9, [sp, #16 * 4] stp x10, x11, [sp, #16 * 5] stp x12, x13, [sp, #16 * 6] stp x14, x15, [sp, #16 * 7] stp x16, x17, [sp, #16 * 8] stp x18, x19, [sp, #16 * 9] stp x20, x21, [sp, #16 * 10] stp x22, x23, [sp, #16 * 11] stp x24, x25, [sp, #16 * 12] stp x26, x27, [sp, #16 * 13] stp x28, x29, [sp, #16 * 14] This part saves all general purpose registers on the stack. Note, that stack pointer was already adjusted in the kernel_ventry to fit everything that needs to be stored. The order in which we save registers matters because in Linux there is a special structure pt_regs that is used to access saved registers later inside an exception handler. As you might see this structure contains not only general purpose registers but also some other information, which is mostly populated later in the kernel_entry macro. I recommend you to remember pt_regs struct because we are going to implement and use a similar one in the next few lessons. .if \\el == 0 mrs x21, sp_el0 x21 now contains aborted stack pointer. Note, that a task in Linux uses 2 different stacks for user and kernel mode. In case of user mode, we can use sp_el0 register to figure out the stack pointer value at the moment when the exception was generated. This line is very important because we need to swap stack pointers during the context switch. We will talk about it in details in the next lesson. ldr_this_cpu tsk, __entry_task, x20 // Ensure MDSCR_EL1.SS is clear, ldr x19, [tsk, #TSK_TI_FLAGS] // since we can unmask debug disable_step_tsk x19, x20 // exceptions when scheduling. MDSCR_EL1.SS bit is responsible for enabling \"Software Step exceptions\". If this bit is set and debug exceptions are unmasked, an exception is generated after any instruction has been executed. This is commonly used by debuggers. When taking exception from user mode, we need to check first whether TIF_SINGLESTEP flag is set for the current task. If yes, this indicates that the task is executing under a debugger and we must unset MDSCR_EL1.SS bit. The important thing to understand in this code is how information about the current task is obtained. In Linux, each process or thread (later I will reference any of them as just \"task\") has a task_struct associated with it. This struct contains all metadata information about a task. On arm64 architecture task_struct embeds another structure that is called thread_info so that a pointer to task_struct can always be used as a pointer to thread_info . thread_info is the place were flags are stored along with some other low-level values that entry.S need direct access to. mov x29, xzr // fp pointed to user-space Though x29 is a general purpose register it usually has a special meaning. It is used as a \"Frame pointer\". Now I want to spend some time to explain its purpose. When a function is compiled, the first couple of instructions are usually responsible for storing old frame pointer and link register values on the stack. (Just a quick reminder: x30 is called link register and it holds a \"return address\" that is used by the ret instruction) Then a new stack frame is allocated, so that it can contain all local variables of the function, and frame pointer register is set to point to the bottom of the frame. Whenever the function needs to access some local variable it simply adds hardcoded offset to the frame pointer. Imagine now that an error has occurred and we need to generate a stack trace. We can use current frame pointer to find all local variables in the stack, and the link register can be used used to figure out the precise location of the caller. Next, we take advantage of the fact that old frame pointer and link register values are always saved at the beginning of the stack frame, and we just read them from there. After we get caller's frame pointer we can now access all its local variables as well. This process is repeated recursively until we reach the top of the stack and is called \"stack unwinding\". A similar algorithm is used by ptrace system call. Now, going back to the kernel_entry macro, it should be clear why do we need to clear x29 register after taking an exception from EL0. That is because in Linux each task uses a different stack for user and kernel mode, and therefore it doesn't make sense to have common stack traces. .else add x21, sp, #S_FRAME_SIZE Now we are inside else clause, which mean that this code is relevant only if we are handling an exception taken from EL1. In this case, we are reusing old stack and the provided code snippet just saves original sp value in the x21 register for later usage. /* Save the task's original addr_limit and set USER_DS (TASK_SIZE_64) */ ldr x20, [tsk, #TSK_TI_ADDR_LIMIT] str x20, [sp, #S_ORIG_ADDR_LIMIT] mov x20, #TASK_SIZE_64 str x20, [tsk, #TSK_TI_ADDR_LIMIT] Task address limit specifies the largest virtual address that can be used. When user process operates in 32-bit mode this limit is 2^32 . For 64 bit kernel it can be larger and usually is 2^48 . If it happens that an exception is taken from 32-bit EL1, task address limit need to be changed to TASK_SIZE_64 . Also, it is required to save the original address limit because it needs to be restored before the execution will be returned to user mode. mrs x22, elr_el1 mrs x23, spsr_el1 elr_el1 and spsr_el1 must be saved on the stack before we start handling an exception. We haven't done it yet in the RPI OS, because for now we always return to the same location from which an exception was taken. But what if we need to do a context switch while handling an exception? We will discuss this scenario in details in the next lesson. stp lr, x21, [sp, #S_LR] Link register and frame pointer registers are saved on the stack. We already saw that frame pointer is calculated differently depending on whether an exception was taken from EL0 or EL1 and the result of this calculation was already stored in x21 register. /* * In order to be able to dump the contents of struct pt_regs at the * time the exception was taken (in case we attempt to walk the call * stack later), chain it together with the stack frames. */ .if \\el == 0 stp xzr, xzr, [sp, #S_STACKFRAME] .else stp x29, x22, [sp, #S_STACKFRAME] .endif add x29, sp, #S_STACKFRAME Here stackframe property of the pt_regs struct is filled. This property also contains link register and frame pointer, though this time the value of elr_el1 (which is now in x22 ) is used instead of lr . stackframe is used solely for stack unwinding. #ifdef CONFIG_ARM64_SW_TTBR0_PAN alternative_if ARM64_HAS_PAN b 1f // skip TTBR0 PAN alternative_else_nop_endif .if \\el != 0 mrs x21, ttbr0_el1 tst x21, #0xffff << 48 // Check for the reserved ASID orr x23, x23, #PSR_PAN_BIT // Set the emulated PAN in the saved SPSR b.eq 1f // TTBR0 access already disabled and x23, x23, #~PSR_PAN_BIT // Clear the emulated PAN in the saved SPSR .endif __uaccess_ttbr0_disable x21 1: #endif CONFIG_ARM64_SW_TTBR0_PAN parameter prevents the kernel from accessing user-space memory directly. If you are wondering when this might be useful you can read this article. For now, I will also skip the detailed explanation of how this works, because such security features are too out of scope for our discussion. stp x22, x23, [sp, #S_PC] Here elr_el1 and spsr_el1 are saved on the stack. /* Not in a syscall by default (el0_svc overwrites for real syscall) */ .if \\el == 0 mov w21, #NO_SYSCALL str w21, [sp, #S_SYSCALLNO] .endif pt_regs struct has a field indicating whether the current exception is a system call or not. By default, we assume that it isn't. Wait till lecture 5 for the detailed explanation how syscalls work. /* * Set sp_el0 to current thread_info. */ .if \\el == 0 msr sp_el0, tsk .endif When a task is executed in kernel mode, sp_el0 is not needed. Its value was previously saved on the stack so it can be easily restored in kernel_exit macro. Starting from this point sp_el0 will be used to hold a pointer to current task_struct for quick access. el1_irq Next thing we are going to explore is the handler that is responsible for processing IRQs taken from EL1. From the vector table we can easily find out that the handler is called el1_irq and is defined here . Let's take a look on the code now and examine it line by line. el1_irq: kernel_entry 1 enable_dbg #ifdef CONFIG_TRACE_IRQFLAGS bl trace_hardirqs_off #endif irq_handler #ifdef CONFIG_PREEMPT ldr w24, [tsk, #TSK_TI_PREEMPT] // get preempt count cbnz w24, 1f // preempt count != 0 ldr x0, [tsk, #TSK_TI_FLAGS] // get flags tbz x0, #TIF_NEED_RESCHED, 1f // needs rescheduling? bl el1_preempt 1: #endif #ifdef CONFIG_TRACE_IRQFLAGS bl trace_hardirqs_on #endif kernel_exit 1 ENDPROC(el1_irq) The following is done inside this function. kernel_entry and kernel_exit macros are called to save and restore processor state. The first parameter indicates that the exception is taken from EL1. Debug interrupts are unmasked by calling enable_dbg macro. At this point, it is safe to do so, because the processor state is already saved and, even if debug exception occurred in the middle of the interrupt handler, it will be processed correctly. If you wonder why is it necessary to unmask debug exceptions during an interrupt processing in the first place - read this commit message. Code inside #ifdef CONFIG_TRACE_IRQFLAGS block is responsible for tracing interrupts. It records 2 events: interrupt start and end. Code inside #ifdef CONFIG_PREEMPT block access current task flags to check whether we need to call the scheduler. This code will be examined details in the next lesson. irq_handler - this is the place were actual interrupt handling is performed. irq_handler is a macro and it is defined as the follows. .macro irq_handler ldr_l x1, handle_arch_irq mov x0, sp irq_stack_entry blr x1 irq_stack_exit .endm As you might see from the code, irq_handler executes handle_arch_irq function. This function is executed with special stack, that is called \"irq stack\". Why is it necessary to switch to a different stack? In RPI OS, for example, we didn't do this. Well, I guess it is not necessary, but without it, an interrupt will be handled using task stack, and we can never be sure how much of it is still left for the interrupt handler. Next, we need to look at handle_arch_irq . It appears that it is not a function, but a variable. It is set inside set_handle_irq function. But who sets it, and what is the fade of an interrupt after it reaches this point? We will figure out the answer in the next chapter of this lesson. Conclusion As a conclusion, I can say that we've already explored the low-level interrupt handling code and trace the path of an interrupt from the vector table all the way to the handle_arch_irq . This is the point were an interrupt leaves architecture specific code and started to be handled by a driver code. Our goal in the next chapter will be to trace the path of a timer interrupt through the driver source code. Previous Page 3.1 Interrupt handling: RPi OS Next Page 3.3 Interrupt handling: Interrupt controllers","title":"Low level exception handling"},{"location":"lesson03/linux/low_level-exception_handling/#32-low-level-exception-handling-in-linux","text":"Given huge Linux kernel source code, what is a good way to find the code that is responsible for interrupt handling? I can suggest one idea. Vector table base address should be stored in the 'vbar_el1' register, so, if you search for vbar_el1 , you should be able to figure out where exactly the vector table is initialized. Indeed, the search gives us just a few usages, one of which belongs to already familiar to us head.S . This code is inside __primary_switched function. This function is executed after the MMU is switched on. The code looks like the following. adr_l x8, vectors // load VBAR_EL1 with virtual msr vbar_el1, x8 // vector table address From this code, we can infer that the vector table is called vectors and you should be able to easily find its definition . /* * Exception vectors. */ .pushsection \".entry.text\", \"ax\" .align 11 ENTRY(vectors) kernel_ventry el1_sync_invalid // Synchronous EL1t kernel_ventry el1_irq_invalid // IRQ EL1t kernel_ventry el1_fiq_invalid // FIQ EL1t kernel_ventry el1_error_invalid // Error EL1t kernel_ventry el1_sync // Synchronous EL1h kernel_ventry el1_irq // IRQ EL1h kernel_ventry el1_fiq_invalid // FIQ EL1h kernel_ventry el1_error_invalid // Error EL1h kernel_ventry el0_sync // Synchronous 64-bit EL0 kernel_ventry el0_irq // IRQ 64-bit EL0 kernel_ventry el0_fiq_invalid // FIQ 64-bit EL0 kernel_ventry el0_error_invalid // Error 64-bit EL0 #ifdef CONFIG_COMPAT kernel_ventry el0_sync_compat // Synchronous 32-bit EL0 kernel_ventry el0_irq_compat // IRQ 32-bit EL0 kernel_ventry el0_fiq_invalid_compat // FIQ 32-bit EL0 kernel_ventry el0_error_invalid_compat // Error 32-bit EL0 #else kernel_ventry el0_sync_invalid // Synchronous 32-bit EL0 kernel_ventry el0_irq_invalid // IRQ 32-bit EL0 kernel_ventry el0_fiq_invalid // FIQ 32-bit EL0 kernel_ventry el0_error_invalid // Error 32-bit EL0 #endif END(vectors) Looks familiar, isn't it? And indeed, I've copied most of this code and just simplified it a little bit. kernel_ventry macro is almost the same as ventry , defined in the RPi OS. One difference, though, is that kernel_ventry also is responsible for checking whether a kernel stack overflow has occurred. This functionality is enabled if CONFIG_VMAP_STACK is set and it is a part of the kernel feature that is called Virtually mapped kernel stacks . I'm not going to explain it in details here, however, if you are interested, I can recommend you to read this article.","title":"3.2: Low-level exception handling in Linux"},{"location":"lesson03/linux/low_level-exception_handling/#kernel_entry","text":"kernel_entry macro should also be familiar to you. It is used exactly in the same way as the corresonding macro in the RPI OS. Original (Linux) version, however, is a lot more complicated. The code is listed below. .macro kernel_entry, el, regsize = 64 .if \\regsize == 32 mov w0, w0 // zero upper 32 bits of x0 .endif stp x0, x1, [sp, #16 * 0] stp x2, x3, [sp, #16 * 1] stp x4, x5, [sp, #16 * 2] stp x6, x7, [sp, #16 * 3] stp x8, x9, [sp, #16 * 4] stp x10, x11, [sp, #16 * 5] stp x12, x13, [sp, #16 * 6] stp x14, x15, [sp, #16 * 7] stp x16, x17, [sp, #16 * 8] stp x18, x19, [sp, #16 * 9] stp x20, x21, [sp, #16 * 10] stp x22, x23, [sp, #16 * 11] stp x24, x25, [sp, #16 * 12] stp x26, x27, [sp, #16 * 13] stp x28, x29, [sp, #16 * 14] .if \\el == 0 mrs x21, sp_el0 ldr_this_cpu tsk, __entry_task, x20 // Ensure MDSCR_EL1.SS is clear, ldr x19, [tsk, #TSK_TI_FLAGS] // since we can unmask debug disable_step_tsk x19, x20 // exceptions when scheduling. mov x29, xzr // fp pointed to user-space .else add x21, sp, #S_FRAME_SIZE get_thread_info tsk /* Save the task's original addr_limit and set USER_DS (TASK_SIZE_64) */ ldr x20, [tsk, #TSK_TI_ADDR_LIMIT] str x20, [sp, #S_ORIG_ADDR_LIMIT] mov x20, #TASK_SIZE_64 str x20, [tsk, #TSK_TI_ADDR_LIMIT] /* No need to reset PSTATE.UAO, hardware's already set it to 0 for us */ .endif /* \\el == 0 */ mrs x22, elr_el1 mrs x23, spsr_el1 stp lr, x21, [sp, #S_LR] /* * In order to be able to dump the contents of struct pt_regs at the * time the exception was taken (in case we attempt to walk the call * stack later), chain it together with the stack frames. */ .if \\el == 0 stp xzr, xzr, [sp, #S_STACKFRAME] .else stp x29, x22, [sp, #S_STACKFRAME] .endif add x29, sp, #S_STACKFRAME #ifdef CONFIG_ARM64_SW_TTBR0_PAN /* * Set the TTBR0 PAN bit in SPSR. When the exception is taken from * EL0, there is no need to check the state of TTBR0_EL1 since * accesses are always enabled. * Note that the meaning of this bit differs from the ARMv8.1 PAN * feature as all TTBR0_EL1 accesses are disabled, not just those to * user mappings. */ alternative_if ARM64_HAS_PAN b 1f // skip TTBR0 PAN alternative_else_nop_endif .if \\el != 0 mrs x21, ttbr0_el1 tst x21, #0xffff << 48 // Check for the reserved ASID orr x23, x23, #PSR_PAN_BIT // Set the emulated PAN in the saved SPSR b.eq 1f // TTBR0 access already disabled and x23, x23, #~PSR_PAN_BIT // Clear the emulated PAN in the saved SPSR .endif __uaccess_ttbr0_disable x21 1: #endif stp x22, x23, [sp, #S_PC] /* Not in a syscall by default (el0_svc overwrites for real syscall) */ .if \\el == 0 mov w21, #NO_SYSCALL str w21, [sp, #S_SYSCALLNO] .endif /* * Set sp_el0 to current thread_info. */ .if \\el == 0 msr sp_el0, tsk .endif /* * Registers that may be useful after this macro is invoked: * * x21 - aborted SP * x22 - aborted PC * x23 - aborted PSTATE */ .endm Now we are going to explore the kernel_entry macro in details. .macro kernel_entry, el, regsize = 64 The macro accepts 2 parameters: el and regsize . el can be either 0 or 1 depending on whether an exception was generated at EL0 or EL1. regsize is 32 if we came from 32-bit EL0 or 64 otherwise. .if \\regsize == 32 mov w0, w0 // zero upper 32 bits of x0 .endif In 32-bit mode, we use 32-bit general purpose registers ( w0 instead of x0 ). w0 is architecturally mapped to the lower part of x0 . The provided code snippet zeroes upper 32 bits of the x0 register by writing w0 to itself. stp x0, x1, [sp, #16 * 0] stp x2, x3, [sp, #16 * 1] stp x4, x5, [sp, #16 * 2] stp x6, x7, [sp, #16 * 3] stp x8, x9, [sp, #16 * 4] stp x10, x11, [sp, #16 * 5] stp x12, x13, [sp, #16 * 6] stp x14, x15, [sp, #16 * 7] stp x16, x17, [sp, #16 * 8] stp x18, x19, [sp, #16 * 9] stp x20, x21, [sp, #16 * 10] stp x22, x23, [sp, #16 * 11] stp x24, x25, [sp, #16 * 12] stp x26, x27, [sp, #16 * 13] stp x28, x29, [sp, #16 * 14] This part saves all general purpose registers on the stack. Note, that stack pointer was already adjusted in the kernel_ventry to fit everything that needs to be stored. The order in which we save registers matters because in Linux there is a special structure pt_regs that is used to access saved registers later inside an exception handler. As you might see this structure contains not only general purpose registers but also some other information, which is mostly populated later in the kernel_entry macro. I recommend you to remember pt_regs struct because we are going to implement and use a similar one in the next few lessons. .if \\el == 0 mrs x21, sp_el0 x21 now contains aborted stack pointer. Note, that a task in Linux uses 2 different stacks for user and kernel mode. In case of user mode, we can use sp_el0 register to figure out the stack pointer value at the moment when the exception was generated. This line is very important because we need to swap stack pointers during the context switch. We will talk about it in details in the next lesson. ldr_this_cpu tsk, __entry_task, x20 // Ensure MDSCR_EL1.SS is clear, ldr x19, [tsk, #TSK_TI_FLAGS] // since we can unmask debug disable_step_tsk x19, x20 // exceptions when scheduling. MDSCR_EL1.SS bit is responsible for enabling \"Software Step exceptions\". If this bit is set and debug exceptions are unmasked, an exception is generated after any instruction has been executed. This is commonly used by debuggers. When taking exception from user mode, we need to check first whether TIF_SINGLESTEP flag is set for the current task. If yes, this indicates that the task is executing under a debugger and we must unset MDSCR_EL1.SS bit. The important thing to understand in this code is how information about the current task is obtained. In Linux, each process or thread (later I will reference any of them as just \"task\") has a task_struct associated with it. This struct contains all metadata information about a task. On arm64 architecture task_struct embeds another structure that is called thread_info so that a pointer to task_struct can always be used as a pointer to thread_info . thread_info is the place were flags are stored along with some other low-level values that entry.S need direct access to. mov x29, xzr // fp pointed to user-space Though x29 is a general purpose register it usually has a special meaning. It is used as a \"Frame pointer\". Now I want to spend some time to explain its purpose. When a function is compiled, the first couple of instructions are usually responsible for storing old frame pointer and link register values on the stack. (Just a quick reminder: x30 is called link register and it holds a \"return address\" that is used by the ret instruction) Then a new stack frame is allocated, so that it can contain all local variables of the function, and frame pointer register is set to point to the bottom of the frame. Whenever the function needs to access some local variable it simply adds hardcoded offset to the frame pointer. Imagine now that an error has occurred and we need to generate a stack trace. We can use current frame pointer to find all local variables in the stack, and the link register can be used used to figure out the precise location of the caller. Next, we take advantage of the fact that old frame pointer and link register values are always saved at the beginning of the stack frame, and we just read them from there. After we get caller's frame pointer we can now access all its local variables as well. This process is repeated recursively until we reach the top of the stack and is called \"stack unwinding\". A similar algorithm is used by ptrace system call. Now, going back to the kernel_entry macro, it should be clear why do we need to clear x29 register after taking an exception from EL0. That is because in Linux each task uses a different stack for user and kernel mode, and therefore it doesn't make sense to have common stack traces. .else add x21, sp, #S_FRAME_SIZE Now we are inside else clause, which mean that this code is relevant only if we are handling an exception taken from EL1. In this case, we are reusing old stack and the provided code snippet just saves original sp value in the x21 register for later usage. /* Save the task's original addr_limit and set USER_DS (TASK_SIZE_64) */ ldr x20, [tsk, #TSK_TI_ADDR_LIMIT] str x20, [sp, #S_ORIG_ADDR_LIMIT] mov x20, #TASK_SIZE_64 str x20, [tsk, #TSK_TI_ADDR_LIMIT] Task address limit specifies the largest virtual address that can be used. When user process operates in 32-bit mode this limit is 2^32 . For 64 bit kernel it can be larger and usually is 2^48 . If it happens that an exception is taken from 32-bit EL1, task address limit need to be changed to TASK_SIZE_64 . Also, it is required to save the original address limit because it needs to be restored before the execution will be returned to user mode. mrs x22, elr_el1 mrs x23, spsr_el1 elr_el1 and spsr_el1 must be saved on the stack before we start handling an exception. We haven't done it yet in the RPI OS, because for now we always return to the same location from which an exception was taken. But what if we need to do a context switch while handling an exception? We will discuss this scenario in details in the next lesson. stp lr, x21, [sp, #S_LR] Link register and frame pointer registers are saved on the stack. We already saw that frame pointer is calculated differently depending on whether an exception was taken from EL0 or EL1 and the result of this calculation was already stored in x21 register. /* * In order to be able to dump the contents of struct pt_regs at the * time the exception was taken (in case we attempt to walk the call * stack later), chain it together with the stack frames. */ .if \\el == 0 stp xzr, xzr, [sp, #S_STACKFRAME] .else stp x29, x22, [sp, #S_STACKFRAME] .endif add x29, sp, #S_STACKFRAME Here stackframe property of the pt_regs struct is filled. This property also contains link register and frame pointer, though this time the value of elr_el1 (which is now in x22 ) is used instead of lr . stackframe is used solely for stack unwinding. #ifdef CONFIG_ARM64_SW_TTBR0_PAN alternative_if ARM64_HAS_PAN b 1f // skip TTBR0 PAN alternative_else_nop_endif .if \\el != 0 mrs x21, ttbr0_el1 tst x21, #0xffff << 48 // Check for the reserved ASID orr x23, x23, #PSR_PAN_BIT // Set the emulated PAN in the saved SPSR b.eq 1f // TTBR0 access already disabled and x23, x23, #~PSR_PAN_BIT // Clear the emulated PAN in the saved SPSR .endif __uaccess_ttbr0_disable x21 1: #endif CONFIG_ARM64_SW_TTBR0_PAN parameter prevents the kernel from accessing user-space memory directly. If you are wondering when this might be useful you can read this article. For now, I will also skip the detailed explanation of how this works, because such security features are too out of scope for our discussion. stp x22, x23, [sp, #S_PC] Here elr_el1 and spsr_el1 are saved on the stack. /* Not in a syscall by default (el0_svc overwrites for real syscall) */ .if \\el == 0 mov w21, #NO_SYSCALL str w21, [sp, #S_SYSCALLNO] .endif pt_regs struct has a field indicating whether the current exception is a system call or not. By default, we assume that it isn't. Wait till lecture 5 for the detailed explanation how syscalls work. /* * Set sp_el0 to current thread_info. */ .if \\el == 0 msr sp_el0, tsk .endif When a task is executed in kernel mode, sp_el0 is not needed. Its value was previously saved on the stack so it can be easily restored in kernel_exit macro. Starting from this point sp_el0 will be used to hold a pointer to current task_struct for quick access.","title":"kernel_entry"},{"location":"lesson03/linux/low_level-exception_handling/#el1_irq","text":"Next thing we are going to explore is the handler that is responsible for processing IRQs taken from EL1. From the vector table we can easily find out that the handler is called el1_irq and is defined here . Let's take a look on the code now and examine it line by line. el1_irq: kernel_entry 1 enable_dbg #ifdef CONFIG_TRACE_IRQFLAGS bl trace_hardirqs_off #endif irq_handler #ifdef CONFIG_PREEMPT ldr w24, [tsk, #TSK_TI_PREEMPT] // get preempt count cbnz w24, 1f // preempt count != 0 ldr x0, [tsk, #TSK_TI_FLAGS] // get flags tbz x0, #TIF_NEED_RESCHED, 1f // needs rescheduling? bl el1_preempt 1: #endif #ifdef CONFIG_TRACE_IRQFLAGS bl trace_hardirqs_on #endif kernel_exit 1 ENDPROC(el1_irq) The following is done inside this function. kernel_entry and kernel_exit macros are called to save and restore processor state. The first parameter indicates that the exception is taken from EL1. Debug interrupts are unmasked by calling enable_dbg macro. At this point, it is safe to do so, because the processor state is already saved and, even if debug exception occurred in the middle of the interrupt handler, it will be processed correctly. If you wonder why is it necessary to unmask debug exceptions during an interrupt processing in the first place - read this commit message. Code inside #ifdef CONFIG_TRACE_IRQFLAGS block is responsible for tracing interrupts. It records 2 events: interrupt start and end. Code inside #ifdef CONFIG_PREEMPT block access current task flags to check whether we need to call the scheduler. This code will be examined details in the next lesson. irq_handler - this is the place were actual interrupt handling is performed. irq_handler is a macro and it is defined as the follows. .macro irq_handler ldr_l x1, handle_arch_irq mov x0, sp irq_stack_entry blr x1 irq_stack_exit .endm As you might see from the code, irq_handler executes handle_arch_irq function. This function is executed with special stack, that is called \"irq stack\". Why is it necessary to switch to a different stack? In RPI OS, for example, we didn't do this. Well, I guess it is not necessary, but without it, an interrupt will be handled using task stack, and we can never be sure how much of it is still left for the interrupt handler. Next, we need to look at handle_arch_irq . It appears that it is not a function, but a variable. It is set inside set_handle_irq function. But who sets it, and what is the fade of an interrupt after it reaches this point? We will figure out the answer in the next chapter of this lesson.","title":"el1_irq"},{"location":"lesson03/linux/low_level-exception_handling/#conclusion","text":"As a conclusion, I can say that we've already explored the low-level interrupt handling code and trace the path of an interrupt from the vector table all the way to the handle_arch_irq . This is the point were an interrupt leaves architecture specific code and started to be handled by a driver code. Our goal in the next chapter will be to trace the path of a timer interrupt through the driver source code.","title":"Conclusion"},{"location":"lesson03/linux/low_level-exception_handling/#previous-page","text":"3.1 Interrupt handling: RPi OS","title":"Previous Page"},{"location":"lesson03/linux/low_level-exception_handling/#next-page","text":"3.3 Interrupt handling: Interrupt controllers","title":"Next Page"},{"location":"lesson03/linux/timer/","text":"3.4: Timers We finished the last chapter by examining global interrupt controller. We were able to trace the path of a timer interrupt all the way up to the bcm2836_chained_handle_irq function. Next logical step is to see how the timer driver handles this interrupt. However, before we can do this, you need to familiarize yourself with a few important concepts related to timer functionality. All of them are explained in the official kernel documentation , and I strongly advise you to read this document. But for those who are too busy to read it, I can provide my own brief explanation of the mentioned concepts. Clock sources Each time you need to find out exactly what time it is now you are using clock source framework. Typically the clock source is implemented as a monotonic, atomic n-bit counter, which counts from 0 to 2^(n-1) and then wraps around to 0 and starts over. The clock source also provides means to translate the counter into a nanosecond value. Clock events This abstraction is introduced to allow anybody to subscribe on timer interrupts. Clock events framework takes designed time of the next event as an input and, based on it, calculates appropriate values of the timer hardware registers. sched_clock() This function returns the number of nanoseconds since the system was started. It usually does so by directly reading timer registers. This function is called very frequently and should be optimized for performance. In the next section, we are going to see how system timer is used to implement clock sources, clock events and sched_clock functionality. BCM2835 System Timer. As usual, we start the exploration of a particular device with finding its location in the device tree. System timer node is defined here . You can keep this definition open for a while because we are going to reference it several times. Next, we need to use compatible property to figure out the location of the corresponding driver. The driver can be found here . The first thing we are going to look at is bcm2835_timer structure. struct bcm2835_timer { void __iomem *control; void __iomem *compare; int match_mask; struct clock_event_device evt; struct irqaction act; }; This structure contains all state needed for the driver to function. control and compare fields holds the addresses of the corresponding memory mapped registers, match_mask is used to determine which of the 4 available timer interrupts we are going to use, evt field contains a structure that is passed to clock events framework and act is an irq action that is used to connect the current driver with the interrupt controller. Next we are going to look at bcm2835_timer_init which is the driver initialization function. It is large, but not as difficult as you might think from the beginning. static int __init bcm2835_timer_init(struct device_node *node) { void __iomem *base; u32 freq; int irq, ret; struct bcm2835_timer *timer; base = of_iomap(node, 0); if (!base) { pr_err(\"Can't remap registers\\n\"); return -ENXIO; } ret = of_property_read_u32(node, \"clock-frequency\", &freq); if (ret) { pr_err(\"Can't read clock-frequency\\n\"); goto err_iounmap; } system_clock = base + REG_COUNTER_LO; sched_clock_register(bcm2835_sched_read, 32, freq); clocksource_mmio_init(base + REG_COUNTER_LO, node->name, freq, 300, 32, clocksource_mmio_readl_up); irq = irq_of_parse_and_map(node, DEFAULT_TIMER); if (irq <= 0) { pr_err(\"Can't parse IRQ\\n\"); ret = -EINVAL; goto err_iounmap; } timer = kzalloc(sizeof(*timer), GFP_KERNEL); if (!timer) { ret = -ENOMEM; goto err_iounmap; } timer->control = base + REG_CONTROL; timer->compare = base + REG_COMPARE(DEFAULT_TIMER); timer->match_mask = BIT(DEFAULT_TIMER); timer->evt.name = node->name; timer->evt.rating = 300; timer->evt.features = CLOCK_EVT_FEAT_ONESHOT; timer->evt.set_next_event = bcm2835_time_set_next_event; timer->evt.cpumask = cpumask_of(0); timer->act.name = node->name; timer->act.flags = IRQF_TIMER | IRQF_SHARED; timer->act.dev_id = timer; timer->act.handler = bcm2835_time_interrupt; ret = setup_irq(irq, &timer->act); if (ret) { pr_err(\"Can't set up timer IRQ\\n\"); goto err_iounmap; } clockevents_config_and_register(&timer->evt, freq, 0xf, 0xffffffff); pr_info(\"bcm2835: system timer (irq = %d)\\n\", irq); return 0; err_iounmap: iounmap(base); return ret; } Now let's take a closer look at this function. base = of_iomap(node, 0); if (!base) { pr_err(\"Can't remap registers\\n\"); return -ENXIO; } It starts with mapping memory registers and obtaining register base address. You should be already familiar with this part. ret = of_property_read_u32(node, \"clock-frequency\", &freq); if (ret) { pr_err(\"Can't read clock-frequency\\n\"); goto err_iounmap; } system_clock = base + REG_COUNTER_LO; sched_clock_register(bcm2835_sched_read, 32, freq); Next, sched_clock subsystem is initialized. sched_clock need to access timer counter registers each time it is executed and bcm2835_sched_read is passed as the first argument to assist with this task. The second argument corresponds to the number of bits that the timer counter has (in our case it is 32). the number of bits is used to calculate how soon the counter is going to wrap to 0. The last argument specifies timer frequency - it is used to convert values of the timer counter to nanoseconds. Timer frequency is defined in the device tree at this line. clocksource_mmio_init(base + REG_COUNTER_LO, node->name, freq, 300, 32, clocksource_mmio_readl_up); Next line initializes clock source framework. clocksource_mmio_init initializes a simple clock source based on memory mapped registers. The clock source framework, in some aspects, duplicates the functionality of sched_clock and it needs access to the same 3 basic parameters. The location of the timer counter register. The number of valid bits in the counter. Timer frequency. Another 3 parameters include the name of the clock source, its rating, which is used to rate clock source devices, and a function that can read timer counter register. irq = irq_of_parse_and_map(node, DEFAULT_TIMER); if (irq <= 0) { pr_err(\"Can't parse IRQ\\n\"); ret = -EINVAL; goto err_iounmap; } This code snippet is used to find Linux irq number, corresponding to the third timer interrupt (Number 3 is hardcoded as DEFAULT_TIMER constant). Just a quick reminder: Raspberry Pi system timer has 4 independent set of timer registers, and here the third one is used. If you go back to the device tree, you can find interrupts property. This property describes all interrupts, supported by a device, and how those interrupts are mapped to interrupt controller lines. It is an array, where each item represents one interrupt. The format of the items is specific to the interrupt controller. In our case, each item consists of 2 numbers: the first one specifies an interrupt bank and the second - interrupt number inside the bank. irq_of_parse_and_map reads the value of interrupts property, then it uses the second argument to find which of the supported interrupts we are interested in and returns Linux irq number for the requested interrupt. timer = kzalloc(sizeof(*timer), GFP_KERNEL); if (!timer) { ret = -ENOMEM; goto err_iounmap; } Here memory for bcm2835_timer structure is allocated. timer->control = base + REG_CONTROL; timer->compare = base + REG_COMPARE(DEFAULT_TIMER); timer->match_mask = BIT(DEFAULT_TIMER); Next, the addresses of the control and compare registers are calculated and match_mask is set to the DEFAULT_TIMER constant. timer->evt.name = node->name; timer->evt.rating = 300; timer->evt.features = CLOCK_EVT_FEAT_ONESHOT; timer->evt.set_next_event = bcm2835_time_set_next_event; timer->evt.cpumask = cpumask_of(0); In this code snippet clock_event_device struct is initialized. The most important property here is set_next_event which points to bcm2835_time_set_next_event function. This function is called by the clock events framework to schedule next interrupt. bcm2835_time_set_next_event is very simple - it updates compare register so that interrupt will be scheduled after a desied interval. This is analogaus to what we did here for the RPi OS. timer->act.flags = IRQF_TIMER | IRQF_SHARED; timer->act.dev_id = timer; timer->act.handler = bcm2835_time_interrupt; Next, irq action is initialized. The most important property here is handler , which points to bcm2835_time_interrupt - this is the function that is called after an interrupt is fired. If you take a look at it, you will see that it redirects all work to the event handler, registered by the clock events framework. We will examine this event handler in a while. ret = setup_irq(irq, &timer->act); if (ret) { pr_err(\"Can't set up timer IRQ\\n\"); goto err_iounmap; } After the irq action is configured, it is added to the list of irq actions of the timer interrupt. clockevents_config_and_register(&timer->evt, freq, 0xf, 0xffffffff); And finally clock events framework is initialized by calling clockevents_config_and_register . evt structure and timer frequency are passed as first 2 arguments. Last 2 arguments are used only in \"one-shot\" timer mode and are not relevant to our current discussion. Now, we have traced the path of a timer interrupt all the way up to the bcm2835_time_interrupt function, but we still didn't find the place were the actual work is done. In the next section, we are going to dig even deeper and find out how an interrupt is processed when it enters the clock events framework. How an interrupt is processed in the clock events framework In the previous section, we have seen that the real work of handling a timer interrupt is outsourced to the clock events framework. This is done in the following few lines. event_handler = ACCESS_ONCE(timer->evt.event_handler); if (event_handler) event_handler(&timer->evt); Now our goal will be to figure out were exactly event_handler is set and what happens after it is called. clockevents_config_and_register function is a good place to start the exploration because this is the place where clock events framework is configured and, if we follow the logic of this function, eventually we should find how event_handler is set. Now let me show you the chain of function calls that leads us to the place we need. clockevents_config_and_register This is the top level initialization function. clockevents_register_device In this function the timer is added to the global list of clock event devices. tick_check_new_device This function checks whether the current device is a good candidate to be used as a \"tick device\". If yes, such device will be used to generate periodic ticks that the rest of the kernel will use to do all work that needs to be done on a regular basis. tick_setup_device This function starts device configuration. tick_setup_periodic This is the place were device is configured for periodic tics. tick_set_periodic_handler Finally we reached the place where the handler is assigned! If you take a look at the last function in the call chain, you will see that Linux uses different handlers depending on whether broadcast is enabled or not. Tick broadcast is used to awake idle CPUs, you can read more about it here . But we are going to ignore it and concentrate on a more general tick handler instead. In general case tick_handle_periodic and then tick_periodic functions are called. The later one is exactly the function that we are interested in. Let me copy its content here. /* * Periodic tick */ static void tick_periodic(int cpu) { if (tick_do_timer_cpu == cpu) { write_seqlock(&jiffies_lock); /* Keep track of the next tick event */ tick_next_period = ktime_add(tick_next_period, tick_period); do_timer(1); write_sequnlock(&jiffies_lock); update_wall_time(); } update_process_times(user_mode(get_irq_regs())); profile_tick(CPU_PROFILING); } A few important things are done in this function: tick_next_period is calculated so that next tick event can be scheduled. do_timer is called, which is responsible for setting 'jiffies'. jiffies is a number of ticks since the last system reboot. jiffies can be used in the same way as sched_clock function, in cases when you don't need nanosecond precision. update_process_times is called. This is the place where currently executing process is given a chance to do all work that needed to be done periodically. This work includes, for example, running local process timers, or, most importantly, notifying the scheduler about the tick event. Conclusion Now you see how long is the way of an ordinary timer interrupt, but we followed it from the beginning to the very end. One of the things that are the most important, is that we finally reached the place where the scheduler is called. The scheduler is one of the most critical parts of any operating system and it relies heavily on timer interrupts. So now, when we've seen where the scheduler functionality is triggered, its time to discuss its implementation - that is something we are going to do in the next lesson. Previous Page 3.3 Interrupt handling: Interrupt controllers Next Page 3.5 Interrupt handling: Exercises","title":"Timer"},{"location":"lesson03/linux/timer/#34-timers","text":"We finished the last chapter by examining global interrupt controller. We were able to trace the path of a timer interrupt all the way up to the bcm2836_chained_handle_irq function. Next logical step is to see how the timer driver handles this interrupt. However, before we can do this, you need to familiarize yourself with a few important concepts related to timer functionality. All of them are explained in the official kernel documentation , and I strongly advise you to read this document. But for those who are too busy to read it, I can provide my own brief explanation of the mentioned concepts. Clock sources Each time you need to find out exactly what time it is now you are using clock source framework. Typically the clock source is implemented as a monotonic, atomic n-bit counter, which counts from 0 to 2^(n-1) and then wraps around to 0 and starts over. The clock source also provides means to translate the counter into a nanosecond value. Clock events This abstraction is introduced to allow anybody to subscribe on timer interrupts. Clock events framework takes designed time of the next event as an input and, based on it, calculates appropriate values of the timer hardware registers. sched_clock() This function returns the number of nanoseconds since the system was started. It usually does so by directly reading timer registers. This function is called very frequently and should be optimized for performance. In the next section, we are going to see how system timer is used to implement clock sources, clock events and sched_clock functionality.","title":"3.4: Timers"},{"location":"lesson03/linux/timer/#bcm2835-system-timer","text":"As usual, we start the exploration of a particular device with finding its location in the device tree. System timer node is defined here . You can keep this definition open for a while because we are going to reference it several times. Next, we need to use compatible property to figure out the location of the corresponding driver. The driver can be found here . The first thing we are going to look at is bcm2835_timer structure. struct bcm2835_timer { void __iomem *control; void __iomem *compare; int match_mask; struct clock_event_device evt; struct irqaction act; }; This structure contains all state needed for the driver to function. control and compare fields holds the addresses of the corresponding memory mapped registers, match_mask is used to determine which of the 4 available timer interrupts we are going to use, evt field contains a structure that is passed to clock events framework and act is an irq action that is used to connect the current driver with the interrupt controller. Next we are going to look at bcm2835_timer_init which is the driver initialization function. It is large, but not as difficult as you might think from the beginning. static int __init bcm2835_timer_init(struct device_node *node) { void __iomem *base; u32 freq; int irq, ret; struct bcm2835_timer *timer; base = of_iomap(node, 0); if (!base) { pr_err(\"Can't remap registers\\n\"); return -ENXIO; } ret = of_property_read_u32(node, \"clock-frequency\", &freq); if (ret) { pr_err(\"Can't read clock-frequency\\n\"); goto err_iounmap; } system_clock = base + REG_COUNTER_LO; sched_clock_register(bcm2835_sched_read, 32, freq); clocksource_mmio_init(base + REG_COUNTER_LO, node->name, freq, 300, 32, clocksource_mmio_readl_up); irq = irq_of_parse_and_map(node, DEFAULT_TIMER); if (irq <= 0) { pr_err(\"Can't parse IRQ\\n\"); ret = -EINVAL; goto err_iounmap; } timer = kzalloc(sizeof(*timer), GFP_KERNEL); if (!timer) { ret = -ENOMEM; goto err_iounmap; } timer->control = base + REG_CONTROL; timer->compare = base + REG_COMPARE(DEFAULT_TIMER); timer->match_mask = BIT(DEFAULT_TIMER); timer->evt.name = node->name; timer->evt.rating = 300; timer->evt.features = CLOCK_EVT_FEAT_ONESHOT; timer->evt.set_next_event = bcm2835_time_set_next_event; timer->evt.cpumask = cpumask_of(0); timer->act.name = node->name; timer->act.flags = IRQF_TIMER | IRQF_SHARED; timer->act.dev_id = timer; timer->act.handler = bcm2835_time_interrupt; ret = setup_irq(irq, &timer->act); if (ret) { pr_err(\"Can't set up timer IRQ\\n\"); goto err_iounmap; } clockevents_config_and_register(&timer->evt, freq, 0xf, 0xffffffff); pr_info(\"bcm2835: system timer (irq = %d)\\n\", irq); return 0; err_iounmap: iounmap(base); return ret; } Now let's take a closer look at this function. base = of_iomap(node, 0); if (!base) { pr_err(\"Can't remap registers\\n\"); return -ENXIO; } It starts with mapping memory registers and obtaining register base address. You should be already familiar with this part. ret = of_property_read_u32(node, \"clock-frequency\", &freq); if (ret) { pr_err(\"Can't read clock-frequency\\n\"); goto err_iounmap; } system_clock = base + REG_COUNTER_LO; sched_clock_register(bcm2835_sched_read, 32, freq); Next, sched_clock subsystem is initialized. sched_clock need to access timer counter registers each time it is executed and bcm2835_sched_read is passed as the first argument to assist with this task. The second argument corresponds to the number of bits that the timer counter has (in our case it is 32). the number of bits is used to calculate how soon the counter is going to wrap to 0. The last argument specifies timer frequency - it is used to convert values of the timer counter to nanoseconds. Timer frequency is defined in the device tree at this line. clocksource_mmio_init(base + REG_COUNTER_LO, node->name, freq, 300, 32, clocksource_mmio_readl_up); Next line initializes clock source framework. clocksource_mmio_init initializes a simple clock source based on memory mapped registers. The clock source framework, in some aspects, duplicates the functionality of sched_clock and it needs access to the same 3 basic parameters. The location of the timer counter register. The number of valid bits in the counter. Timer frequency. Another 3 parameters include the name of the clock source, its rating, which is used to rate clock source devices, and a function that can read timer counter register. irq = irq_of_parse_and_map(node, DEFAULT_TIMER); if (irq <= 0) { pr_err(\"Can't parse IRQ\\n\"); ret = -EINVAL; goto err_iounmap; } This code snippet is used to find Linux irq number, corresponding to the third timer interrupt (Number 3 is hardcoded as DEFAULT_TIMER constant). Just a quick reminder: Raspberry Pi system timer has 4 independent set of timer registers, and here the third one is used. If you go back to the device tree, you can find interrupts property. This property describes all interrupts, supported by a device, and how those interrupts are mapped to interrupt controller lines. It is an array, where each item represents one interrupt. The format of the items is specific to the interrupt controller. In our case, each item consists of 2 numbers: the first one specifies an interrupt bank and the second - interrupt number inside the bank. irq_of_parse_and_map reads the value of interrupts property, then it uses the second argument to find which of the supported interrupts we are interested in and returns Linux irq number for the requested interrupt. timer = kzalloc(sizeof(*timer), GFP_KERNEL); if (!timer) { ret = -ENOMEM; goto err_iounmap; } Here memory for bcm2835_timer structure is allocated. timer->control = base + REG_CONTROL; timer->compare = base + REG_COMPARE(DEFAULT_TIMER); timer->match_mask = BIT(DEFAULT_TIMER); Next, the addresses of the control and compare registers are calculated and match_mask is set to the DEFAULT_TIMER constant. timer->evt.name = node->name; timer->evt.rating = 300; timer->evt.features = CLOCK_EVT_FEAT_ONESHOT; timer->evt.set_next_event = bcm2835_time_set_next_event; timer->evt.cpumask = cpumask_of(0); In this code snippet clock_event_device struct is initialized. The most important property here is set_next_event which points to bcm2835_time_set_next_event function. This function is called by the clock events framework to schedule next interrupt. bcm2835_time_set_next_event is very simple - it updates compare register so that interrupt will be scheduled after a desied interval. This is analogaus to what we did here for the RPi OS. timer->act.flags = IRQF_TIMER | IRQF_SHARED; timer->act.dev_id = timer; timer->act.handler = bcm2835_time_interrupt; Next, irq action is initialized. The most important property here is handler , which points to bcm2835_time_interrupt - this is the function that is called after an interrupt is fired. If you take a look at it, you will see that it redirects all work to the event handler, registered by the clock events framework. We will examine this event handler in a while. ret = setup_irq(irq, &timer->act); if (ret) { pr_err(\"Can't set up timer IRQ\\n\"); goto err_iounmap; } After the irq action is configured, it is added to the list of irq actions of the timer interrupt. clockevents_config_and_register(&timer->evt, freq, 0xf, 0xffffffff); And finally clock events framework is initialized by calling clockevents_config_and_register . evt structure and timer frequency are passed as first 2 arguments. Last 2 arguments are used only in \"one-shot\" timer mode and are not relevant to our current discussion. Now, we have traced the path of a timer interrupt all the way up to the bcm2835_time_interrupt function, but we still didn't find the place were the actual work is done. In the next section, we are going to dig even deeper and find out how an interrupt is processed when it enters the clock events framework.","title":"BCM2835 System Timer."},{"location":"lesson03/linux/timer/#how-an-interrupt-is-processed-in-the-clock-events-framework","text":"In the previous section, we have seen that the real work of handling a timer interrupt is outsourced to the clock events framework. This is done in the following few lines. event_handler = ACCESS_ONCE(timer->evt.event_handler); if (event_handler) event_handler(&timer->evt); Now our goal will be to figure out were exactly event_handler is set and what happens after it is called. clockevents_config_and_register function is a good place to start the exploration because this is the place where clock events framework is configured and, if we follow the logic of this function, eventually we should find how event_handler is set. Now let me show you the chain of function calls that leads us to the place we need. clockevents_config_and_register This is the top level initialization function. clockevents_register_device In this function the timer is added to the global list of clock event devices. tick_check_new_device This function checks whether the current device is a good candidate to be used as a \"tick device\". If yes, such device will be used to generate periodic ticks that the rest of the kernel will use to do all work that needs to be done on a regular basis. tick_setup_device This function starts device configuration. tick_setup_periodic This is the place were device is configured for periodic tics. tick_set_periodic_handler Finally we reached the place where the handler is assigned! If you take a look at the last function in the call chain, you will see that Linux uses different handlers depending on whether broadcast is enabled or not. Tick broadcast is used to awake idle CPUs, you can read more about it here . But we are going to ignore it and concentrate on a more general tick handler instead. In general case tick_handle_periodic and then tick_periodic functions are called. The later one is exactly the function that we are interested in. Let me copy its content here. /* * Periodic tick */ static void tick_periodic(int cpu) { if (tick_do_timer_cpu == cpu) { write_seqlock(&jiffies_lock); /* Keep track of the next tick event */ tick_next_period = ktime_add(tick_next_period, tick_period); do_timer(1); write_sequnlock(&jiffies_lock); update_wall_time(); } update_process_times(user_mode(get_irq_regs())); profile_tick(CPU_PROFILING); } A few important things are done in this function: tick_next_period is calculated so that next tick event can be scheduled. do_timer is called, which is responsible for setting 'jiffies'. jiffies is a number of ticks since the last system reboot. jiffies can be used in the same way as sched_clock function, in cases when you don't need nanosecond precision. update_process_times is called. This is the place where currently executing process is given a chance to do all work that needed to be done periodically. This work includes, for example, running local process timers, or, most importantly, notifying the scheduler about the tick event.","title":"How an interrupt is processed in the clock events framework"},{"location":"lesson03/linux/timer/#conclusion","text":"Now you see how long is the way of an ordinary timer interrupt, but we followed it from the beginning to the very end. One of the things that are the most important, is that we finally reached the place where the scheduler is called. The scheduler is one of the most critical parts of any operating system and it relies heavily on timer interrupts. So now, when we've seen where the scheduler functionality is triggered, its time to discuss its implementation - that is something we are going to do in the next lesson.","title":"Conclusion"},{"location":"lesson03/linux/timer/#previous-page","text":"3.3 Interrupt handling: Interrupt controllers","title":"Previous Page"},{"location":"lesson03/linux/timer/#next-page","text":"3.5 Interrupt handling: Exercises","title":"Next Page"},{"location":"lesson04a/exercises/","text":"Exercises Modify the kernel to add an idle task. What should be executed in the body of the idle task? Add WAIT state to tasks. How are we going to bookkeep tasks in WAIT state? How do you demonstrate that your implementation is correct? (optional) Modify the kernel: allow it to have an unlimited number of tasks (right now the limit is 64). Deliverable A code tarball implementing (1) above. A code tarball implementing (2) above. A code tarball implementing (3) above.","title":"Exercises"},{"location":"lesson04a/exercises/#exercises","text":"Modify the kernel to add an idle task. What should be executed in the body of the idle task? Add WAIT state to tasks. How are we going to bookkeep tasks in WAIT state? How do you demonstrate that your implementation is correct? (optional) Modify the kernel: allow it to have an unlimited number of tasks (right now the limit is 64).","title":"Exercises"},{"location":"lesson04a/exercises/#deliverable","text":"A code tarball implementing (1) above. A code tarball implementing (2) above. A code tarball implementing (3) above.","title":"Deliverable"},{"location":"lesson04a/rpi-os/","text":"4a: Cooperative Multitasking Objectives We will build a minimum kernel that can schedule multiple cooperative tasks. Roadmap From this experiment onward, our kernel starts to schedule multiple tasks. This makes it a true \"kernel\" instead of a baremetal program. We will intentionally leave out interrupts, i.e. timer interrupts are OFF . Tasks must voluntarily yield to each other. As a result, we focus on scheduling and task switch and defer treatment of interrupt handling to upcoming experiment. . We will implement: The task_struct data structure Task creation by manipulating task_struct , registers, and stack Minimalist memory allocation A minimalist task scheduler Processes vs tasks . As we do not have virtual memory yet, we use the term \"tasks\" instead of \"processes\". Code Walkthrough task_struct To manage tasks, the first thing we should do is to create a struct that describes a task. Linux has such a struct and it is called task_struct (in Linux both thread and processes are just different types of tasks; the difference is in how they share address spaces). As we are mostly mimicking Linux implementation, we are going to do the same. Our task_struct looks like the following. struct cpu_context { unsigned long x19; unsigned long x20; unsigned long x21; unsigned long x22; unsigned long x23; unsigned long x24; unsigned long x25; unsigned long x26; unsigned long x27; unsigned long x28; unsigned long fp; unsigned long sp; unsigned long pc; }; struct task_struct { struct cpu_context cpu_context; long state; long counter; long priority; long preempt_count; }; This struct has the following members: cpu_context This is an embedded structure that contains values of all registers that might be different between the tasks, that are being switched. A reasonable question to ask is why do we save not all registers, but only registers x19 - x30 and sp ? ( fp is x29 and pc is x30 ) The answer is that task switch happens only when a task calls cpu_switch_to function. So, from the point of view of the task that is being switched, it just calls cpu_switch_to function and it returns after some (potentially long) time. The \"switched from\" task doesn't notice that another task happens to runs during this period. Accordingly to ARM calling conventions registers x0 - x18 can be overwritten by the callee, so the caller must not assume that the values of those registers will survive after a function call. That's why it doesn't make sense to save x0 - x18 registers. state This is the state of the currently running task (note: this is NOT CPU state which is an orthogonal concept). For tasks that are just doing some work on the CPU the state will always be TASK_RUNNING . Actually, this is the only state that the RPi OS is going to support for now. Later we may add a few additional states. For example, a task that is waiting for an interrupt should be moved to a different state, because it doesn't make sense to awake the task while the required interrupt hasn't yet happened. counter This field is used to determine how long the current task has been running. counter decreases by 1 each timer tick and when it reaches 0 another task is scheduled. This supports our simple scheduling algorithm. priority When a new task is scheduled its priority is copied to counter . By setting tasks priority, we can regulate the amount of processor time that the task gets relative to other tasks. preempt_count If this field has a non-zero value it is an indicator that right now the current task is executing some critical function that must not be interrupted (for example, it runs the scheduling function.). If timer tick occurs at such time it is ignored and rescheduling is not triggered. After the kernel startup, there is only one task running: the one that runs kernel_main function. It is called \"init task\". Before the scheduler functionality is enabled, we must fill task_struct corresponding to the init task. This is done here . All task_struct s are stored in task array. This array has only 64 slots - that is the maximum number of simultaneous tasks that we can have in the RPi OS. It is definitely not the best solution for the production-ready OS, but it is ok for our goals. A very important global variable called current that always points to the task_struct of currently executing task. Both current and task array are initially set to hold a pointer to the init task. There is also a global variable called nr_tasks - it contains the number of currently running tasks in the system. Those are all structures and global variables that we are going to use to implement the scheduler functionality. In the description of the task_struct I already briefly mentioned some aspects of how scheduling works, because it is impossible to understand the meaning of a particular task_struct field without understanding how this field is used. Now we are going to examine the scheduling algorithm in much more details and we will start with the kernel_main function. kernel_main() Before we dig into the scheduler implementation, I want to quickly show you how we are going to prove that the scheduler actually works. To understand it, you need to take a look at the kernel.c file. Let me copy the relevant content here. void kernel_main(void) { uart_init(); init_printf(0, putc); irq_vector_init(); int res = copy_process((unsigned long)&process, (unsigned long)\"12345\"); if (res != 0) { printf(\"error while starting process 1\"); return; } res = copy_process((unsigned long)&process, (unsigned long)\"abcde\"); if (res != 0) { printf(\"error while starting process 2\"); return; } while (1){ schedule(); } } There are a few important things about this code. New function copy_process is introduced. copy_process takes 2 arguments: a function to execute in a new thread and an argument that need to be passed to this function. copy_process allocates a new task_struct and makes it available for the scheduler. Another new function for us is called schedule . This is the core scheduler function: it checks whether there is a new task that needs to preempt the current one. In cooperative scheduling, a task voluntarily calls schedule if it doesn't have any work to do at the moment. Spoiler: for preemptive multitasking, schedule is also called from the timer interrupt handler. We are calling copy_process 2 times, each time passing a pointer to the process function as the first argument. process function is very simple. void process(char *array) { while (1){ for (int i = 0; i < 5; i++){ uart_send(array[i]); delay(100000); schedule(); } } } It just keeps printing on the screen characters from the array, that is passed as an argument The first time it is called with the argument \"12345\" and second time the argument is \"abcde\". After printing out a string, a task yields to others by calling schedule() . If our scheduler implementation is correct, we should see on the output from both threads. Switching tasks (sched.c & sched.S) This is where the magic happens. The code looks like this. void switch_to(struct task_struct * next) { if (current == next) return; struct task_struct * prev = current; current = next; cpu_switch_to(prev, next); } Here we check that next process is not the same as the current, and if not, current variable is updated. The actual work is redirected to cpu_switch_to function. It is in assembly as it manipulates registers. .globl cpu_switch_to cpu_switch_to: mov x10, #THREAD_CPU_CONTEXT add x8, x0, x10 mov x9, sp stp x19, x20, [x8], #16 // store callee-saved registers stp x21, x22, [x8], #16 stp x23, x24, [x8], #16 stp x25, x26, [x8], #16 stp x27, x28, [x8], #16 stp x29, x9, [x8], #16 str x30, [x8] add x8, x1, x10 ldp x19, x20, [x8], #16 // restore callee-saved registers ldp x21, x22, [x8], #16 ldp x23, x24, [x8], #16 ldp x25, x26, [x8], #16 ldp x27, x28, [x8], #16 ldp x29, x9, [x8], #16 ldr x30, [x8] mov sp, x9 ret This is the place where the real context switch happens. Let's examine it line by line. mov x10, #THREAD_CPU_CONTEXT add x8, x0, x10 THREAD_CPU_CONTEXT constant contains offset of the cpu_context structure in the task_struct . x0 contains a pointer to the first argument, which is the current task_struct (i.e. the \"switch_from\" task). After the copied 2 lines are executed, x8 will contain a pointer to the current cpu_context . mov x9, sp stp x19, x20, [x8], #16 // store callee-saved registers stp x21, x22, [x8], #16 stp x23, x24, [x8], #16 stp x25, x26, [x8], #16 stp x27, x28, [x8], #16 stp x29, x9, [x8], #16 str x30, [x8] Next all callee-saved registers are stored in the order, in which they are defined in cpu_context structure. The current stack pointer is saved as cpu_context.sp and x29 is saved as cpu_context.fp (frame pointer). Note: x30 , the link register containing function return address, is stored as cpu_context.pc . Why? Now we calculate the address of the next task's cpu_context : add x8, x1, x10 This a cute hack. x10 contains an offset of the cpu_context structure inside task_struct , x1 is a pointer to the next task_struct , so x8 will contain a pointer to the next cpu_context . Now, restore the CPU context of \"switch_to\" task from memory to CPU regs. A mirror procedure. ldp x19, x20, [x8], #16 // restore callee-saved registers ldp x21, x22, [x8], #16 ldp x23, x24, [x8], #16 ldp x25, x26, [x8], #16 ldp x27, x28, [x8], #16 ldp x29, x9, [x8], #16 ldr x30, [x8] mov sp, x9 ret After ret , kernel returns to the location pointed to by the link register ( x30 ). If we are switching to a task for the first time, this will be ret_from_fork function. More on it below. In all other cases this will be the location, previously saved in the cpu_context.pc by the cpu_switch_to function. Think: which instruction does it point to? Creating a new task After seeing task switch, new task creation starts to make more sense. It is implemented in copy_process function. Keep in mind: after copy_process finishes execution, no context switch happens. The function only prepares new task_struct and adds it to the task array \u2014 this task will be executed only after schedule function is called. int copy_process(unsigned long fn, unsigned long arg) { struct task_struct *p; p = (struct task_struct *) get_free_page(); if (!p) return 1; p->priority = current->priority; p->state = TASK_RUNNING; p->counter = p->priority; p->cpu_context.x19 = fn; p->cpu_context.x20 = arg; p->cpu_context.pc = (unsigned long)ret_from_fork; p->cpu_context.sp = (unsigned long)p + THREAD_SIZE; int pid = nr_tasks++; task[pid] = p; return 0; } Now, we are going to examine it in details. struct task_struct *p; The function starts with allocating a pointer for the new task. As interrupts are off, the kernel will not be interrupted in the middle of the copy_process function. p = (struct task_struct *) get_free_page(); if (!p) return 1; Next, a new page is allocated. At the bottom of this page, we are putting the task_struct for the newly created task. The rest of this page will be used as the task stack. A few lines below, context.sp is set as p + THREAD_SIZE . THREAD_SIZE is defined as 4KB. It is the total amount of memory for a task. The name, again, is following the Linux kernel convention. p->priority = current->priority; p->state = TASK_RUNNING; p->counter = p->priority; After the task_struct is allocated, we can initialize its properties. Priority and initial counters are set based on the current task priority. p->cpu_context.x19 = fn; p->cpu_context.x20 = arg; p->cpu_context.pc = (unsigned long)ret_from_fork; p->cpu_context.sp = (unsigned long)p + THREAD_SIZE; This is the most important part of the function. Here cpu_context is initialized. The stack pointer is set to the top of the newly allocated memory page. pc is set to the ret_from_fork function, and we need to look at this function now in order to understand why the rest of the cpu_context registers are initialized in the way they are. ret_from_fork (entry.S) This is the first piece of code executed by a newly created process. A new process P executes ret_from_fork after it is switched to for the first time. That is right after the scheduler picks P for the first time and restores P's CPU context from task_struct to CPU registers. Throughout its lifetime, P only executes ret_from_fork once. About naming: despite the name \"fork\", we are not doing fork() as in Linux/Unix. We are simply copying a task_struct while fork() does far more things like duplicating process address spaces. The naming follows the Linux kernel convention; and we will evolve our ret_from_fork in subsequent experiments. .globl ret_from_fork ret_from_fork: bl schedule_tail // will talk about this later mov x0, x20 blr x19 //should never return Where do x19 and x20 come from? See code copy_process above, which saves fn (the process's main function) and arg (the argument passed to the process) to task_struct . When switching to P, the kernel restores fn and arg from task_struct to x19 and x20 . As a result, ret_from_fork calls the function stored in x19 register with the argument stored in x20 . Aside: Memory allocation Each task in the system should have its dedicated stack. That's why when creating a new task we must have a way to allocate memory. For now, our memory allocator is extremely primitive. (The implementation can be found in mm.c file) static unsigned short mem_map [ PAGING_PAGES ] = {0,}; unsigned long get_free_page() { for (int i = 0; i < PAGING_PAGES; i++){ if (mem_map[i] == 0){ mem_map[i] = 1; return LOW_MEMORY + i*PAGE_SIZE; } } return 0; } void free_page(unsigned long p){ mem_map[(p - LOW_MEMORY) / PAGE_SIZE] = 0; } The allocator can work only with memory pages (each page is 4 KB in size). There is an array called mem_map that for each page in the system holds its status: whether it is allocated or free. Whenever we need to allocate a new page, we just loop through this array and return the first free page. This implementation is based on 2 assumptions: We know the total amount of memory in the system. It is 1 GB - 1 MB (the last megabyte of memory is reserved for device registers.). This value is stored in the HIGH_MEMORY constant. First 4 MB of memory are reserved for the kernel image and init task stack. This value is stored in LOW_MEMORY constant. All memory allocations start right after this point. Note: even with QEMU our kernel must start from 0x80000 (512KB), the above assumptions are good as there's still plenty room in 512KB -- LOW_MEMORY for our tiny kernel. Scheduling algorithm Finally, we are ready to look at the scheduler algorithm. I almost precisely copied this algorithm from the first release of the Linux kernel. You can find the original version here . void _schedule(void) { int next,c; struct task_struct * p; while (1) { c = -1; next = 0; // try to pick a task for (int i = 0; i < NR_TASKS; i++){ p = task[i]; if (p && p->state == TASK_RUNNING && p->counter > c) { c = p->counter; next = i; } } if (c) { break; } // update counters for (int i = 0; i < NR_TASKS; i++) { p = task[i]; if (p) { p->counter = (p->counter >> 1) + p->priority; } } } switch_to(task[next]); } The simple algorithm works like the following: The first for loop iterates over all tasks and tries to find a task in TASK_RUNNING state with the maximum counter. If such a task is found, we immediately break from the while loop and switch to this task. If no such task is found, this is either because i) no task is in TASK_RUNNING state or ii) all such tasks have 0 counters. In a real OS, i) might happen, for example, when all tasks are waiting for an interrupt. In our current tiny kernel, all tasks are always in TASK_RUNNING (Why?) The scheduler moves to the 2nd for loop to \"recharge\" counters. It bumps counters for all tasks once. The increment depends on a task's priority. Note: a task counter can never get larger than 2 * priority . With updated counters, the scheduler goes back to the 1st for loop to pick a task. We will augment the scheduling algorithm for preemptive multitasking later. Conclusion We have seen important nuts & bolts of multitasking. The subsequent experiment will enable task preemption. We will show a detailed workflow of context switch there.","title":"4a: Cooperative Multitasking"},{"location":"lesson04a/rpi-os/#4a-cooperative-multitasking","text":"","title":"4a: Cooperative Multitasking"},{"location":"lesson04a/rpi-os/#objectives","text":"We will build a minimum kernel that can schedule multiple cooperative tasks.","title":"Objectives"},{"location":"lesson04a/rpi-os/#roadmap","text":"From this experiment onward, our kernel starts to schedule multiple tasks. This makes it a true \"kernel\" instead of a baremetal program. We will intentionally leave out interrupts, i.e. timer interrupts are OFF . Tasks must voluntarily yield to each other. As a result, we focus on scheduling and task switch and defer treatment of interrupt handling to upcoming experiment. . We will implement: The task_struct data structure Task creation by manipulating task_struct , registers, and stack Minimalist memory allocation A minimalist task scheduler Processes vs tasks . As we do not have virtual memory yet, we use the term \"tasks\" instead of \"processes\".","title":"Roadmap"},{"location":"lesson04a/rpi-os/#code-walkthrough","text":"","title":"Code Walkthrough"},{"location":"lesson04a/rpi-os/#task_struct","text":"To manage tasks, the first thing we should do is to create a struct that describes a task. Linux has such a struct and it is called task_struct (in Linux both thread and processes are just different types of tasks; the difference is in how they share address spaces). As we are mostly mimicking Linux implementation, we are going to do the same. Our task_struct looks like the following. struct cpu_context { unsigned long x19; unsigned long x20; unsigned long x21; unsigned long x22; unsigned long x23; unsigned long x24; unsigned long x25; unsigned long x26; unsigned long x27; unsigned long x28; unsigned long fp; unsigned long sp; unsigned long pc; }; struct task_struct { struct cpu_context cpu_context; long state; long counter; long priority; long preempt_count; }; This struct has the following members: cpu_context This is an embedded structure that contains values of all registers that might be different between the tasks, that are being switched. A reasonable question to ask is why do we save not all registers, but only registers x19 - x30 and sp ? ( fp is x29 and pc is x30 ) The answer is that task switch happens only when a task calls cpu_switch_to function. So, from the point of view of the task that is being switched, it just calls cpu_switch_to function and it returns after some (potentially long) time. The \"switched from\" task doesn't notice that another task happens to runs during this period. Accordingly to ARM calling conventions registers x0 - x18 can be overwritten by the callee, so the caller must not assume that the values of those registers will survive after a function call. That's why it doesn't make sense to save x0 - x18 registers. state This is the state of the currently running task (note: this is NOT CPU state which is an orthogonal concept). For tasks that are just doing some work on the CPU the state will always be TASK_RUNNING . Actually, this is the only state that the RPi OS is going to support for now. Later we may add a few additional states. For example, a task that is waiting for an interrupt should be moved to a different state, because it doesn't make sense to awake the task while the required interrupt hasn't yet happened. counter This field is used to determine how long the current task has been running. counter decreases by 1 each timer tick and when it reaches 0 another task is scheduled. This supports our simple scheduling algorithm. priority When a new task is scheduled its priority is copied to counter . By setting tasks priority, we can regulate the amount of processor time that the task gets relative to other tasks. preempt_count If this field has a non-zero value it is an indicator that right now the current task is executing some critical function that must not be interrupted (for example, it runs the scheduling function.). If timer tick occurs at such time it is ignored and rescheduling is not triggered. After the kernel startup, there is only one task running: the one that runs kernel_main function. It is called \"init task\". Before the scheduler functionality is enabled, we must fill task_struct corresponding to the init task. This is done here . All task_struct s are stored in task array. This array has only 64 slots - that is the maximum number of simultaneous tasks that we can have in the RPi OS. It is definitely not the best solution for the production-ready OS, but it is ok for our goals. A very important global variable called current that always points to the task_struct of currently executing task. Both current and task array are initially set to hold a pointer to the init task. There is also a global variable called nr_tasks - it contains the number of currently running tasks in the system. Those are all structures and global variables that we are going to use to implement the scheduler functionality. In the description of the task_struct I already briefly mentioned some aspects of how scheduling works, because it is impossible to understand the meaning of a particular task_struct field without understanding how this field is used. Now we are going to examine the scheduling algorithm in much more details and we will start with the kernel_main function.","title":"task_struct"},{"location":"lesson04a/rpi-os/#kernel_main","text":"Before we dig into the scheduler implementation, I want to quickly show you how we are going to prove that the scheduler actually works. To understand it, you need to take a look at the kernel.c file. Let me copy the relevant content here. void kernel_main(void) { uart_init(); init_printf(0, putc); irq_vector_init(); int res = copy_process((unsigned long)&process, (unsigned long)\"12345\"); if (res != 0) { printf(\"error while starting process 1\"); return; } res = copy_process((unsigned long)&process, (unsigned long)\"abcde\"); if (res != 0) { printf(\"error while starting process 2\"); return; } while (1){ schedule(); } } There are a few important things about this code. New function copy_process is introduced. copy_process takes 2 arguments: a function to execute in a new thread and an argument that need to be passed to this function. copy_process allocates a new task_struct and makes it available for the scheduler. Another new function for us is called schedule . This is the core scheduler function: it checks whether there is a new task that needs to preempt the current one. In cooperative scheduling, a task voluntarily calls schedule if it doesn't have any work to do at the moment. Spoiler: for preemptive multitasking, schedule is also called from the timer interrupt handler. We are calling copy_process 2 times, each time passing a pointer to the process function as the first argument. process function is very simple. void process(char *array) { while (1){ for (int i = 0; i < 5; i++){ uart_send(array[i]); delay(100000); schedule(); } } } It just keeps printing on the screen characters from the array, that is passed as an argument The first time it is called with the argument \"12345\" and second time the argument is \"abcde\". After printing out a string, a task yields to others by calling schedule() . If our scheduler implementation is correct, we should see on the output from both threads.","title":"kernel_main()"},{"location":"lesson04a/rpi-os/#switching-tasks-schedc-scheds","text":"This is where the magic happens. The code looks like this. void switch_to(struct task_struct * next) { if (current == next) return; struct task_struct * prev = current; current = next; cpu_switch_to(prev, next); } Here we check that next process is not the same as the current, and if not, current variable is updated. The actual work is redirected to cpu_switch_to function. It is in assembly as it manipulates registers. .globl cpu_switch_to cpu_switch_to: mov x10, #THREAD_CPU_CONTEXT add x8, x0, x10 mov x9, sp stp x19, x20, [x8], #16 // store callee-saved registers stp x21, x22, [x8], #16 stp x23, x24, [x8], #16 stp x25, x26, [x8], #16 stp x27, x28, [x8], #16 stp x29, x9, [x8], #16 str x30, [x8] add x8, x1, x10 ldp x19, x20, [x8], #16 // restore callee-saved registers ldp x21, x22, [x8], #16 ldp x23, x24, [x8], #16 ldp x25, x26, [x8], #16 ldp x27, x28, [x8], #16 ldp x29, x9, [x8], #16 ldr x30, [x8] mov sp, x9 ret This is the place where the real context switch happens. Let's examine it line by line. mov x10, #THREAD_CPU_CONTEXT add x8, x0, x10 THREAD_CPU_CONTEXT constant contains offset of the cpu_context structure in the task_struct . x0 contains a pointer to the first argument, which is the current task_struct (i.e. the \"switch_from\" task). After the copied 2 lines are executed, x8 will contain a pointer to the current cpu_context . mov x9, sp stp x19, x20, [x8], #16 // store callee-saved registers stp x21, x22, [x8], #16 stp x23, x24, [x8], #16 stp x25, x26, [x8], #16 stp x27, x28, [x8], #16 stp x29, x9, [x8], #16 str x30, [x8] Next all callee-saved registers are stored in the order, in which they are defined in cpu_context structure. The current stack pointer is saved as cpu_context.sp and x29 is saved as cpu_context.fp (frame pointer). Note: x30 , the link register containing function return address, is stored as cpu_context.pc . Why? Now we calculate the address of the next task's cpu_context : add x8, x1, x10 This a cute hack. x10 contains an offset of the cpu_context structure inside task_struct , x1 is a pointer to the next task_struct , so x8 will contain a pointer to the next cpu_context . Now, restore the CPU context of \"switch_to\" task from memory to CPU regs. A mirror procedure. ldp x19, x20, [x8], #16 // restore callee-saved registers ldp x21, x22, [x8], #16 ldp x23, x24, [x8], #16 ldp x25, x26, [x8], #16 ldp x27, x28, [x8], #16 ldp x29, x9, [x8], #16 ldr x30, [x8] mov sp, x9 ret After ret , kernel returns to the location pointed to by the link register ( x30 ). If we are switching to a task for the first time, this will be ret_from_fork function. More on it below. In all other cases this will be the location, previously saved in the cpu_context.pc by the cpu_switch_to function. Think: which instruction does it point to?","title":"Switching tasks (sched.c &amp; sched.S)"},{"location":"lesson04a/rpi-os/#creating-a-new-task","text":"After seeing task switch, new task creation starts to make more sense. It is implemented in copy_process function. Keep in mind: after copy_process finishes execution, no context switch happens. The function only prepares new task_struct and adds it to the task array \u2014 this task will be executed only after schedule function is called. int copy_process(unsigned long fn, unsigned long arg) { struct task_struct *p; p = (struct task_struct *) get_free_page(); if (!p) return 1; p->priority = current->priority; p->state = TASK_RUNNING; p->counter = p->priority; p->cpu_context.x19 = fn; p->cpu_context.x20 = arg; p->cpu_context.pc = (unsigned long)ret_from_fork; p->cpu_context.sp = (unsigned long)p + THREAD_SIZE; int pid = nr_tasks++; task[pid] = p; return 0; } Now, we are going to examine it in details. struct task_struct *p; The function starts with allocating a pointer for the new task. As interrupts are off, the kernel will not be interrupted in the middle of the copy_process function. p = (struct task_struct *) get_free_page(); if (!p) return 1; Next, a new page is allocated. At the bottom of this page, we are putting the task_struct for the newly created task. The rest of this page will be used as the task stack. A few lines below, context.sp is set as p + THREAD_SIZE . THREAD_SIZE is defined as 4KB. It is the total amount of memory for a task. The name, again, is following the Linux kernel convention. p->priority = current->priority; p->state = TASK_RUNNING; p->counter = p->priority; After the task_struct is allocated, we can initialize its properties. Priority and initial counters are set based on the current task priority. p->cpu_context.x19 = fn; p->cpu_context.x20 = arg; p->cpu_context.pc = (unsigned long)ret_from_fork; p->cpu_context.sp = (unsigned long)p + THREAD_SIZE; This is the most important part of the function. Here cpu_context is initialized. The stack pointer is set to the top of the newly allocated memory page. pc is set to the ret_from_fork function, and we need to look at this function now in order to understand why the rest of the cpu_context registers are initialized in the way they are.","title":"Creating a new task"},{"location":"lesson04a/rpi-os/#ret_from_fork-entrys","text":"This is the first piece of code executed by a newly created process. A new process P executes ret_from_fork after it is switched to for the first time. That is right after the scheduler picks P for the first time and restores P's CPU context from task_struct to CPU registers. Throughout its lifetime, P only executes ret_from_fork once. About naming: despite the name \"fork\", we are not doing fork() as in Linux/Unix. We are simply copying a task_struct while fork() does far more things like duplicating process address spaces. The naming follows the Linux kernel convention; and we will evolve our ret_from_fork in subsequent experiments. .globl ret_from_fork ret_from_fork: bl schedule_tail // will talk about this later mov x0, x20 blr x19 //should never return Where do x19 and x20 come from? See code copy_process above, which saves fn (the process's main function) and arg (the argument passed to the process) to task_struct . When switching to P, the kernel restores fn and arg from task_struct to x19 and x20 . As a result, ret_from_fork calls the function stored in x19 register with the argument stored in x20 .","title":"ret_from_fork (entry.S)"},{"location":"lesson04a/rpi-os/#aside-memory-allocation","text":"Each task in the system should have its dedicated stack. That's why when creating a new task we must have a way to allocate memory. For now, our memory allocator is extremely primitive. (The implementation can be found in mm.c file) static unsigned short mem_map [ PAGING_PAGES ] = {0,}; unsigned long get_free_page() { for (int i = 0; i < PAGING_PAGES; i++){ if (mem_map[i] == 0){ mem_map[i] = 1; return LOW_MEMORY + i*PAGE_SIZE; } } return 0; } void free_page(unsigned long p){ mem_map[(p - LOW_MEMORY) / PAGE_SIZE] = 0; } The allocator can work only with memory pages (each page is 4 KB in size). There is an array called mem_map that for each page in the system holds its status: whether it is allocated or free. Whenever we need to allocate a new page, we just loop through this array and return the first free page. This implementation is based on 2 assumptions: We know the total amount of memory in the system. It is 1 GB - 1 MB (the last megabyte of memory is reserved for device registers.). This value is stored in the HIGH_MEMORY constant. First 4 MB of memory are reserved for the kernel image and init task stack. This value is stored in LOW_MEMORY constant. All memory allocations start right after this point. Note: even with QEMU our kernel must start from 0x80000 (512KB), the above assumptions are good as there's still plenty room in 512KB -- LOW_MEMORY for our tiny kernel.","title":"Aside: Memory allocation"},{"location":"lesson04a/rpi-os/#scheduling-algorithm","text":"Finally, we are ready to look at the scheduler algorithm. I almost precisely copied this algorithm from the first release of the Linux kernel. You can find the original version here . void _schedule(void) { int next,c; struct task_struct * p; while (1) { c = -1; next = 0; // try to pick a task for (int i = 0; i < NR_TASKS; i++){ p = task[i]; if (p && p->state == TASK_RUNNING && p->counter > c) { c = p->counter; next = i; } } if (c) { break; } // update counters for (int i = 0; i < NR_TASKS; i++) { p = task[i]; if (p) { p->counter = (p->counter >> 1) + p->priority; } } } switch_to(task[next]); } The simple algorithm works like the following: The first for loop iterates over all tasks and tries to find a task in TASK_RUNNING state with the maximum counter. If such a task is found, we immediately break from the while loop and switch to this task. If no such task is found, this is either because i) no task is in TASK_RUNNING state or ii) all such tasks have 0 counters. In a real OS, i) might happen, for example, when all tasks are waiting for an interrupt. In our current tiny kernel, all tasks are always in TASK_RUNNING (Why?) The scheduler moves to the 2nd for loop to \"recharge\" counters. It bumps counters for all tasks once. The increment depends on a task's priority. Note: a task counter can never get larger than 2 * priority . With updated counters, the scheduler goes back to the 1st for loop to pick a task. We will augment the scheduling algorithm for preemptive multitasking later.","title":"Scheduling algorithm"},{"location":"lesson04a/rpi-os/#conclusion","text":"We have seen important nuts & bolts of multitasking. The subsequent experiment will enable task preemption. We will show a detailed workflow of context switch there.","title":"Conclusion"},{"location":"lesson04a/linux/basic_structures/","text":"4.2: Scheduler basic structures In all previous lessons we have been working mostly with either architecture specific code or driver code, and now it is the first time we will dig deep into the core of the Linux kernel. This task isn't simple, and it requires some preparations: before you will be able to understand the Linux scheduler source code, you need to become familiar with a few major concepts that the scheduler is based on. task_struct This is one of the most critical structures in the whole kernel \u2014 it contains all information about a running task. We already briefly touched task_struct in lesson 2 and we even have implemented our own task_struct for the RPi OS, so I assume that by this time you should already have a basic understanding how it is used. Now I want to highlight a few important fields of this struct that are relevant to our discussion. thread_info This is the first field of the task_struct and it contains all fields that must be accessed by the low-level architecture code. We have already seen how this happens in lesson 2 and will encounter a few other examples later. thread_info is architecture specific. In arm64 case, it is a simple structure with a few fields. struct thread_info { unsigned long flags; /* low level flags */ mm_segment_t addr_limit; /* address limit */ #ifdef CONFIG_ARM64_SW_TTBR0_PAN u64 ttbr0; /* saved TTBR0_EL1 */ #endif int preempt_count; /* 0 => preemptable, <0 => bug */ }; flags field is used very frequently \u2014 it contains information about the current task state (whether it is under a trace, whether a signal is pending, etc.). All possible flags values can be found here . state Task current state (whether it is currently running, waiting for an interrupt, exited etc.). All possible task states are described here . stack When working on the RPi OS, we have seen that task_struct is always kept at the bottom of the task stack, so we can use a pointer to task_struct as a pointer to the stack. Kernel stacks have constant size, so finding stack end is also an easy task. I think that the same approach was used in the early versions of the Linux kernel, but right now, after the introduction of the vitually mapped stacks , stack field is used to store a pointer to the kernel stack. thread Another important architecture specific structure is thread_struct . It contains all information (such as cpu_context ) that is used during a context switch. In fact, the RPi OS implements its own cpu_context that is used exactly in the same way as the original one. sched_class and sched_entity Those fields are used in schedule algorithm - more on them follows. Scheduler class In Linux, there is an extendable mechanism that allows each task to use its own scheduling algorithm. This mechanism uses a structure sched_class . You can think about this structure as an interface that defines all methods that a schedules class have to implement. Let's see what kind of methods are defined in the sched_class interface. (Not all of the methods are shown, but only those that I consider the most important for us) enqueue_task is executed each time a new task is added to a scheduler class. dequeue_task is called when a task can be removed from the scheduler. pick_next_task Core scheduler code calls this method when it needs to decide which task to run next. task_tick is called on each timer tick and gives the scheduler class an opportunity to measure how long the current task is being executed and notify core scheduler code if it needs to be preempted. There are a few sched_class implementations. The most commonly used one, which is typically used for all user tasks, is called \"Completely Fair Scheduler (CFS)\" Completely Fair Scheduler (CFS) The principals behind CFS algorithm are very simple: 1. For each task in the system, CFS measures how much CPU time has been already allocated to it (The value is stored in sum_exec_runtime field of the sched_entity structure) 1. sum_exec_runtime is adjusted accordingly to task priority and saved in vruntime field. 1. When CFS need to pick a new task to execute, the one with the smallest vruntime is selected. Linux scheduler uses another important data structure that is called \"runqueue\" and is described by the rq struct. There is a single instance of a runqueue per CPU. When a new task needs to be selected for execution, the selection is made only from the local runqueue. But if there is a need, tasks can be balanced between different rq structures. Runqueues are used by all scheduler classes, not only by CFS. All CFS specific information is kept in cfs_rq struct, which is embedded in the rq struct. One important field of the cfs_rq struct is called min_vruntime \u2014 this is the lowest vruntime from all tasks, assigned to a runqueue. min_vruntime is assigned to a newly forked task \u2014 this ensures that the task will be selected next, because CFS always ensures that a task with the smallest vruntime is picked. This approach also ensures that the new task will not be running for an unreasonably long time before it will be preempted. All tasks, assigned to a particular runqueue and tracked by CFS are kept in tasks_timeline field of the cfs_rq struct. tasks_timeline represents a Red\u2013black tree , which can be used to pick tasks ordered by their vruntime value. Red-black trees have an important property: all operations on it (search, insert, delete) can be done in O(log n) time. This means that even if we have thousands of concurrent tasks in the system all scheduler methods still executes very quickly. Another important property of a red-black tree is that for any node in the tree its right child will always have larger vruntime value than the parent, and left child's vruntime will be always less or equal then the parent's vruntime . This has an important implication: the leftmost node is always the one with the smallest vruntime . struct pt_regs I have already shown you how all general purpose registers are saved on the stack when an interrupt is taken \u2014 we explored how this process works in Linux kernel and implemented a similar one for the RPi OS. What I haven't mentioned yet, is that it is entirely legal to manipulate those registers while processing an interrupt. It is also legal to manually prepare a set of registers and put them on the stack \u2014 this is done when a kernel thread is moved to a user mode, and we are going to implement the same functionality in the next lesson. For now, you just need to remember that pt_regs structure is used to describe saved registers and it must have its fields ordered in the same order in with register are saved in the kernel_entry macro. In this lesson, we are going to see a few examples of how this structure is used. Conclusion There are far more important structures, algorithms and concepts related to scheduling, but what we've just explored is a minimal set, required to understand the next two chapters. Now it's time to see how all of this actually works. Previous Page 4.1 Process scheduler: RPi OS Scheduler Next Page 4.3 Process scheduler: Forking a task","title":"Basic structures"},{"location":"lesson04a/linux/basic_structures/#42-scheduler-basic-structures","text":"In all previous lessons we have been working mostly with either architecture specific code or driver code, and now it is the first time we will dig deep into the core of the Linux kernel. This task isn't simple, and it requires some preparations: before you will be able to understand the Linux scheduler source code, you need to become familiar with a few major concepts that the scheduler is based on.","title":"4.2: Scheduler basic structures"},{"location":"lesson04a/linux/basic_structures/#task_struct","text":"This is one of the most critical structures in the whole kernel \u2014 it contains all information about a running task. We already briefly touched task_struct in lesson 2 and we even have implemented our own task_struct for the RPi OS, so I assume that by this time you should already have a basic understanding how it is used. Now I want to highlight a few important fields of this struct that are relevant to our discussion. thread_info This is the first field of the task_struct and it contains all fields that must be accessed by the low-level architecture code. We have already seen how this happens in lesson 2 and will encounter a few other examples later. thread_info is architecture specific. In arm64 case, it is a simple structure with a few fields. struct thread_info { unsigned long flags; /* low level flags */ mm_segment_t addr_limit; /* address limit */ #ifdef CONFIG_ARM64_SW_TTBR0_PAN u64 ttbr0; /* saved TTBR0_EL1 */ #endif int preempt_count; /* 0 => preemptable, <0 => bug */ }; flags field is used very frequently \u2014 it contains information about the current task state (whether it is under a trace, whether a signal is pending, etc.). All possible flags values can be found here . state Task current state (whether it is currently running, waiting for an interrupt, exited etc.). All possible task states are described here . stack When working on the RPi OS, we have seen that task_struct is always kept at the bottom of the task stack, so we can use a pointer to task_struct as a pointer to the stack. Kernel stacks have constant size, so finding stack end is also an easy task. I think that the same approach was used in the early versions of the Linux kernel, but right now, after the introduction of the vitually mapped stacks , stack field is used to store a pointer to the kernel stack. thread Another important architecture specific structure is thread_struct . It contains all information (such as cpu_context ) that is used during a context switch. In fact, the RPi OS implements its own cpu_context that is used exactly in the same way as the original one. sched_class and sched_entity Those fields are used in schedule algorithm - more on them follows.","title":"task_struct"},{"location":"lesson04a/linux/basic_structures/#scheduler-class","text":"In Linux, there is an extendable mechanism that allows each task to use its own scheduling algorithm. This mechanism uses a structure sched_class . You can think about this structure as an interface that defines all methods that a schedules class have to implement. Let's see what kind of methods are defined in the sched_class interface. (Not all of the methods are shown, but only those that I consider the most important for us) enqueue_task is executed each time a new task is added to a scheduler class. dequeue_task is called when a task can be removed from the scheduler. pick_next_task Core scheduler code calls this method when it needs to decide which task to run next. task_tick is called on each timer tick and gives the scheduler class an opportunity to measure how long the current task is being executed and notify core scheduler code if it needs to be preempted. There are a few sched_class implementations. The most commonly used one, which is typically used for all user tasks, is called \"Completely Fair Scheduler (CFS)\"","title":"Scheduler class"},{"location":"lesson04a/linux/basic_structures/#completely-fair-scheduler-cfs","text":"The principals behind CFS algorithm are very simple: 1. For each task in the system, CFS measures how much CPU time has been already allocated to it (The value is stored in sum_exec_runtime field of the sched_entity structure) 1. sum_exec_runtime is adjusted accordingly to task priority and saved in vruntime field. 1. When CFS need to pick a new task to execute, the one with the smallest vruntime is selected. Linux scheduler uses another important data structure that is called \"runqueue\" and is described by the rq struct. There is a single instance of a runqueue per CPU. When a new task needs to be selected for execution, the selection is made only from the local runqueue. But if there is a need, tasks can be balanced between different rq structures. Runqueues are used by all scheduler classes, not only by CFS. All CFS specific information is kept in cfs_rq struct, which is embedded in the rq struct. One important field of the cfs_rq struct is called min_vruntime \u2014 this is the lowest vruntime from all tasks, assigned to a runqueue. min_vruntime is assigned to a newly forked task \u2014 this ensures that the task will be selected next, because CFS always ensures that a task with the smallest vruntime is picked. This approach also ensures that the new task will not be running for an unreasonably long time before it will be preempted. All tasks, assigned to a particular runqueue and tracked by CFS are kept in tasks_timeline field of the cfs_rq struct. tasks_timeline represents a Red\u2013black tree , which can be used to pick tasks ordered by their vruntime value. Red-black trees have an important property: all operations on it (search, insert, delete) can be done in O(log n) time. This means that even if we have thousands of concurrent tasks in the system all scheduler methods still executes very quickly. Another important property of a red-black tree is that for any node in the tree its right child will always have larger vruntime value than the parent, and left child's vruntime will be always less or equal then the parent's vruntime . This has an important implication: the leftmost node is always the one with the smallest vruntime .","title":"Completely Fair Scheduler (CFS)"},{"location":"lesson04a/linux/basic_structures/#struct-pt_regs","text":"I have already shown you how all general purpose registers are saved on the stack when an interrupt is taken \u2014 we explored how this process works in Linux kernel and implemented a similar one for the RPi OS. What I haven't mentioned yet, is that it is entirely legal to manipulate those registers while processing an interrupt. It is also legal to manually prepare a set of registers and put them on the stack \u2014 this is done when a kernel thread is moved to a user mode, and we are going to implement the same functionality in the next lesson. For now, you just need to remember that pt_regs structure is used to describe saved registers and it must have its fields ordered in the same order in with register are saved in the kernel_entry macro. In this lesson, we are going to see a few examples of how this structure is used.","title":"struct pt_regs"},{"location":"lesson04a/linux/basic_structures/#conclusion","text":"There are far more important structures, algorithms and concepts related to scheduling, but what we've just explored is a minimal set, required to understand the next two chapters. Now it's time to see how all of this actually works.","title":"Conclusion"},{"location":"lesson04a/linux/basic_structures/#previous-page","text":"4.1 Process scheduler: RPi OS Scheduler","title":"Previous Page"},{"location":"lesson04a/linux/basic_structures/#next-page","text":"4.3 Process scheduler: Forking a task","title":"Next Page"},{"location":"lesson04a/linux/fork/","text":"4.3: Forking a task Scheduling is all about selecting a proper task to run from the list of available tasks. But before the scheduler will be able to do its job we need to somehow fill this list. The way in which new tasks can be created is the main topic of this chapter. For now, we want to focus only on kernel threads and postpone the discussion of user-mode functionality till the next lesson. However, not everywhere it will be possible, so be prepared to learn a little bit about executing tasks in user mode as well. Init task When the kernel is started there is a single task running: init task. The corresponding task_struct is defined here and is initialized by INIT_TASK macro. This task is critical for the system because all other tasks in the system are derived from it. Creating new tasks In Linux it is not possible to create a new task from scratch - instead, all tasks are forked from a currently running task. Now, as we've seen from were the initial task came from, we can try to explore how new tasks can be created from it. There are 4 ways in which a new task can be created. fork system call creates a full copy of the current process, including its virtual memory and it is used to create new processes (not threads). This syscall is defined here . vfork system call is similar to fork but it differs in that the child reuses parent virtual memory as well as stack, and the parent is blocked until the child finished execution. The definition of this syscall can be found here . clone system call is the most flexible one - it also copies the current task but it allows to customize the process using flags parameter and allows to configure the entry point for the child task. In the next lesson, we will see how glibc clone wrapper function is implemented - this wrapper allows to use clone syscall to create new threads. Finally, kernel_thread function can be used to create new kernel threads. All of the above functions calls _do_fork , which accept the following arguments. clone_flags Flags are used to configure fork behavior. The complete list of the flags can be found here . stack_start In case of clone syscall this parameter indicates the location of the user stack for the new task. If 'kernel_thread' calls _do_fork this parameter points to the function that needs to be executed in a kernel thread. stack_size In arm64 architecture this parameter is only used in the case when _do_fork is called by `kernel_thread. It is a pointer to the argument that needs to be passed to the kernel thread function. (And yes, I also find the naming of the last two parameters misleading) parent_tidptr child_tidptr Those 2 parameters are used only in clone syscall. Fork will store the child thread ID at the location parent_tidptr in the parent's memory, or it can store parent's ID at child_tidptr location. tls Thread Local Storage Fork procedure Next, I want to highlight the most important events that take place during _do_fork execution, preserving their order. _do_fork calls copy_process copy_process is responsible for configuring new task_struct . copy_process calls dup_task_struct , which allocates new task_struct and copies all fields from the original one. Actual copying takes place in the architecture-specific arch_dup_task_struct New kernel stack is allocated. If CONFIG_VMAP_STACK is enabled the kernel uses virtually mapped stacks to protect against kernel stack overflow. link Task's credentials are copied. link The scheduler is notified that a new task is forked. link task_fork_fair method of the CFS scheduler class is called. This method updates vruntime value for the currently running task (this is done inside update_curr function) and updates min_vruntime value for the current runqueue (inside update_min_vruntime ). Then min_vruntime value is assigned to the forked task - this ensures that this task will be picked up next. Note, that at this point of time new task still hasn't been added to the task_timeline . A lot of different properties, such as information about filesystems, open files, virtual memory, signals, namespaces, are either reused or copied from the current task. The decision whether to copy something or reuse current property is usually made based on the clone_flags parameter. link copy_thread_tls is called which in turn calls architecture specific copy_thread function. This function deserves a special attention because it works as a prototype for the copy_process function in the RPi OS, and I want to investigate it deeper. copy_thread The whole function is listed below. int copy_thread(unsigned long clone_flags, unsigned long stack_start, unsigned long stk_sz, struct task_struct *p) { struct pt_regs *childregs = task_pt_regs(p); memset(&p->thread.cpu_context, 0, sizeof(struct cpu_context)); if (likely(!(p->flags & PF_KTHREAD))) { *childregs = *current_pt_regs(); childregs->regs[0] = 0; /* * Read the current TLS pointer from tpidr_el0 as it may be * out-of-sync with the saved value. */ *task_user_tls(p) = read_sysreg(tpidr_el0); if (stack_start) { if (is_compat_thread(task_thread_info(p))) childregs->compat_sp = stack_start; else childregs->sp = stack_start; } /* * If a TLS pointer was passed to clone (4th argument), use it * for the new thread. */ if (clone_flags & CLONE_SETTLS) p->thread.tp_value = childregs->regs[3]; } else { memset(childregs, 0, sizeof(struct pt_regs)); childregs->pstate = PSR_MODE_EL1h; if (IS_ENABLED(CONFIG_ARM64_UAO) && cpus_have_const_cap(ARM64_HAS_UAO)) childregs->pstate |= PSR_UAO_BIT; p->thread.cpu_context.x19 = stack_start; p->thread.cpu_context.x20 = stk_sz; } p->thread.cpu_context.pc = (unsigned long)ret_from_fork; p->thread.cpu_context.sp = (unsigned long)childregs; ptrace_hw_copy_thread(p); return 0; } Some of this code can be already a little bit familiar to you. Let's dig dipper into it. struct pt_regs *childregs = task_pt_regs(p); The function starts with allocating new pt_regs struct. This struct is used to provide access to the registers, saved during kernel_entry . childregs variable then can be used to prepare whatever state we need for the newly created task. If the task then decides to move to user mode the state will be restored by the kernel_exit macro. An important thing to understand here is that task_pt_regs macro doesn't allocate anything - it just calculate the position on the kernel stack, were kernel_entry stores registers, and for the newly created task, this position will always be at the top of the kernel stack. memset(&p->thread.cpu_context, 0, sizeof(struct cpu_context)); Next, forked task cpu_context is cleared. if (likely(!(p->flags & PF_KTHREAD))) { Then a check is made to determine whether we are creating a kernel or a user thread. For now, we are interested only in kernel thread case and we will discuss the second option in the next lesson. memset(childregs, 0, sizeof(struct pt_regs)); childregs->pstate = PSR_MODE_EL1h; if (IS_ENABLED(CONFIG_ARM64_UAO) && cpus_have_const_cap(ARM64_HAS_UAO)) childregs->pstate |= PSR_UAO_BIT; p->thread.cpu_context.x19 = stack_start; p->thread.cpu_context.x20 = stk_sz; If we are creating a kernel thread x19 and x20 registers of the cpu_context are set to point to the function that needs to be executed ( stack_start ) and its argument ( stk_sz ). After CPU will be switched to the forked task, ret_from_fork will use those registers to jump to the needed function. (I don't quite understand why do we also need to set childregs->pstate here. ret_from_fork will not call kernel_exit before jumping to the function stored in x19 , and even if the kernel thread decides to move to the user mode childregs will be overwritten anyway. Any ideas?) p->thread.cpu_context.pc = (unsigned long)ret_from_fork; p->thread.cpu_context.sp = (unsigned long)childregs; Next cpu_context.pc is set to ret_from_fork pointer - this ensures that we return to the ret_from_fork after the first context switch. cpu_context.sp is set to the location just below the childregs . We still need childregs at the top of the stack because after the kernel thread finishes its execution the task will be moved to user mode and childregs structure will be used. In the next lesson, we will discuss in details how this happens. That's it about copy_thread function. Now let's return to the place in the fork procedure from where we left. Fork procedure (continued) After copy_process succsesfully prepares task_struct for the forked task _do_fork can now run it by calling wake_up_new_task . This is done here . Then task state is changed to TASK_RUNNING and enqueue_task_fair CFS method is called, wich triggers execution of the __enqueue_entity that actually adds task to the task_timeline red-black tree. At this line, check_preempt_curr is called, which in turn calls check_preempt_wakeup CFS method. This method is responsible for checking whether the current task should be preempted by some other task. That is exactly what is going to happen because we have just put a new task on the timeline that has minimal possible vruntime . So resched_curr function is triggered, which sets TIF_NEED_RESCHED flag for the current task. TIF_NEED_RESCHED is checked just before the current task exit from an exception handler ( fork , vfork and clone are all system call, and each system call is a special type of exception.). The check is made here . Note that _TIF_WORK_MASK includes _TIF_NEED_RESCHED . It is also important to understand that in case of a kernel thread creation, the new thread will not be started until the next timer tick or until the parent task volatirely calls schedule() . If the current task needs to be rescheduled, do_notify_resume is triggered, which in turn calls schedule . Finally we reached the point where task scheduling is triggered, and we are going to stop at this point. Conclusion Now that you understand how new tasks are created and added to the scheduler, it is time to take a look on how the scheduler itself works and how context switch is implemented. That is something we are going to explore in the next chapter. Previous Page 4.2 Process scheduler: Scheduler basic structures Next Page 4.4 Process scheduler: Scheduler","title":"Fork"},{"location":"lesson04a/linux/fork/#43-forking-a-task","text":"Scheduling is all about selecting a proper task to run from the list of available tasks. But before the scheduler will be able to do its job we need to somehow fill this list. The way in which new tasks can be created is the main topic of this chapter. For now, we want to focus only on kernel threads and postpone the discussion of user-mode functionality till the next lesson. However, not everywhere it will be possible, so be prepared to learn a little bit about executing tasks in user mode as well.","title":"4.3: Forking a task"},{"location":"lesson04a/linux/fork/#init-task","text":"When the kernel is started there is a single task running: init task. The corresponding task_struct is defined here and is initialized by INIT_TASK macro. This task is critical for the system because all other tasks in the system are derived from it.","title":"Init task"},{"location":"lesson04a/linux/fork/#creating-new-tasks","text":"In Linux it is not possible to create a new task from scratch - instead, all tasks are forked from a currently running task. Now, as we've seen from were the initial task came from, we can try to explore how new tasks can be created from it. There are 4 ways in which a new task can be created. fork system call creates a full copy of the current process, including its virtual memory and it is used to create new processes (not threads). This syscall is defined here . vfork system call is similar to fork but it differs in that the child reuses parent virtual memory as well as stack, and the parent is blocked until the child finished execution. The definition of this syscall can be found here . clone system call is the most flexible one - it also copies the current task but it allows to customize the process using flags parameter and allows to configure the entry point for the child task. In the next lesson, we will see how glibc clone wrapper function is implemented - this wrapper allows to use clone syscall to create new threads. Finally, kernel_thread function can be used to create new kernel threads. All of the above functions calls _do_fork , which accept the following arguments. clone_flags Flags are used to configure fork behavior. The complete list of the flags can be found here . stack_start In case of clone syscall this parameter indicates the location of the user stack for the new task. If 'kernel_thread' calls _do_fork this parameter points to the function that needs to be executed in a kernel thread. stack_size In arm64 architecture this parameter is only used in the case when _do_fork is called by `kernel_thread. It is a pointer to the argument that needs to be passed to the kernel thread function. (And yes, I also find the naming of the last two parameters misleading) parent_tidptr child_tidptr Those 2 parameters are used only in clone syscall. Fork will store the child thread ID at the location parent_tidptr in the parent's memory, or it can store parent's ID at child_tidptr location. tls Thread Local Storage","title":"Creating new tasks"},{"location":"lesson04a/linux/fork/#fork-procedure","text":"Next, I want to highlight the most important events that take place during _do_fork execution, preserving their order. _do_fork calls copy_process copy_process is responsible for configuring new task_struct . copy_process calls dup_task_struct , which allocates new task_struct and copies all fields from the original one. Actual copying takes place in the architecture-specific arch_dup_task_struct New kernel stack is allocated. If CONFIG_VMAP_STACK is enabled the kernel uses virtually mapped stacks to protect against kernel stack overflow. link Task's credentials are copied. link The scheduler is notified that a new task is forked. link task_fork_fair method of the CFS scheduler class is called. This method updates vruntime value for the currently running task (this is done inside update_curr function) and updates min_vruntime value for the current runqueue (inside update_min_vruntime ). Then min_vruntime value is assigned to the forked task - this ensures that this task will be picked up next. Note, that at this point of time new task still hasn't been added to the task_timeline . A lot of different properties, such as information about filesystems, open files, virtual memory, signals, namespaces, are either reused or copied from the current task. The decision whether to copy something or reuse current property is usually made based on the clone_flags parameter. link copy_thread_tls is called which in turn calls architecture specific copy_thread function. This function deserves a special attention because it works as a prototype for the copy_process function in the RPi OS, and I want to investigate it deeper.","title":"Fork procedure"},{"location":"lesson04a/linux/fork/#copy_thread","text":"The whole function is listed below. int copy_thread(unsigned long clone_flags, unsigned long stack_start, unsigned long stk_sz, struct task_struct *p) { struct pt_regs *childregs = task_pt_regs(p); memset(&p->thread.cpu_context, 0, sizeof(struct cpu_context)); if (likely(!(p->flags & PF_KTHREAD))) { *childregs = *current_pt_regs(); childregs->regs[0] = 0; /* * Read the current TLS pointer from tpidr_el0 as it may be * out-of-sync with the saved value. */ *task_user_tls(p) = read_sysreg(tpidr_el0); if (stack_start) { if (is_compat_thread(task_thread_info(p))) childregs->compat_sp = stack_start; else childregs->sp = stack_start; } /* * If a TLS pointer was passed to clone (4th argument), use it * for the new thread. */ if (clone_flags & CLONE_SETTLS) p->thread.tp_value = childregs->regs[3]; } else { memset(childregs, 0, sizeof(struct pt_regs)); childregs->pstate = PSR_MODE_EL1h; if (IS_ENABLED(CONFIG_ARM64_UAO) && cpus_have_const_cap(ARM64_HAS_UAO)) childregs->pstate |= PSR_UAO_BIT; p->thread.cpu_context.x19 = stack_start; p->thread.cpu_context.x20 = stk_sz; } p->thread.cpu_context.pc = (unsigned long)ret_from_fork; p->thread.cpu_context.sp = (unsigned long)childregs; ptrace_hw_copy_thread(p); return 0; } Some of this code can be already a little bit familiar to you. Let's dig dipper into it. struct pt_regs *childregs = task_pt_regs(p); The function starts with allocating new pt_regs struct. This struct is used to provide access to the registers, saved during kernel_entry . childregs variable then can be used to prepare whatever state we need for the newly created task. If the task then decides to move to user mode the state will be restored by the kernel_exit macro. An important thing to understand here is that task_pt_regs macro doesn't allocate anything - it just calculate the position on the kernel stack, were kernel_entry stores registers, and for the newly created task, this position will always be at the top of the kernel stack. memset(&p->thread.cpu_context, 0, sizeof(struct cpu_context)); Next, forked task cpu_context is cleared. if (likely(!(p->flags & PF_KTHREAD))) { Then a check is made to determine whether we are creating a kernel or a user thread. For now, we are interested only in kernel thread case and we will discuss the second option in the next lesson. memset(childregs, 0, sizeof(struct pt_regs)); childregs->pstate = PSR_MODE_EL1h; if (IS_ENABLED(CONFIG_ARM64_UAO) && cpus_have_const_cap(ARM64_HAS_UAO)) childregs->pstate |= PSR_UAO_BIT; p->thread.cpu_context.x19 = stack_start; p->thread.cpu_context.x20 = stk_sz; If we are creating a kernel thread x19 and x20 registers of the cpu_context are set to point to the function that needs to be executed ( stack_start ) and its argument ( stk_sz ). After CPU will be switched to the forked task, ret_from_fork will use those registers to jump to the needed function. (I don't quite understand why do we also need to set childregs->pstate here. ret_from_fork will not call kernel_exit before jumping to the function stored in x19 , and even if the kernel thread decides to move to the user mode childregs will be overwritten anyway. Any ideas?) p->thread.cpu_context.pc = (unsigned long)ret_from_fork; p->thread.cpu_context.sp = (unsigned long)childregs; Next cpu_context.pc is set to ret_from_fork pointer - this ensures that we return to the ret_from_fork after the first context switch. cpu_context.sp is set to the location just below the childregs . We still need childregs at the top of the stack because after the kernel thread finishes its execution the task will be moved to user mode and childregs structure will be used. In the next lesson, we will discuss in details how this happens. That's it about copy_thread function. Now let's return to the place in the fork procedure from where we left.","title":"copy_thread"},{"location":"lesson04a/linux/fork/#fork-procedure-continued","text":"After copy_process succsesfully prepares task_struct for the forked task _do_fork can now run it by calling wake_up_new_task . This is done here . Then task state is changed to TASK_RUNNING and enqueue_task_fair CFS method is called, wich triggers execution of the __enqueue_entity that actually adds task to the task_timeline red-black tree. At this line, check_preempt_curr is called, which in turn calls check_preempt_wakeup CFS method. This method is responsible for checking whether the current task should be preempted by some other task. That is exactly what is going to happen because we have just put a new task on the timeline that has minimal possible vruntime . So resched_curr function is triggered, which sets TIF_NEED_RESCHED flag for the current task. TIF_NEED_RESCHED is checked just before the current task exit from an exception handler ( fork , vfork and clone are all system call, and each system call is a special type of exception.). The check is made here . Note that _TIF_WORK_MASK includes _TIF_NEED_RESCHED . It is also important to understand that in case of a kernel thread creation, the new thread will not be started until the next timer tick or until the parent task volatirely calls schedule() . If the current task needs to be rescheduled, do_notify_resume is triggered, which in turn calls schedule . Finally we reached the point where task scheduling is triggered, and we are going to stop at this point.","title":"Fork procedure (continued)"},{"location":"lesson04a/linux/fork/#conclusion","text":"Now that you understand how new tasks are created and added to the scheduler, it is time to take a look on how the scheduler itself works and how context switch is implemented. That is something we are going to explore in the next chapter.","title":"Conclusion"},{"location":"lesson04a/linux/fork/#previous-page","text":"4.2 Process scheduler: Scheduler basic structures","title":"Previous Page"},{"location":"lesson04a/linux/fork/#next-page","text":"4.4 Process scheduler: Scheduler","title":"Next Page"},{"location":"lesson04a/linux/scheduler/","text":"4.4: Scheduler We have already learned a lot of details about the Linux scheduler inner workings, so there is not so much left for us. To make the whole picture complete in this chapter we will take a look at 2 important scheduler entry points: scheduler_tick() function, which is called at each timer interrupt. schedule() function, which is called each time when the current task needs to be rescheduled. The third major thing that we are going to investigate in this chapter is the concept of context switch. A context switch is the process that suspends the current task and runs another task instead - this process is highly architecture specific and closely correlates with what we have been doing when working with RPi OS. scheduler_tick This function is important for 2 reasons: It provides a way for the scheduler to update time statistics and runtime information for the current task. Runtime information then is used to determine whether the current task needs to be preempted, and if so schedule() function is called. As well as most of the previously explored functions, scheduler_tick is too complex to be fully explained - instead, as usual, I will just highlight the most important parts. The main work is done inside CFS method task_tick_fair . This method calls entity_tick for the sched_entity corresponding to the current task. When looking at the source code, you may be wondering why instead of just calling entry_tick for the current sched_entry , for_each_sched_entity macro is used instead? for_each_sched_entity doesn't iterate over all sched_entry in the system. Instead, it only traverses the sched_entry inheritance tree up to the root. This is useful when tasks are grouped - after updating runtime information for a particular task, sched_entry corresponding to the whole group is also updated. entity_tick does 2 main things: Calls update_curr , which is responsible for updating task's vruntime as well as runqueue's min_vruntime . An important thing to remember here is that vruntime is always based on 2 things: how long task has actually been executed and tasks priority. Calls check_preempt_tick , which checks whether the current task needs to be preempted. Preemption happens in 2 cases: If the current task has been running for too long (the comparison is made using normal time, not vruntime ). link If there is a task with smaller vruntime and the difference between vruntime values is greater than some threshold. link In both cases the current task is marked for preemption by calling resched_curr function. We have already seen in the previous chapter how calling resched_curr leads to TIF_NEED_RESCHED flag being set for the current task and eventually schedule being called. That's it about schedule_tick now we are finally ready to take a look at the schedule function. schedule We have already seen so many examples were schedule is used, so now you are probably anxious to see how this function actually works. You will be surprised to know that the internals of this function are rather simple. The main work is done inside __schedule function. __schedule calls pick_next_task which redirect most of the work to the pick_next_task_fair method of the CFS scheduler. As you might expect pick_next_task_fair in a normal case just selects the leftmost element from the red-black tree and returns it. It happens here . __schedule calls context_switch , which does some preparation work and calls architecture specific __switch_to function, where low-level arch specific task parameters are prepared to the switch. __switch_to first switches some additional task components, like, for example, TLS (Thread-local Store) and saved floating point and NEON registers. Actual switch takes place in the assembler cpu_switch_to function. This function should be already familiar to you because I copied it almost without any changes to the RPi OS. As you might remember, this function switches callee-saved registers and task stack. After it returns, the new task will be running using its own kernel stack. Conclusion Now we are done with the Linux scheduler. The good thing is that it appears to be not so difficult if you focus only on the very basic workflow. After you understand the basic workflow you probably might want to to make another path through the schedule code and pay more attention to the details, because there are so many of them. But for now, we are happy with our current understanding and ready to move to the following lesson, which describes user processes and system calls. Previous Page 4.3 Process scheduler: Forking a task Next Page 4.5 Process scheduler: Exercises","title":"Scheduler"},{"location":"lesson04a/linux/scheduler/#44-scheduler","text":"We have already learned a lot of details about the Linux scheduler inner workings, so there is not so much left for us. To make the whole picture complete in this chapter we will take a look at 2 important scheduler entry points: scheduler_tick() function, which is called at each timer interrupt. schedule() function, which is called each time when the current task needs to be rescheduled. The third major thing that we are going to investigate in this chapter is the concept of context switch. A context switch is the process that suspends the current task and runs another task instead - this process is highly architecture specific and closely correlates with what we have been doing when working with RPi OS.","title":"4.4: Scheduler"},{"location":"lesson04a/linux/scheduler/#scheduler_tick","text":"This function is important for 2 reasons: It provides a way for the scheduler to update time statistics and runtime information for the current task. Runtime information then is used to determine whether the current task needs to be preempted, and if so schedule() function is called. As well as most of the previously explored functions, scheduler_tick is too complex to be fully explained - instead, as usual, I will just highlight the most important parts. The main work is done inside CFS method task_tick_fair . This method calls entity_tick for the sched_entity corresponding to the current task. When looking at the source code, you may be wondering why instead of just calling entry_tick for the current sched_entry , for_each_sched_entity macro is used instead? for_each_sched_entity doesn't iterate over all sched_entry in the system. Instead, it only traverses the sched_entry inheritance tree up to the root. This is useful when tasks are grouped - after updating runtime information for a particular task, sched_entry corresponding to the whole group is also updated. entity_tick does 2 main things: Calls update_curr , which is responsible for updating task's vruntime as well as runqueue's min_vruntime . An important thing to remember here is that vruntime is always based on 2 things: how long task has actually been executed and tasks priority. Calls check_preempt_tick , which checks whether the current task needs to be preempted. Preemption happens in 2 cases: If the current task has been running for too long (the comparison is made using normal time, not vruntime ). link If there is a task with smaller vruntime and the difference between vruntime values is greater than some threshold. link In both cases the current task is marked for preemption by calling resched_curr function. We have already seen in the previous chapter how calling resched_curr leads to TIF_NEED_RESCHED flag being set for the current task and eventually schedule being called. That's it about schedule_tick now we are finally ready to take a look at the schedule function.","title":"scheduler_tick"},{"location":"lesson04a/linux/scheduler/#schedule","text":"We have already seen so many examples were schedule is used, so now you are probably anxious to see how this function actually works. You will be surprised to know that the internals of this function are rather simple. The main work is done inside __schedule function. __schedule calls pick_next_task which redirect most of the work to the pick_next_task_fair method of the CFS scheduler. As you might expect pick_next_task_fair in a normal case just selects the leftmost element from the red-black tree and returns it. It happens here . __schedule calls context_switch , which does some preparation work and calls architecture specific __switch_to function, where low-level arch specific task parameters are prepared to the switch. __switch_to first switches some additional task components, like, for example, TLS (Thread-local Store) and saved floating point and NEON registers. Actual switch takes place in the assembler cpu_switch_to function. This function should be already familiar to you because I copied it almost without any changes to the RPi OS. As you might remember, this function switches callee-saved registers and task stack. After it returns, the new task will be running using its own kernel stack.","title":"schedule"},{"location":"lesson04a/linux/scheduler/#conclusion","text":"Now we are done with the Linux scheduler. The good thing is that it appears to be not so difficult if you focus only on the very basic workflow. After you understand the basic workflow you probably might want to to make another path through the schedule code and pay more attention to the details, because there are so many of them. But for now, we are happy with our current understanding and ready to move to the following lesson, which describes user processes and system calls.","title":"Conclusion"},{"location":"lesson04a/linux/scheduler/#previous-page","text":"4.3 Process scheduler: Forking a task","title":"Previous Page"},{"location":"lesson04a/linux/scheduler/#next-page","text":"4.5 Process scheduler: Exercises","title":"Next Page"},{"location":"lesson04b/exercises/","text":"Exercises Increase the rate of context switch to 10 Hz Tracing context switch. Whenever a context switch happens, record the timestamp and the IDs of tasks that are switching in and out. After 200 of context switches, print out a list of switch records. Why is it a bad idea to print out the information of a context switch as it happens? Deliverable A code tarball implementing (1) above. A code tarball implementing (2) above.","title":"Exercises"},{"location":"lesson04b/exercises/#exercises","text":"Increase the rate of context switch to 10 Hz Tracing context switch. Whenever a context switch happens, record the timestamp and the IDs of tasks that are switching in and out. After 200 of context switches, print out a list of switch records. Why is it a bad idea to print out the information of a context switch as it happens?","title":"Exercises"},{"location":"lesson04b/exercises/#deliverable","text":"A code tarball implementing (1) above. A code tarball implementing (2) above.","title":"Deliverable"},{"location":"lesson04b/rpi-os/","text":"4b: Preemptive Multitasking Objectives A minimum kernel that can schedule multiple tasks in a preemptive fashion. With this experiment, our tiny kernel is more like a \"real-time kernel\" commonly seen in embedded systems, e.g. FreeRTOS. Preempt tasks with time interrupts Understand context switch driven by interrupts, in particular switch to/from interrupt handlers Atomic kernel regions where preemption is disallowed Roadmap We will turn on timer interrupts. In the interrupt handler, our kernel invokes its scheduler to switch among runnable tasks. In addition to switch_to , the kernel should save & restore CPU state upon entering/existing interrupt handling. Turn on timer interrupts! We turn on timer interrupts in kernel_main . void kernel_main(void) { uart_init(); init_printf(0, putc); irq_vector_init(); timer_init(); /* new addition */ enable_interrupt_controller(); /* new addition */ enable_irq(); /* new addition */ ... } Tasks no longer need to call schedule() voluntarily. void process(char *array) { while (1){ for (int i = 0; i < 5; i++){ uart_send(array[i]); delay(100000); } // schedule(); } } Calling schedule() in timer tick With preemptive scheduling, schedule() are called in 2 scenarios. A task can call schedule voluntarily. schedule is also called on a regular basis from the timer interrupt handler . Now let's take a look at timer_tick function, which is called from the timer interrupt. void timer_tick() { --current->counter; if (current->counter>0 || current->preempt_count >0) { return; } current->counter=0; enable_irq(); _schedule(); disable_irq(); ... First of all, it decreases current task's counter. If the counter is greater then 0 or preemption is currently disabled the function returns, but otherwise schedule is called with interrupts enabled. (Note: we just came from an interrupt handler and CPU just automatically disabled all interrupts) We will see why interrupts must be enabled during scheduler execution in the next section. How scheduling works with interrupt entry/exit? With preemptive scheduling, the kernel must save & restore CPU contexts for each task being interrupted. This is because, e.g. a task A may be interrupted at any point and get descheduled. Later, when the kernel reschedules A, A should resume from where it is interrupted. In previous baremetal experiments with only one task, we have seen how kernel_entry and kernel_exit macros save and restore general-purpose CPU regs upon switch to/from an interrupt/exception handler. (btw, where was the CPU state saved?) There, we rely on that the hardware automatically saves exception return address and CPU status in registers, elr_el1 register and spsr_el register. When eret is executed, CPU restores execution from these registers. With multitasking, the kernel now has to create per-task copies of CPU context in memory: general-purpose registers plus elr_el1 and spsr_el . Where to store the CPU context? Our implementation choice is on the current task's kernel stack (not in task_struct.cpu_context ). There are alternative designs to be examined later. Example workflow Kernel boots kernel_main function is executed. The initial stack is configured to start at LOW_MEMORY , which is at 0x0040:0000 (4 MB). Task 1 creation kernel_main calls copy_process for the first time. A new 4 KB page is allocated, and task_struct is placed at the bottom of this page. Task 2 creation kernel_main calls copy_process for the second time and the same process repeats. Task 2 is created and added to the task list. Switching to task 1; task 1 runs kernel_main voluntarily calls schedule function and it decides to switch to task 1. cpu_switch_to saves callee-saved registers in the init task cpu_context , which is located inside the kernel image. cpu_switch_to restores callee-saved registers from task 1's task_struct . At this point, cpu_context.sp points to 0x00401000 , lr points to ret_from_fork function, x19 contains a pointer to the start of process() and x20 a pointer to string \"12345\", which is located somewhere in the kernel image. cpu_switch_to executes ret , which jumps to the ret_from_fork function. ret_from_fork reads x19 and x20 registers and calls process function with the argument \"12345\". After process function starts, the stack of task 1 begins to grow. A timer interrupt occurred kernel_entry macro saves all general purpose registers + elr_el1 and spsr_el1 to the bottom of task 1 stack (\"saved regs\" in the figure below). The kernel now executes in the irq context. It continues to grow the current stack which belongs to task 1. The growth is below the \"saved regs\" region and is marked as \"irq frame\" on the figure (i.e. the stack frame created by the execution in the irq context). The kernel proceeds to schedule and picks task 2. Switching to task 2; task 2 runs cpu_switch_to executes exactly the same sequence of steps that it does for task 1. Task 2 started to execute and it stack grows. Note: until now, the kernel has NOT executed eret for the previous irq. This is fine, as an implementation choice made for this particular experiment. How can we execute task 2 in the context of the previous irq? It works because ARM64 CPU does not differentiate execution in an irq context vs. in an exception (i.e. syscall) context. All the CPU knows is the current EL (we always stay at EL1 before/after the irq) and the irq enable status. And irqs have been enabled previously in timer_tick before schedule was called. Nevertheless, there's a more common design in which the kernel returns from the irq context before switching to a new task. See \"alternative design\" below. Another timer interrupt occurred while task 2 is running kernel_entry saves all general purpose registers + elr_el1 and spsr_el1 at the bottom of task 2's stack. Task 2's irq frame begins to grow. Scheduling out task 2 The kernel calls schedule() . It observes that all tasks have their counters set to 0 and set counters to their tasks priorities. schedule selects init task to run. (This is because all tasks now have their counters set to 1 and init task is the first in the list). But actually, it would be fully legal for schedule to select task 1 or task 2 at this point, because their counters has equal values. We are more interested in the case when task 1 is selected so let's now assume that this is what had happened. Switching to task 1, exiting from the 1st irq cpu_switch_to is called and it restores previously saved callee-saved registers from task 1 cpu_context . Link register now points here because this is the place from which cpu_switch_to was called last time when task 1 was executed. sp points to the bottom of task 1 interrupt stack. timer_tick function resumes execution, starting from this line. (Can you pinpoint the instruction?) It disables interrupts and finally kernel_exit is executed. By the time kernel_exit is called, task 1 irq frame is unwound. kernel_exit further unwinds the saved regs. Task 1 resumes normal execution kernel_exit restores all general purpose registers as well as elr_el1 and spsr_el1 . elr_el1 now points somewhere in the middle of the process function. sp points to the bottom of task 1 stack. (Note: the remaining task size depends on the size of local variables in process ) Finally, kernel_exit executes eret instruction which uses elr_el1 register to jump back to process function. Task 1 resumes it normal execution! An alternative design When an interrupt happens, the CPU saves irq stack frame automatically on the stack of the current task, e.g. A. This is the same as the design above. The kernel copies the auto saved register contents from the irq frame to the current task's task_struct , representing the CPU context when this task was interrupted by irq. The kernel calls its scheduler and returns from the irq (possibly to a different task). The irq stack on the A's stack is then unwound. Now irq is on. Later, when A is scheduled in, the kernel restores its CPU context from A's task_struct . Can you implement the alternative design? Disable preemption The kernel needs mechanism to (temporarily) disable preemption. Example: in creating a new task_struct , we do not want rescheduling to happen. Otherwise the scheduler may see an incomplete task_struct . In other words, the creation of task_struct should be atomic . To disable preemption, one method is to disable interrupts. Beyond that, the kernel also needs fine-grained control. Per-task preempt_count To task_struct , we add: struct task_struct { struct cpu_context cpu_context; long state; long counter; long priority; long preempt_count; // new addition }; preempt_count >0 indicates that right now the current task is non-preemptable. The following two functions operate on it: void preempt_disable(void) { current->preempt_count++;} void preempt_enable(void) { current->preempt_count--;} Seeing this flag, the kernel will not invoke scheduler() at all, let alone descheduling this task (i.e. switching to a different task). This is done via the following code. void timer_tick() { if (current->counter>0 || current->preempt_count >0) return; ... Why a count instead of a binary flag? This again mimics the Linux implementation. Individual kernel functions could increment & decrement preempt_count . If all kernel functions have finished decrementing preempt_count , the count drops to zero and the scheduler is free to deschedule the task. This mechanism is called reference count, which is common in system software. preempt_count does not prevent a task from shooting in its own foot, though. For instance, a misbehaving task calling schedule() when preempt_count > 0 will likely corrupt kernel data structures. Try it out! Creating a task_struct atomically Going back to making copy_process atomic: int copy_process(unsigned long fn, unsigned long arg) { preempt_disable(); /* new addition */ struct task_struct *p; p = (struct task_struct *) get_free_page(); if (!p) return 1; p->priority = current->priority; p->state = TASK_RUNNING; p->counter = p->priority; p->preempt_count = 1; // new addition p->cpu_context.x19 = fn; p->cpu_context.x20 = arg; p->cpu_context.pc = (unsigned long)ret_from_fork; p->cpu_context.sp = (unsigned long)p + THREAD_SIZE; int pid = nr_tasks++; task[pid] = p; preempt_enable(); /* new addition */ return 0; } preempt_count is set to 1, preventing the new task, once it starts to execute, from being preempted until it completes some initialization work. After that, the new task executes ret_from_fork , which calls schedule_tail() which will call preempt_enable() // entry.S .globl ret_from_fork ret_from_fork: bl schedule_tail ... Making the scheduling algorithm atomic The scheduler is non-reentrant . Making it atomic looks easy: we just call preempt_disable/enable() upon entering/leaving the scheduler. void _schedule(void) { preempt_disable(); /* new addition */ int next,c; struct task_struct * p; while (1) { c = -1; next = 0; for (int i = 0; i < NR_TASKS; i++){ p = task[i]; if (p && p->state == TASK_RUNNING && p->counter > c) { c = p->counter; next = i; } } if (c) { break; } for (int i = 0; i < NR_TASKS; i++) { p = task[i]; if (p) { p->counter = (p->counter >> 1) + p->priority; } } } switch_to(task[next]); preempt_enable(); /* new addition */ } Why does the kernel disable preemption , instead of disabling all interrupts? By design, if no TASK_RUNNING tasks are there, the scheduler will run its while loop over and over again until some of the tasks will move to TASK_RUNNING state. But if we are running on a single CPU, how then a task state can change while this loop is running? The answer is that if some task is waiting for an interrupt, this interrupt can happen while schedule function is executed and interrupt handler can change the state of the task. This actually explains why interrupts must be enabled during schedule execution. This also demonstrates an important distinction between disabling interrupts and disabling preemption. schedule disables preemption for the duration of the whole function. This ensures that nested schedule will not be called while we are in the middle of the original function execution. However, interrupts can legally happen during schedule function execution. Note: our kernel does not (yet) have the mechanism for tasks to wait for interrupts. It's a important mechanism to be added. I am not very satisfied with leaving interrupt on during schedule(). There shall be an idle task which does WFI when no other tasks are runnable. In that way, the scheduler can avoid spinning and can run with interrupt off. To implement the idle task, the kernel shall implement task wait state. Conclusion We are done with scheduling, but right now our kernel can manage only kernel threads: they are executed at EL1 and can directly access any kernel functions or data. In the next 2 lessons we are going fix this and introduce system calls and virtual memory. Previous Page 3.5 Interrupt handling: Exercises Next Page 4.2 Process scheduler: Scheduler basic structures","title":"4b: Preemptive Multitasking"},{"location":"lesson04b/rpi-os/#4b-preemptive-multitasking","text":"","title":"4b: Preemptive Multitasking"},{"location":"lesson04b/rpi-os/#objectives","text":"A minimum kernel that can schedule multiple tasks in a preemptive fashion. With this experiment, our tiny kernel is more like a \"real-time kernel\" commonly seen in embedded systems, e.g. FreeRTOS. Preempt tasks with time interrupts Understand context switch driven by interrupts, in particular switch to/from interrupt handlers Atomic kernel regions where preemption is disallowed","title":"Objectives"},{"location":"lesson04b/rpi-os/#roadmap","text":"We will turn on timer interrupts. In the interrupt handler, our kernel invokes its scheduler to switch among runnable tasks. In addition to switch_to , the kernel should save & restore CPU state upon entering/existing interrupt handling.","title":"Roadmap"},{"location":"lesson04b/rpi-os/#turn-on-timer-interrupts","text":"We turn on timer interrupts in kernel_main . void kernel_main(void) { uart_init(); init_printf(0, putc); irq_vector_init(); timer_init(); /* new addition */ enable_interrupt_controller(); /* new addition */ enable_irq(); /* new addition */ ... } Tasks no longer need to call schedule() voluntarily. void process(char *array) { while (1){ for (int i = 0; i < 5; i++){ uart_send(array[i]); delay(100000); } // schedule(); } }","title":"Turn on timer interrupts!"},{"location":"lesson04b/rpi-os/#calling-schedule-in-timer-tick","text":"With preemptive scheduling, schedule() are called in 2 scenarios. A task can call schedule voluntarily. schedule is also called on a regular basis from the timer interrupt handler . Now let's take a look at timer_tick function, which is called from the timer interrupt. void timer_tick() { --current->counter; if (current->counter>0 || current->preempt_count >0) { return; } current->counter=0; enable_irq(); _schedule(); disable_irq(); ... First of all, it decreases current task's counter. If the counter is greater then 0 or preemption is currently disabled the function returns, but otherwise schedule is called with interrupts enabled. (Note: we just came from an interrupt handler and CPU just automatically disabled all interrupts) We will see why interrupts must be enabled during scheduler execution in the next section.","title":"Calling schedule() in timer tick"},{"location":"lesson04b/rpi-os/#how-scheduling-works-with-interrupt-entryexit","text":"With preemptive scheduling, the kernel must save & restore CPU contexts for each task being interrupted. This is because, e.g. a task A may be interrupted at any point and get descheduled. Later, when the kernel reschedules A, A should resume from where it is interrupted. In previous baremetal experiments with only one task, we have seen how kernel_entry and kernel_exit macros save and restore general-purpose CPU regs upon switch to/from an interrupt/exception handler. (btw, where was the CPU state saved?) There, we rely on that the hardware automatically saves exception return address and CPU status in registers, elr_el1 register and spsr_el register. When eret is executed, CPU restores execution from these registers. With multitasking, the kernel now has to create per-task copies of CPU context in memory: general-purpose registers plus elr_el1 and spsr_el . Where to store the CPU context? Our implementation choice is on the current task's kernel stack (not in task_struct.cpu_context ). There are alternative designs to be examined later.","title":"How scheduling works with interrupt entry/exit?"},{"location":"lesson04b/rpi-os/#example-workflow","text":"Kernel boots kernel_main function is executed. The initial stack is configured to start at LOW_MEMORY , which is at 0x0040:0000 (4 MB). Task 1 creation kernel_main calls copy_process for the first time. A new 4 KB page is allocated, and task_struct is placed at the bottom of this page. Task 2 creation kernel_main calls copy_process for the second time and the same process repeats. Task 2 is created and added to the task list. Switching to task 1; task 1 runs kernel_main voluntarily calls schedule function and it decides to switch to task 1. cpu_switch_to saves callee-saved registers in the init task cpu_context , which is located inside the kernel image. cpu_switch_to restores callee-saved registers from task 1's task_struct . At this point, cpu_context.sp points to 0x00401000 , lr points to ret_from_fork function, x19 contains a pointer to the start of process() and x20 a pointer to string \"12345\", which is located somewhere in the kernel image. cpu_switch_to executes ret , which jumps to the ret_from_fork function. ret_from_fork reads x19 and x20 registers and calls process function with the argument \"12345\". After process function starts, the stack of task 1 begins to grow. A timer interrupt occurred kernel_entry macro saves all general purpose registers + elr_el1 and spsr_el1 to the bottom of task 1 stack (\"saved regs\" in the figure below). The kernel now executes in the irq context. It continues to grow the current stack which belongs to task 1. The growth is below the \"saved regs\" region and is marked as \"irq frame\" on the figure (i.e. the stack frame created by the execution in the irq context). The kernel proceeds to schedule and picks task 2. Switching to task 2; task 2 runs cpu_switch_to executes exactly the same sequence of steps that it does for task 1. Task 2 started to execute and it stack grows. Note: until now, the kernel has NOT executed eret for the previous irq. This is fine, as an implementation choice made for this particular experiment. How can we execute task 2 in the context of the previous irq? It works because ARM64 CPU does not differentiate execution in an irq context vs. in an exception (i.e. syscall) context. All the CPU knows is the current EL (we always stay at EL1 before/after the irq) and the irq enable status. And irqs have been enabled previously in timer_tick before schedule was called. Nevertheless, there's a more common design in which the kernel returns from the irq context before switching to a new task. See \"alternative design\" below. Another timer interrupt occurred while task 2 is running kernel_entry saves all general purpose registers + elr_el1 and spsr_el1 at the bottom of task 2's stack. Task 2's irq frame begins to grow. Scheduling out task 2 The kernel calls schedule() . It observes that all tasks have their counters set to 0 and set counters to their tasks priorities. schedule selects init task to run. (This is because all tasks now have their counters set to 1 and init task is the first in the list). But actually, it would be fully legal for schedule to select task 1 or task 2 at this point, because their counters has equal values. We are more interested in the case when task 1 is selected so let's now assume that this is what had happened. Switching to task 1, exiting from the 1st irq cpu_switch_to is called and it restores previously saved callee-saved registers from task 1 cpu_context . Link register now points here because this is the place from which cpu_switch_to was called last time when task 1 was executed. sp points to the bottom of task 1 interrupt stack. timer_tick function resumes execution, starting from this line. (Can you pinpoint the instruction?) It disables interrupts and finally kernel_exit is executed. By the time kernel_exit is called, task 1 irq frame is unwound. kernel_exit further unwinds the saved regs. Task 1 resumes normal execution kernel_exit restores all general purpose registers as well as elr_el1 and spsr_el1 . elr_el1 now points somewhere in the middle of the process function. sp points to the bottom of task 1 stack. (Note: the remaining task size depends on the size of local variables in process ) Finally, kernel_exit executes eret instruction which uses elr_el1 register to jump back to process function. Task 1 resumes it normal execution!","title":"Example workflow"},{"location":"lesson04b/rpi-os/#an-alternative-design","text":"When an interrupt happens, the CPU saves irq stack frame automatically on the stack of the current task, e.g. A. This is the same as the design above. The kernel copies the auto saved register contents from the irq frame to the current task's task_struct , representing the CPU context when this task was interrupted by irq. The kernel calls its scheduler and returns from the irq (possibly to a different task). The irq stack on the A's stack is then unwound. Now irq is on. Later, when A is scheduled in, the kernel restores its CPU context from A's task_struct . Can you implement the alternative design?","title":"An alternative design"},{"location":"lesson04b/rpi-os/#disable-preemption","text":"The kernel needs mechanism to (temporarily) disable preemption. Example: in creating a new task_struct , we do not want rescheduling to happen. Otherwise the scheduler may see an incomplete task_struct . In other words, the creation of task_struct should be atomic . To disable preemption, one method is to disable interrupts. Beyond that, the kernel also needs fine-grained control.","title":"Disable preemption"},{"location":"lesson04b/rpi-os/#per-task-preempt_count","text":"To task_struct , we add: struct task_struct { struct cpu_context cpu_context; long state; long counter; long priority; long preempt_count; // new addition }; preempt_count >0 indicates that right now the current task is non-preemptable. The following two functions operate on it: void preempt_disable(void) { current->preempt_count++;} void preempt_enable(void) { current->preempt_count--;} Seeing this flag, the kernel will not invoke scheduler() at all, let alone descheduling this task (i.e. switching to a different task). This is done via the following code. void timer_tick() { if (current->counter>0 || current->preempt_count >0) return; ... Why a count instead of a binary flag? This again mimics the Linux implementation. Individual kernel functions could increment & decrement preempt_count . If all kernel functions have finished decrementing preempt_count , the count drops to zero and the scheduler is free to deschedule the task. This mechanism is called reference count, which is common in system software. preempt_count does not prevent a task from shooting in its own foot, though. For instance, a misbehaving task calling schedule() when preempt_count > 0 will likely corrupt kernel data structures. Try it out!","title":"Per-task preempt_count"},{"location":"lesson04b/rpi-os/#creating-a-task_struct-atomically","text":"Going back to making copy_process atomic: int copy_process(unsigned long fn, unsigned long arg) { preempt_disable(); /* new addition */ struct task_struct *p; p = (struct task_struct *) get_free_page(); if (!p) return 1; p->priority = current->priority; p->state = TASK_RUNNING; p->counter = p->priority; p->preempt_count = 1; // new addition p->cpu_context.x19 = fn; p->cpu_context.x20 = arg; p->cpu_context.pc = (unsigned long)ret_from_fork; p->cpu_context.sp = (unsigned long)p + THREAD_SIZE; int pid = nr_tasks++; task[pid] = p; preempt_enable(); /* new addition */ return 0; } preempt_count is set to 1, preventing the new task, once it starts to execute, from being preempted until it completes some initialization work. After that, the new task executes ret_from_fork , which calls schedule_tail() which will call preempt_enable() // entry.S .globl ret_from_fork ret_from_fork: bl schedule_tail ...","title":"Creating a task_struct atomically"},{"location":"lesson04b/rpi-os/#making-the-scheduling-algorithm-atomic","text":"The scheduler is non-reentrant . Making it atomic looks easy: we just call preempt_disable/enable() upon entering/leaving the scheduler. void _schedule(void) { preempt_disable(); /* new addition */ int next,c; struct task_struct * p; while (1) { c = -1; next = 0; for (int i = 0; i < NR_TASKS; i++){ p = task[i]; if (p && p->state == TASK_RUNNING && p->counter > c) { c = p->counter; next = i; } } if (c) { break; } for (int i = 0; i < NR_TASKS; i++) { p = task[i]; if (p) { p->counter = (p->counter >> 1) + p->priority; } } } switch_to(task[next]); preempt_enable(); /* new addition */ } Why does the kernel disable preemption , instead of disabling all interrupts? By design, if no TASK_RUNNING tasks are there, the scheduler will run its while loop over and over again until some of the tasks will move to TASK_RUNNING state. But if we are running on a single CPU, how then a task state can change while this loop is running? The answer is that if some task is waiting for an interrupt, this interrupt can happen while schedule function is executed and interrupt handler can change the state of the task. This actually explains why interrupts must be enabled during schedule execution. This also demonstrates an important distinction between disabling interrupts and disabling preemption. schedule disables preemption for the duration of the whole function. This ensures that nested schedule will not be called while we are in the middle of the original function execution. However, interrupts can legally happen during schedule function execution. Note: our kernel does not (yet) have the mechanism for tasks to wait for interrupts. It's a important mechanism to be added. I am not very satisfied with leaving interrupt on during schedule(). There shall be an idle task which does WFI when no other tasks are runnable. In that way, the scheduler can avoid spinning and can run with interrupt off. To implement the idle task, the kernel shall implement task wait state.","title":"Making the scheduling algorithm atomic"},{"location":"lesson04b/rpi-os/#conclusion","text":"We are done with scheduling, but right now our kernel can manage only kernel threads: they are executed at EL1 and can directly access any kernel functions or data. In the next 2 lessons we are going fix this and introduce system calls and virtual memory. Previous Page 3.5 Interrupt handling: Exercises Next Page 4.2 Process scheduler: Scheduler basic structures","title":"Conclusion"},{"location":"lesson04b/linux/basic_structures/","text":"4.2: Scheduler basic structures In all previous lessons we have been working mostly with either architecture specific code or driver code, and now it is the first time we will dig deep into the core of the Linux kernel. This task isn't simple, and it requires some preparations: before you will be able to understand the Linux scheduler source code, you need to become familiar with a few major concepts that the scheduler is based on. task_struct This is one of the most critical structures in the whole kernel \u2014 it contains all information about a running task. We already briefly touched task_struct in lesson 2 and we even have implemented our own task_struct for the RPi OS, so I assume that by this time you should already have a basic understanding how it is used. Now I want to highlight a few important fields of this struct that are relevant to our discussion. thread_info This is the first field of the task_struct and it contains all fields that must be accessed by the low-level architecture code. We have already seen how this happens in lesson 2 and will encounter a few other examples later. thread_info is architecture specific. In arm64 case, it is a simple structure with a few fields. struct thread_info { unsigned long flags; /* low level flags */ mm_segment_t addr_limit; /* address limit */ #ifdef CONFIG_ARM64_SW_TTBR0_PAN u64 ttbr0; /* saved TTBR0_EL1 */ #endif int preempt_count; /* 0 => preemptable, <0 => bug */ }; flags field is used very frequently \u2014 it contains information about the current task state (whether it is under a trace, whether a signal is pending, etc.). All possible flags values can be found here . state Task current state (whether it is currently running, waiting for an interrupt, exited etc.). All possible task states are described here . stack When working on the RPi OS, we have seen that task_struct is always kept at the bottom of the task stack, so we can use a pointer to task_struct as a pointer to the stack. Kernel stacks have constant size, so finding stack end is also an easy task. I think that the same approach was used in the early versions of the Linux kernel, but right now, after the introduction of the vitually mapped stacks , stack field is used to store a pointer to the kernel stack. thread Another important architecture specific structure is thread_struct . It contains all information (such as cpu_context ) that is used during a context switch. In fact, the RPi OS implements its own cpu_context that is used exactly in the same way as the original one. sched_class and sched_entity Those fields are used in schedule algorithm - more on them follows. Scheduler class In Linux, there is an extendable mechanism that allows each task to use its own scheduling algorithm. This mechanism uses a structure sched_class . You can think about this structure as an interface that defines all methods that a schedules class have to implement. Let's see what kind of methods are defined in the sched_class interface. (Not all of the methods are shown, but only those that I consider the most important for us) enqueue_task is executed each time a new task is added to a scheduler class. dequeue_task is called when a task can be removed from the scheduler. pick_next_task Core scheduler code calls this method when it needs to decide which task to run next. task_tick is called on each timer tick and gives the scheduler class an opportunity to measure how long the current task is being executed and notify core scheduler code if it needs to be preempted. There are a few sched_class implementations. The most commonly used one, which is typically used for all user tasks, is called \"Completely Fair Scheduler (CFS)\" Completely Fair Scheduler (CFS) The principals behind CFS algorithm are very simple: 1. For each task in the system, CFS measures how much CPU time has been already allocated to it (The value is stored in sum_exec_runtime field of the sched_entity structure) 1. sum_exec_runtime is adjusted accordingly to task priority and saved in vruntime field. 1. When CFS need to pick a new task to execute, the one with the smallest vruntime is selected. Linux scheduler uses another important data structure that is called \"runqueue\" and is described by the rq struct. There is a single instance of a runqueue per CPU. When a new task needs to be selected for execution, the selection is made only from the local runqueue. But if there is a need, tasks can be balanced between different rq structures. Runqueues are used by all scheduler classes, not only by CFS. All CFS specific information is kept in cfs_rq struct, which is embedded in the rq struct. One important field of the cfs_rq struct is called min_vruntime \u2014 this is the lowest vruntime from all tasks, assigned to a runqueue. min_vruntime is assigned to a newly forked task \u2014 this ensures that the task will be selected next, because CFS always ensures that a task with the smallest vruntime is picked. This approach also ensures that the new task will not be running for an unreasonably long time before it will be preempted. All tasks, assigned to a particular runqueue and tracked by CFS are kept in tasks_timeline field of the cfs_rq struct. tasks_timeline represents a Red\u2013black tree , which can be used to pick tasks ordered by their vruntime value. Red-black trees have an important property: all operations on it (search, insert, delete) can be done in O(log n) time. This means that even if we have thousands of concurrent tasks in the system all scheduler methods still executes very quickly. Another important property of a red-black tree is that for any node in the tree its right child will always have larger vruntime value than the parent, and left child's vruntime will be always less or equal then the parent's vruntime . This has an important implication: the leftmost node is always the one with the smallest vruntime . struct pt_regs I have already shown you how all general purpose registers are saved on the stack when an interrupt is taken \u2014 we explored how this process works in Linux kernel and implemented a similar one for the RPi OS. What I haven't mentioned yet, is that it is entirely legal to manipulate those registers while processing an interrupt. It is also legal to manually prepare a set of registers and put them on the stack \u2014 this is done when a kernel thread is moved to a user mode, and we are going to implement the same functionality in the next lesson. For now, you just need to remember that pt_regs structure is used to describe saved registers and it must have its fields ordered in the same order in with register are saved in the kernel_entry macro. In this lesson, we are going to see a few examples of how this structure is used. Conclusion There are far more important structures, algorithms and concepts related to scheduling, but what we've just explored is a minimal set, required to understand the next two chapters. Now it's time to see how all of this actually works. Previous Page 4.1 Process scheduler: RPi OS Scheduler Next Page 4.3 Process scheduler: Forking a task","title":"Basic structures"},{"location":"lesson04b/linux/basic_structures/#42-scheduler-basic-structures","text":"In all previous lessons we have been working mostly with either architecture specific code or driver code, and now it is the first time we will dig deep into the core of the Linux kernel. This task isn't simple, and it requires some preparations: before you will be able to understand the Linux scheduler source code, you need to become familiar with a few major concepts that the scheduler is based on.","title":"4.2: Scheduler basic structures"},{"location":"lesson04b/linux/basic_structures/#task_struct","text":"This is one of the most critical structures in the whole kernel \u2014 it contains all information about a running task. We already briefly touched task_struct in lesson 2 and we even have implemented our own task_struct for the RPi OS, so I assume that by this time you should already have a basic understanding how it is used. Now I want to highlight a few important fields of this struct that are relevant to our discussion. thread_info This is the first field of the task_struct and it contains all fields that must be accessed by the low-level architecture code. We have already seen how this happens in lesson 2 and will encounter a few other examples later. thread_info is architecture specific. In arm64 case, it is a simple structure with a few fields. struct thread_info { unsigned long flags; /* low level flags */ mm_segment_t addr_limit; /* address limit */ #ifdef CONFIG_ARM64_SW_TTBR0_PAN u64 ttbr0; /* saved TTBR0_EL1 */ #endif int preempt_count; /* 0 => preemptable, <0 => bug */ }; flags field is used very frequently \u2014 it contains information about the current task state (whether it is under a trace, whether a signal is pending, etc.). All possible flags values can be found here . state Task current state (whether it is currently running, waiting for an interrupt, exited etc.). All possible task states are described here . stack When working on the RPi OS, we have seen that task_struct is always kept at the bottom of the task stack, so we can use a pointer to task_struct as a pointer to the stack. Kernel stacks have constant size, so finding stack end is also an easy task. I think that the same approach was used in the early versions of the Linux kernel, but right now, after the introduction of the vitually mapped stacks , stack field is used to store a pointer to the kernel stack. thread Another important architecture specific structure is thread_struct . It contains all information (such as cpu_context ) that is used during a context switch. In fact, the RPi OS implements its own cpu_context that is used exactly in the same way as the original one. sched_class and sched_entity Those fields are used in schedule algorithm - more on them follows.","title":"task_struct"},{"location":"lesson04b/linux/basic_structures/#scheduler-class","text":"In Linux, there is an extendable mechanism that allows each task to use its own scheduling algorithm. This mechanism uses a structure sched_class . You can think about this structure as an interface that defines all methods that a schedules class have to implement. Let's see what kind of methods are defined in the sched_class interface. (Not all of the methods are shown, but only those that I consider the most important for us) enqueue_task is executed each time a new task is added to a scheduler class. dequeue_task is called when a task can be removed from the scheduler. pick_next_task Core scheduler code calls this method when it needs to decide which task to run next. task_tick is called on each timer tick and gives the scheduler class an opportunity to measure how long the current task is being executed and notify core scheduler code if it needs to be preempted. There are a few sched_class implementations. The most commonly used one, which is typically used for all user tasks, is called \"Completely Fair Scheduler (CFS)\"","title":"Scheduler class"},{"location":"lesson04b/linux/basic_structures/#completely-fair-scheduler-cfs","text":"The principals behind CFS algorithm are very simple: 1. For each task in the system, CFS measures how much CPU time has been already allocated to it (The value is stored in sum_exec_runtime field of the sched_entity structure) 1. sum_exec_runtime is adjusted accordingly to task priority and saved in vruntime field. 1. When CFS need to pick a new task to execute, the one with the smallest vruntime is selected. Linux scheduler uses another important data structure that is called \"runqueue\" and is described by the rq struct. There is a single instance of a runqueue per CPU. When a new task needs to be selected for execution, the selection is made only from the local runqueue. But if there is a need, tasks can be balanced between different rq structures. Runqueues are used by all scheduler classes, not only by CFS. All CFS specific information is kept in cfs_rq struct, which is embedded in the rq struct. One important field of the cfs_rq struct is called min_vruntime \u2014 this is the lowest vruntime from all tasks, assigned to a runqueue. min_vruntime is assigned to a newly forked task \u2014 this ensures that the task will be selected next, because CFS always ensures that a task with the smallest vruntime is picked. This approach also ensures that the new task will not be running for an unreasonably long time before it will be preempted. All tasks, assigned to a particular runqueue and tracked by CFS are kept in tasks_timeline field of the cfs_rq struct. tasks_timeline represents a Red\u2013black tree , which can be used to pick tasks ordered by their vruntime value. Red-black trees have an important property: all operations on it (search, insert, delete) can be done in O(log n) time. This means that even if we have thousands of concurrent tasks in the system all scheduler methods still executes very quickly. Another important property of a red-black tree is that for any node in the tree its right child will always have larger vruntime value than the parent, and left child's vruntime will be always less or equal then the parent's vruntime . This has an important implication: the leftmost node is always the one with the smallest vruntime .","title":"Completely Fair Scheduler (CFS)"},{"location":"lesson04b/linux/basic_structures/#struct-pt_regs","text":"I have already shown you how all general purpose registers are saved on the stack when an interrupt is taken \u2014 we explored how this process works in Linux kernel and implemented a similar one for the RPi OS. What I haven't mentioned yet, is that it is entirely legal to manipulate those registers while processing an interrupt. It is also legal to manually prepare a set of registers and put them on the stack \u2014 this is done when a kernel thread is moved to a user mode, and we are going to implement the same functionality in the next lesson. For now, you just need to remember that pt_regs structure is used to describe saved registers and it must have its fields ordered in the same order in with register are saved in the kernel_entry macro. In this lesson, we are going to see a few examples of how this structure is used.","title":"struct pt_regs"},{"location":"lesson04b/linux/basic_structures/#conclusion","text":"There are far more important structures, algorithms and concepts related to scheduling, but what we've just explored is a minimal set, required to understand the next two chapters. Now it's time to see how all of this actually works.","title":"Conclusion"},{"location":"lesson04b/linux/basic_structures/#previous-page","text":"4.1 Process scheduler: RPi OS Scheduler","title":"Previous Page"},{"location":"lesson04b/linux/basic_structures/#next-page","text":"4.3 Process scheduler: Forking a task","title":"Next Page"},{"location":"lesson04b/linux/fork/","text":"4.3: Forking a task Scheduling is all about selecting a proper task to run from the list of available tasks. But before the scheduler will be able to do its job we need to somehow fill this list. The way in which new tasks can be created is the main topic of this chapter. For now, we want to focus only on kernel threads and postpone the discussion of user-mode functionality till the next lesson. However, not everywhere it will be possible, so be prepared to learn a little bit about executing tasks in user mode as well. Init task When the kernel is started there is a single task running: init task. The corresponding task_struct is defined here and is initialized by INIT_TASK macro. This task is critical for the system because all other tasks in the system are derived from it. Creating new tasks In Linux it is not possible to create a new task from scratch - instead, all tasks are forked from a currently running task. Now, as we've seen from were the initial task came from, we can try to explore how new tasks can be created from it. There are 4 ways in which a new task can be created. fork system call creates a full copy of the current process, including its virtual memory and it is used to create new processes (not threads). This syscall is defined here . vfork system call is similar to fork but it differs in that the child reuses parent virtual memory as well as stack, and the parent is blocked until the child finished execution. The definition of this syscall can be found here . clone system call is the most flexible one - it also copies the current task but it allows to customize the process using flags parameter and allows to configure the entry point for the child task. In the next lesson, we will see how glibc clone wrapper function is implemented - this wrapper allows to use clone syscall to create new threads. Finally, kernel_thread function can be used to create new kernel threads. All of the above functions calls _do_fork , which accept the following arguments. clone_flags Flags are used to configure fork behavior. The complete list of the flags can be found here . stack_start In case of clone syscall this parameter indicates the location of the user stack for the new task. If 'kernel_thread' calls _do_fork this parameter points to the function that needs to be executed in a kernel thread. stack_size In arm64 architecture this parameter is only used in the case when _do_fork is called by `kernel_thread. It is a pointer to the argument that needs to be passed to the kernel thread function. (And yes, I also find the naming of the last two parameters misleading) parent_tidptr child_tidptr Those 2 parameters are used only in clone syscall. Fork will store the child thread ID at the location parent_tidptr in the parent's memory, or it can store parent's ID at child_tidptr location. tls Thread Local Storage Fork procedure Next, I want to highlight the most important events that take place during _do_fork execution, preserving their order. _do_fork calls copy_process copy_process is responsible for configuring new task_struct . copy_process calls dup_task_struct , which allocates new task_struct and copies all fields from the original one. Actual copying takes place in the architecture-specific arch_dup_task_struct New kernel stack is allocated. If CONFIG_VMAP_STACK is enabled the kernel uses virtually mapped stacks to protect against kernel stack overflow. link Task's credentials are copied. link The scheduler is notified that a new task is forked. link task_fork_fair method of the CFS scheduler class is called. This method updates vruntime value for the currently running task (this is done inside update_curr function) and updates min_vruntime value for the current runqueue (inside update_min_vruntime ). Then min_vruntime value is assigned to the forked task - this ensures that this task will be picked up next. Note, that at this point of time new task still hasn't been added to the task_timeline . A lot of different properties, such as information about filesystems, open files, virtual memory, signals, namespaces, are either reused or copied from the current task. The decision whether to copy something or reuse current property is usually made based on the clone_flags parameter. link copy_thread_tls is called which in turn calls architecture specific copy_thread function. This function deserves a special attention because it works as a prototype for the copy_process function in the RPi OS, and I want to investigate it deeper. copy_thread The whole function is listed below. int copy_thread(unsigned long clone_flags, unsigned long stack_start, unsigned long stk_sz, struct task_struct *p) { struct pt_regs *childregs = task_pt_regs(p); memset(&p->thread.cpu_context, 0, sizeof(struct cpu_context)); if (likely(!(p->flags & PF_KTHREAD))) { *childregs = *current_pt_regs(); childregs->regs[0] = 0; /* * Read the current TLS pointer from tpidr_el0 as it may be * out-of-sync with the saved value. */ *task_user_tls(p) = read_sysreg(tpidr_el0); if (stack_start) { if (is_compat_thread(task_thread_info(p))) childregs->compat_sp = stack_start; else childregs->sp = stack_start; } /* * If a TLS pointer was passed to clone (4th argument), use it * for the new thread. */ if (clone_flags & CLONE_SETTLS) p->thread.tp_value = childregs->regs[3]; } else { memset(childregs, 0, sizeof(struct pt_regs)); childregs->pstate = PSR_MODE_EL1h; if (IS_ENABLED(CONFIG_ARM64_UAO) && cpus_have_const_cap(ARM64_HAS_UAO)) childregs->pstate |= PSR_UAO_BIT; p->thread.cpu_context.x19 = stack_start; p->thread.cpu_context.x20 = stk_sz; } p->thread.cpu_context.pc = (unsigned long)ret_from_fork; p->thread.cpu_context.sp = (unsigned long)childregs; ptrace_hw_copy_thread(p); return 0; } Some of this code can be already a little bit familiar to you. Let's dig dipper into it. struct pt_regs *childregs = task_pt_regs(p); The function starts with allocating new pt_regs struct. This struct is used to provide access to the registers, saved during kernel_entry . childregs variable then can be used to prepare whatever state we need for the newly created task. If the task then decides to move to user mode the state will be restored by the kernel_exit macro. An important thing to understand here is that task_pt_regs macro doesn't allocate anything - it just calculate the position on the kernel stack, were kernel_entry stores registers, and for the newly created task, this position will always be at the top of the kernel stack. memset(&p->thread.cpu_context, 0, sizeof(struct cpu_context)); Next, forked task cpu_context is cleared. if (likely(!(p->flags & PF_KTHREAD))) { Then a check is made to determine whether we are creating a kernel or a user thread. For now, we are interested only in kernel thread case and we will discuss the second option in the next lesson. memset(childregs, 0, sizeof(struct pt_regs)); childregs->pstate = PSR_MODE_EL1h; if (IS_ENABLED(CONFIG_ARM64_UAO) && cpus_have_const_cap(ARM64_HAS_UAO)) childregs->pstate |= PSR_UAO_BIT; p->thread.cpu_context.x19 = stack_start; p->thread.cpu_context.x20 = stk_sz; If we are creating a kernel thread x19 and x20 registers of the cpu_context are set to point to the function that needs to be executed ( stack_start ) and its argument ( stk_sz ). After CPU will be switched to the forked task, ret_from_fork will use those registers to jump to the needed function. (I don't quite understand why do we also need to set childregs->pstate here. ret_from_fork will not call kernel_exit before jumping to the function stored in x19 , and even if the kernel thread decides to move to the user mode childregs will be overwritten anyway. Any ideas?) p->thread.cpu_context.pc = (unsigned long)ret_from_fork; p->thread.cpu_context.sp = (unsigned long)childregs; Next cpu_context.pc is set to ret_from_fork pointer - this ensures that we return to the ret_from_fork after the first context switch. cpu_context.sp is set to the location just below the childregs . We still need childregs at the top of the stack because after the kernel thread finishes its execution the task will be moved to user mode and childregs structure will be used. In the next lesson, we will discuss in details how this happens. That's it about copy_thread function. Now let's return to the place in the fork procedure from where we left. Fork procedure (continued) After copy_process succsesfully prepares task_struct for the forked task _do_fork can now run it by calling wake_up_new_task . This is done here . Then task state is changed to TASK_RUNNING and enqueue_task_fair CFS method is called, wich triggers execution of the __enqueue_entity that actually adds task to the task_timeline red-black tree. At this line, check_preempt_curr is called, which in turn calls check_preempt_wakeup CFS method. This method is responsible for checking whether the current task should be preempted by some other task. That is exactly what is going to happen because we have just put a new task on the timeline that has minimal possible vruntime . So resched_curr function is triggered, which sets TIF_NEED_RESCHED flag for the current task. TIF_NEED_RESCHED is checked just before the current task exit from an exception handler ( fork , vfork and clone are all system call, and each system call is a special type of exception.). The check is made here . Note that _TIF_WORK_MASK includes _TIF_NEED_RESCHED . It is also important to understand that in case of a kernel thread creation, the new thread will not be started until the next timer tick or until the parent task volatirely calls schedule() . If the current task needs to be rescheduled, do_notify_resume is triggered, which in turn calls schedule . Finally we reached the point where task scheduling is triggered, and we are going to stop at this point. Conclusion Now that you understand how new tasks are created and added to the scheduler, it is time to take a look on how the scheduler itself works and how context switch is implemented. That is something we are going to explore in the next chapter. Previous Page 4.2 Process scheduler: Scheduler basic structures Next Page 4.4 Process scheduler: Scheduler","title":"Fork"},{"location":"lesson04b/linux/fork/#43-forking-a-task","text":"Scheduling is all about selecting a proper task to run from the list of available tasks. But before the scheduler will be able to do its job we need to somehow fill this list. The way in which new tasks can be created is the main topic of this chapter. For now, we want to focus only on kernel threads and postpone the discussion of user-mode functionality till the next lesson. However, not everywhere it will be possible, so be prepared to learn a little bit about executing tasks in user mode as well.","title":"4.3: Forking a task"},{"location":"lesson04b/linux/fork/#init-task","text":"When the kernel is started there is a single task running: init task. The corresponding task_struct is defined here and is initialized by INIT_TASK macro. This task is critical for the system because all other tasks in the system are derived from it.","title":"Init task"},{"location":"lesson04b/linux/fork/#creating-new-tasks","text":"In Linux it is not possible to create a new task from scratch - instead, all tasks are forked from a currently running task. Now, as we've seen from were the initial task came from, we can try to explore how new tasks can be created from it. There are 4 ways in which a new task can be created. fork system call creates a full copy of the current process, including its virtual memory and it is used to create new processes (not threads). This syscall is defined here . vfork system call is similar to fork but it differs in that the child reuses parent virtual memory as well as stack, and the parent is blocked until the child finished execution. The definition of this syscall can be found here . clone system call is the most flexible one - it also copies the current task but it allows to customize the process using flags parameter and allows to configure the entry point for the child task. In the next lesson, we will see how glibc clone wrapper function is implemented - this wrapper allows to use clone syscall to create new threads. Finally, kernel_thread function can be used to create new kernel threads. All of the above functions calls _do_fork , which accept the following arguments. clone_flags Flags are used to configure fork behavior. The complete list of the flags can be found here . stack_start In case of clone syscall this parameter indicates the location of the user stack for the new task. If 'kernel_thread' calls _do_fork this parameter points to the function that needs to be executed in a kernel thread. stack_size In arm64 architecture this parameter is only used in the case when _do_fork is called by `kernel_thread. It is a pointer to the argument that needs to be passed to the kernel thread function. (And yes, I also find the naming of the last two parameters misleading) parent_tidptr child_tidptr Those 2 parameters are used only in clone syscall. Fork will store the child thread ID at the location parent_tidptr in the parent's memory, or it can store parent's ID at child_tidptr location. tls Thread Local Storage","title":"Creating new tasks"},{"location":"lesson04b/linux/fork/#fork-procedure","text":"Next, I want to highlight the most important events that take place during _do_fork execution, preserving their order. _do_fork calls copy_process copy_process is responsible for configuring new task_struct . copy_process calls dup_task_struct , which allocates new task_struct and copies all fields from the original one. Actual copying takes place in the architecture-specific arch_dup_task_struct New kernel stack is allocated. If CONFIG_VMAP_STACK is enabled the kernel uses virtually mapped stacks to protect against kernel stack overflow. link Task's credentials are copied. link The scheduler is notified that a new task is forked. link task_fork_fair method of the CFS scheduler class is called. This method updates vruntime value for the currently running task (this is done inside update_curr function) and updates min_vruntime value for the current runqueue (inside update_min_vruntime ). Then min_vruntime value is assigned to the forked task - this ensures that this task will be picked up next. Note, that at this point of time new task still hasn't been added to the task_timeline . A lot of different properties, such as information about filesystems, open files, virtual memory, signals, namespaces, are either reused or copied from the current task. The decision whether to copy something or reuse current property is usually made based on the clone_flags parameter. link copy_thread_tls is called which in turn calls architecture specific copy_thread function. This function deserves a special attention because it works as a prototype for the copy_process function in the RPi OS, and I want to investigate it deeper.","title":"Fork procedure"},{"location":"lesson04b/linux/fork/#copy_thread","text":"The whole function is listed below. int copy_thread(unsigned long clone_flags, unsigned long stack_start, unsigned long stk_sz, struct task_struct *p) { struct pt_regs *childregs = task_pt_regs(p); memset(&p->thread.cpu_context, 0, sizeof(struct cpu_context)); if (likely(!(p->flags & PF_KTHREAD))) { *childregs = *current_pt_regs(); childregs->regs[0] = 0; /* * Read the current TLS pointer from tpidr_el0 as it may be * out-of-sync with the saved value. */ *task_user_tls(p) = read_sysreg(tpidr_el0); if (stack_start) { if (is_compat_thread(task_thread_info(p))) childregs->compat_sp = stack_start; else childregs->sp = stack_start; } /* * If a TLS pointer was passed to clone (4th argument), use it * for the new thread. */ if (clone_flags & CLONE_SETTLS) p->thread.tp_value = childregs->regs[3]; } else { memset(childregs, 0, sizeof(struct pt_regs)); childregs->pstate = PSR_MODE_EL1h; if (IS_ENABLED(CONFIG_ARM64_UAO) && cpus_have_const_cap(ARM64_HAS_UAO)) childregs->pstate |= PSR_UAO_BIT; p->thread.cpu_context.x19 = stack_start; p->thread.cpu_context.x20 = stk_sz; } p->thread.cpu_context.pc = (unsigned long)ret_from_fork; p->thread.cpu_context.sp = (unsigned long)childregs; ptrace_hw_copy_thread(p); return 0; } Some of this code can be already a little bit familiar to you. Let's dig dipper into it. struct pt_regs *childregs = task_pt_regs(p); The function starts with allocating new pt_regs struct. This struct is used to provide access to the registers, saved during kernel_entry . childregs variable then can be used to prepare whatever state we need for the newly created task. If the task then decides to move to user mode the state will be restored by the kernel_exit macro. An important thing to understand here is that task_pt_regs macro doesn't allocate anything - it just calculate the position on the kernel stack, were kernel_entry stores registers, and for the newly created task, this position will always be at the top of the kernel stack. memset(&p->thread.cpu_context, 0, sizeof(struct cpu_context)); Next, forked task cpu_context is cleared. if (likely(!(p->flags & PF_KTHREAD))) { Then a check is made to determine whether we are creating a kernel or a user thread. For now, we are interested only in kernel thread case and we will discuss the second option in the next lesson. memset(childregs, 0, sizeof(struct pt_regs)); childregs->pstate = PSR_MODE_EL1h; if (IS_ENABLED(CONFIG_ARM64_UAO) && cpus_have_const_cap(ARM64_HAS_UAO)) childregs->pstate |= PSR_UAO_BIT; p->thread.cpu_context.x19 = stack_start; p->thread.cpu_context.x20 = stk_sz; If we are creating a kernel thread x19 and x20 registers of the cpu_context are set to point to the function that needs to be executed ( stack_start ) and its argument ( stk_sz ). After CPU will be switched to the forked task, ret_from_fork will use those registers to jump to the needed function. (I don't quite understand why do we also need to set childregs->pstate here. ret_from_fork will not call kernel_exit before jumping to the function stored in x19 , and even if the kernel thread decides to move to the user mode childregs will be overwritten anyway. Any ideas?) p->thread.cpu_context.pc = (unsigned long)ret_from_fork; p->thread.cpu_context.sp = (unsigned long)childregs; Next cpu_context.pc is set to ret_from_fork pointer - this ensures that we return to the ret_from_fork after the first context switch. cpu_context.sp is set to the location just below the childregs . We still need childregs at the top of the stack because after the kernel thread finishes its execution the task will be moved to user mode and childregs structure will be used. In the next lesson, we will discuss in details how this happens. That's it about copy_thread function. Now let's return to the place in the fork procedure from where we left.","title":"copy_thread"},{"location":"lesson04b/linux/fork/#fork-procedure-continued","text":"After copy_process succsesfully prepares task_struct for the forked task _do_fork can now run it by calling wake_up_new_task . This is done here . Then task state is changed to TASK_RUNNING and enqueue_task_fair CFS method is called, wich triggers execution of the __enqueue_entity that actually adds task to the task_timeline red-black tree. At this line, check_preempt_curr is called, which in turn calls check_preempt_wakeup CFS method. This method is responsible for checking whether the current task should be preempted by some other task. That is exactly what is going to happen because we have just put a new task on the timeline that has minimal possible vruntime . So resched_curr function is triggered, which sets TIF_NEED_RESCHED flag for the current task. TIF_NEED_RESCHED is checked just before the current task exit from an exception handler ( fork , vfork and clone are all system call, and each system call is a special type of exception.). The check is made here . Note that _TIF_WORK_MASK includes _TIF_NEED_RESCHED . It is also important to understand that in case of a kernel thread creation, the new thread will not be started until the next timer tick or until the parent task volatirely calls schedule() . If the current task needs to be rescheduled, do_notify_resume is triggered, which in turn calls schedule . Finally we reached the point where task scheduling is triggered, and we are going to stop at this point.","title":"Fork procedure (continued)"},{"location":"lesson04b/linux/fork/#conclusion","text":"Now that you understand how new tasks are created and added to the scheduler, it is time to take a look on how the scheduler itself works and how context switch is implemented. That is something we are going to explore in the next chapter.","title":"Conclusion"},{"location":"lesson04b/linux/fork/#previous-page","text":"4.2 Process scheduler: Scheduler basic structures","title":"Previous Page"},{"location":"lesson04b/linux/fork/#next-page","text":"4.4 Process scheduler: Scheduler","title":"Next Page"},{"location":"lesson04b/linux/scheduler/","text":"4.4: Scheduler We have already learned a lot of details about the Linux scheduler inner workings, so there is not so much left for us. To make the whole picture complete in this chapter we will take a look at 2 important scheduler entry points: scheduler_tick() function, which is called at each timer interrupt. schedule() function, which is called each time when the current task needs to be rescheduled. The third major thing that we are going to investigate in this chapter is the concept of context switch. A context switch is the process that suspends the current task and runs another task instead - this process is highly architecture specific and closely correlates with what we have been doing when working with RPi OS. scheduler_tick This function is important for 2 reasons: It provides a way for the scheduler to update time statistics and runtime information for the current task. Runtime information then is used to determine whether the current task needs to be preempted, and if so schedule() function is called. As well as most of the previously explored functions, scheduler_tick is too complex to be fully explained - instead, as usual, I will just highlight the most important parts. The main work is done inside CFS method task_tick_fair . This method calls entity_tick for the sched_entity corresponding to the current task. When looking at the source code, you may be wondering why instead of just calling entry_tick for the current sched_entry , for_each_sched_entity macro is used instead? for_each_sched_entity doesn't iterate over all sched_entry in the system. Instead, it only traverses the sched_entry inheritance tree up to the root. This is useful when tasks are grouped - after updating runtime information for a particular task, sched_entry corresponding to the whole group is also updated. entity_tick does 2 main things: Calls update_curr , which is responsible for updating task's vruntime as well as runqueue's min_vruntime . An important thing to remember here is that vruntime is always based on 2 things: how long task has actually been executed and tasks priority. Calls check_preempt_tick , which checks whether the current task needs to be preempted. Preemption happens in 2 cases: If the current task has been running for too long (the comparison is made using normal time, not vruntime ). link If there is a task with smaller vruntime and the difference between vruntime values is greater than some threshold. link In both cases the current task is marked for preemption by calling resched_curr function. We have already seen in the previous chapter how calling resched_curr leads to TIF_NEED_RESCHED flag being set for the current task and eventually schedule being called. That's it about schedule_tick now we are finally ready to take a look at the schedule function. schedule We have already seen so many examples were schedule is used, so now you are probably anxious to see how this function actually works. You will be surprised to know that the internals of this function are rather simple. The main work is done inside __schedule function. __schedule calls pick_next_task which redirect most of the work to the pick_next_task_fair method of the CFS scheduler. As you might expect pick_next_task_fair in a normal case just selects the leftmost element from the red-black tree and returns it. It happens here . __schedule calls context_switch , which does some preparation work and calls architecture specific __switch_to function, where low-level arch specific task parameters are prepared to the switch. __switch_to first switches some additional task components, like, for example, TLS (Thread-local Store) and saved floating point and NEON registers. Actual switch takes place in the assembler cpu_switch_to function. This function should be already familiar to you because I copied it almost without any changes to the RPi OS. As you might remember, this function switches callee-saved registers and task stack. After it returns, the new task will be running using its own kernel stack. Conclusion Now we are done with the Linux scheduler. The good thing is that it appears to be not so difficult if you focus only on the very basic workflow. After you understand the basic workflow you probably might want to to make another path through the schedule code and pay more attention to the details, because there are so many of them. But for now, we are happy with our current understanding and ready to move to the following lesson, which describes user processes and system calls. Previous Page 4.3 Process scheduler: Forking a task Next Page 4.5 Process scheduler: Exercises","title":"Scheduler"},{"location":"lesson04b/linux/scheduler/#44-scheduler","text":"We have already learned a lot of details about the Linux scheduler inner workings, so there is not so much left for us. To make the whole picture complete in this chapter we will take a look at 2 important scheduler entry points: scheduler_tick() function, which is called at each timer interrupt. schedule() function, which is called each time when the current task needs to be rescheduled. The third major thing that we are going to investigate in this chapter is the concept of context switch. A context switch is the process that suspends the current task and runs another task instead - this process is highly architecture specific and closely correlates with what we have been doing when working with RPi OS.","title":"4.4: Scheduler"},{"location":"lesson04b/linux/scheduler/#scheduler_tick","text":"This function is important for 2 reasons: It provides a way for the scheduler to update time statistics and runtime information for the current task. Runtime information then is used to determine whether the current task needs to be preempted, and if so schedule() function is called. As well as most of the previously explored functions, scheduler_tick is too complex to be fully explained - instead, as usual, I will just highlight the most important parts. The main work is done inside CFS method task_tick_fair . This method calls entity_tick for the sched_entity corresponding to the current task. When looking at the source code, you may be wondering why instead of just calling entry_tick for the current sched_entry , for_each_sched_entity macro is used instead? for_each_sched_entity doesn't iterate over all sched_entry in the system. Instead, it only traverses the sched_entry inheritance tree up to the root. This is useful when tasks are grouped - after updating runtime information for a particular task, sched_entry corresponding to the whole group is also updated. entity_tick does 2 main things: Calls update_curr , which is responsible for updating task's vruntime as well as runqueue's min_vruntime . An important thing to remember here is that vruntime is always based on 2 things: how long task has actually been executed and tasks priority. Calls check_preempt_tick , which checks whether the current task needs to be preempted. Preemption happens in 2 cases: If the current task has been running for too long (the comparison is made using normal time, not vruntime ). link If there is a task with smaller vruntime and the difference between vruntime values is greater than some threshold. link In both cases the current task is marked for preemption by calling resched_curr function. We have already seen in the previous chapter how calling resched_curr leads to TIF_NEED_RESCHED flag being set for the current task and eventually schedule being called. That's it about schedule_tick now we are finally ready to take a look at the schedule function.","title":"scheduler_tick"},{"location":"lesson04b/linux/scheduler/#schedule","text":"We have already seen so many examples were schedule is used, so now you are probably anxious to see how this function actually works. You will be surprised to know that the internals of this function are rather simple. The main work is done inside __schedule function. __schedule calls pick_next_task which redirect most of the work to the pick_next_task_fair method of the CFS scheduler. As you might expect pick_next_task_fair in a normal case just selects the leftmost element from the red-black tree and returns it. It happens here . __schedule calls context_switch , which does some preparation work and calls architecture specific __switch_to function, where low-level arch specific task parameters are prepared to the switch. __switch_to first switches some additional task components, like, for example, TLS (Thread-local Store) and saved floating point and NEON registers. Actual switch takes place in the assembler cpu_switch_to function. This function should be already familiar to you because I copied it almost without any changes to the RPi OS. As you might remember, this function switches callee-saved registers and task stack. After it returns, the new task will be running using its own kernel stack.","title":"schedule"},{"location":"lesson04b/linux/scheduler/#conclusion","text":"Now we are done with the Linux scheduler. The good thing is that it appears to be not so difficult if you focus only on the very basic workflow. After you understand the basic workflow you probably might want to to make another path through the schedule code and pay more attention to the details, because there are so many of them. But for now, we are happy with our current understanding and ready to move to the following lesson, which describes user processes and system calls.","title":"Conclusion"},{"location":"lesson04b/linux/scheduler/#previous-page","text":"4.3 Process scheduler: Forking a task","title":"Previous Page"},{"location":"lesson04b/linux/scheduler/#next-page","text":"4.5 Process scheduler: Exercises","title":"Next Page"},{"location":"lesson05/exercises/","text":"5.3: Exercises When a task is executed in user mode, try to access some of the system registers. Make sure that a synchronous exception is generated in this case. Handle this exception, use esr_el1 register to distinguish it from a system call. Revisit: backport the switch of E0->EL1 to exp2. As a result, the kernel in exp2 can switch from EL1 to EL0 and then from EL0 to EL1. Implement a sleep() syscall. See its interface here . You do not have to implement the signal part. Deliverable A code tarball implementing (1) above. A code tarball implementing (2) above. A code tarball implementing (3) above.","title":"Exercises"},{"location":"lesson05/exercises/#53-exercises","text":"When a task is executed in user mode, try to access some of the system registers. Make sure that a synchronous exception is generated in this case. Handle this exception, use esr_el1 register to distinguish it from a system call. Revisit: backport the switch of E0->EL1 to exp2. As a result, the kernel in exp2 can switch from EL1 to EL0 and then from EL0 to EL1. Implement a sleep() syscall. See its interface here . You do not have to implement the signal part.","title":"5.3: Exercises"},{"location":"lesson05/exercises/#deliverable","text":"A code tarball implementing (1) above. A code tarball implementing (2) above. A code tarball implementing (3) above.","title":"Deliverable"},{"location":"lesson05/linux/","text":"5.2: User processes and system calls This chapter is going to be a short one. The reason is that I copied syscall implementation from Linux to RPi OS almost precisely, therefore it doesn't require a lot of explanations. But still I want to guide you through the Linux source code so that you can see where and how a particular syscall functionality is implemented. Creating first user process The first question we are going to tackle is how the first user process is created. A good place to start looking for the answer is start_kernel function - as we've seen previously, this is the first architecture independent function that is called early in the kernel boot process. This is where the kernel initialization begins, and it would make sense to run the first user process during kernel initialization. An indeed, if you follow the logic of start_kernel you will soon discover kernel_init function that has the following code. if (!try_to_run_init_process(\"/sbin/init\") || !try_to_run_init_process(\"/etc/init\") || !try_to_run_init_process(\"/bin/init\") || !try_to_run_init_process(\"/bin/sh\")) return 0; It looks like this is precisely what we are looking for. From this code we can also infer where exactly and in which order Linux kernel looks for the init user program. try_to_run_init_process then executes do_execve function, which is also responsible for handling execve system call. This system call reads a binary executable file and runs it inside the current process. The logic behind the execve system call will be explored in details in the lesson 9, for now, it will be enough to mention that the most important work that this syscall does is parsing executable file and loading its content into memory, and it is done inside load_elf_binary function. (Here we assume that the executable file is in ELF format - it is the most popular, but not the only choice) At the end of load_elf_binary method (actually here ) there is a call to architecture specific start_thread function. I used it as a prototype for the RPi OS move_to_user_mode routine, and this is the code that we mostly care about. Here it is. static inline void start_thread_common(struct pt_regs *regs, unsigned long pc) { memset(regs, 0, sizeof(*regs)); forget_syscall(regs); regs->pc = pc; } static inline void start_thread(struct pt_regs *regs, unsigned long pc, unsigned long sp) { start_thread_common(regs, pc); regs->pstate = PSR_MODE_EL0t; regs->sp = sp; } By the time start_thread is executed, the current process operates in the kernel mode. start_thread is given access to the current pt_regs struct, which is used to set saved pstate , sp and pc fields. The logic here is exactly the same as in the RPi OS move_to_user_mode function, so I don't want to repeat it one more time. An important thing to remember is that start_thread prepares saved processor state in such a way that kernel_exit macro will eventually move the process to user mode. Linux syscalls It would be no surprise to you that the primary syscall mechanism is exactly the same in Linux and RPi OS. Now we are going to use already familiar clone syscall to understand the details of this mechanism. It would make sense to start our exploration with the glibc clone wrapper function . It works exactly the same as call_sys_clone function in the RPi OS, with the exception that the former function performers arguments sanity check and handles exceptions properly. An important thing to remember and understand is that in both cases we are using svc instruction to generate a synchronous exception, syscall number is passed using x8 register and all arguments are passed in registers x0 - x7 . Next, let's take a look at how clone syscall is defined. The definition can be found here and looks like this. SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp, int __user *, parent_tidptr, int __user *, child_tidptr, unsigned long, tls) { return _do_fork(clone_flags, newsp, 0, parent_tidptr, child_tidptr, tls); } SYSCALL_DEFINE5 macro has number 5 in its name indicating that we are defining a syscall with 5 parameters. The macro allocates and populates new syscall_metadata struct and creates sys_<syscall name> function. For example, in case of the clone syscall sys_clone function will be defined - this is the actuall syscall handler that will be called from the low-level architecture code. When a syscall is executed, the kernel needs a way to find syscall handler by the syscall number. The easiest way to achieve this is to create an array of pointers to syscall handlers and use syscall number as an index in this array. This is the approach we used in the RPi OS and exactly the same approach is used in the Linux kernel. The array of pointers to syscall handlers is called sys_call_table and is defined like this. void * const sys_call_table[__NR_syscalls] __aligned(4096) = { [0 ... __NR_syscalls - 1] = sys_ni_syscall, #include <asm/unistd.h> }; All syscalls are initially allocated to point to sys_ni_syscall function (\"ni\" here means \"non-existent\"). Syscall with number 0 and all syscalls that aren't defined for the current architecture will keep this handler. All other syscall handlers in the sys_call_table array are rewritten in the asm/unistd.h header file. As you might see, this file simply provides a mapping between syscall number and syscall handler function. Low-level syscall handling code Ok, we've seen how sys_call_table is created and populated, now it is time to investigate how it is used by the low-level syscall handling code. And once again the basic mechanism here will be almost exactly the same as in the RPi OS. We know that any syscall is a synchronous exception and all exception handlers are defined in the exception vector table (it is our favorite vectors array). The handler that we are interested in should be the one that handles synchronous exceptions generated at EL0. All of this makes it trivial to find the right handler, it is called el0_sync and looks like the following. el0_sync: kernel_entry 0 mrs x25, esr_el1 // read the syndrome register lsr x24, x25, #ESR_ELx_EC_SHIFT // exception class cmp x24, #ESR_ELx_EC_SVC64 // SVC in 64-bit state b.eq el0_svc cmp x24, #ESR_ELx_EC_DABT_LOW // data abort in EL0 b.eq el0_da cmp x24, #ESR_ELx_EC_IABT_LOW // instruction abort in EL0 b.eq el0_ia cmp x24, #ESR_ELx_EC_FP_ASIMD // FP/ASIMD access b.eq el0_fpsimd_acc cmp x24, #ESR_ELx_EC_FP_EXC64 // FP/ASIMD exception b.eq el0_fpsimd_exc cmp x24, #ESR_ELx_EC_SYS64 // configurable trap b.eq el0_sys cmp x24, #ESR_ELx_EC_SP_ALIGN // stack alignment exception b.eq el0_sp_pc cmp x24, #ESR_ELx_EC_PC_ALIGN // pc alignment exception b.eq el0_sp_pc cmp x24, #ESR_ELx_EC_UNKNOWN // unknown exception in EL0 b.eq el0_undef cmp x24, #ESR_ELx_EC_BREAKPT_LOW // debug exception in EL0 b.ge el0_dbg b el0_inv Here esr_el1 (exception syndrome register) is used to figure out whether the current exception is a system call. If this is the case el0_svc function is called. This function is listed below. el0_svc: adrp stbl, sys_call_table // load syscall table pointer mov wscno, w8 // syscall number in w8 mov wsc_nr, #__NR_syscalls el0_svc_naked: // compat entry point stp x0, xscno, [sp, #S_ORIG_X0] // save the original x0 and syscall number enable_dbg_and_irq ct_user_exit 1 ldr x16, [tsk, #TSK_TI_FLAGS] // check for syscall hooks tst x16, #_TIF_SYSCALL_WORK b.ne __sys_trace cmp wscno, wsc_nr // check upper syscall limit b.hs ni_sys ldr x16, [stbl, xscno, lsl #3] // address in the syscall table blr x16 // call sys_* routine b ret_fast_syscall ni_sys: mov x0, sp bl do_ni_syscall b ret_fast_syscall ENDPROC(el0_svc) Now, let's examine it line by line. el0_svc: adrp stbl, sys_call_table // load syscall table pointer mov wscno, w8 // syscall number in w8 mov wsc_nr, #__NR_syscalls The first 3 lines initialize stbl , wscno and wsc_nr variables that are just aliases for some registers. stbl holds the address of the syscall table, wsc_nr contains the total number of system calls and wscno is the current syscall number from w8 register. stp x0, xscno, [sp, #S_ORIG_X0] // save the original x0 and syscall number As you might remember from our RPi OS syscall discussion, x0 is overwritten in the pt_regs area after a syscall finishes. In case when original value of the x0 register might be needed, it is saved in the separate field of the pt_regs struct. Similarly syscall number is also saved in the pt_regs . enable_dbg_and_irq Interrupts and debug exceptions are enabled. ct_user_exit 1 The event of switching from the user mode to the kernel mode is recorded. ldr x16, [tsk, #TSK_TI_FLAGS] // check for syscall hooks tst x16, #_TIF_SYSCALL_WORK b.ne __sys_trace In case the current task is executed under a syscall tracer _TIF_SYSCALL_WORK flag should be set. In this case, __sys_trace function will be called. As our discussion is only focused on the general case, we are going to skip this function. cmp wscno, wsc_nr // check upper syscall limit b.hs ni_sys If current syscall number is greater then total syscall count an error is returned to the user. ldr x16, [stbl, xscno, lsl #3] // address in the syscall table blr x16 // call sys_* routine b ret_fast_syscall Syscall number is used as an index in the syscall table array to find the handler. Then handler address is loaded into x16 register and it is executed. Finally ret_fast_syscall function is called. ret_fast_syscall: disable_irq // disable interrupts str x0, [sp, #S_X0] // returned x0 ldr x1, [tsk, #TSK_TI_FLAGS] // re-check for syscall tracing and x2, x1, #_TIF_SYSCALL_WORK cbnz x2, ret_fast_syscall_trace and x2, x1, #_TIF_WORK_MASK cbnz x2, work_pending enable_step_tsk x1, x2 kernel_exit 0 The important things here are the first line, were interrupts are disabled, and the last line, were kernel_exit is called - everything else is related to special case processing. So as you might guess this is the place where a system call actually finishes and execution is transfered to user process. Conclusion We've now gone through the process of generating and processing a system call. This process is relatively simple, but it is vital for the OS, because it allows the kernel to set up an API and make sure that this API is the only mean of communication between a user program and the kernel. Previous Page 5.1 User processes and system calls: RPi OS Next Page 5.3 User processes and system calls: Exercises","title":"Linux"},{"location":"lesson05/linux/#52-user-processes-and-system-calls","text":"This chapter is going to be a short one. The reason is that I copied syscall implementation from Linux to RPi OS almost precisely, therefore it doesn't require a lot of explanations. But still I want to guide you through the Linux source code so that you can see where and how a particular syscall functionality is implemented.","title":"5.2: User processes and system calls"},{"location":"lesson05/linux/#creating-first-user-process","text":"The first question we are going to tackle is how the first user process is created. A good place to start looking for the answer is start_kernel function - as we've seen previously, this is the first architecture independent function that is called early in the kernel boot process. This is where the kernel initialization begins, and it would make sense to run the first user process during kernel initialization. An indeed, if you follow the logic of start_kernel you will soon discover kernel_init function that has the following code. if (!try_to_run_init_process(\"/sbin/init\") || !try_to_run_init_process(\"/etc/init\") || !try_to_run_init_process(\"/bin/init\") || !try_to_run_init_process(\"/bin/sh\")) return 0; It looks like this is precisely what we are looking for. From this code we can also infer where exactly and in which order Linux kernel looks for the init user program. try_to_run_init_process then executes do_execve function, which is also responsible for handling execve system call. This system call reads a binary executable file and runs it inside the current process. The logic behind the execve system call will be explored in details in the lesson 9, for now, it will be enough to mention that the most important work that this syscall does is parsing executable file and loading its content into memory, and it is done inside load_elf_binary function. (Here we assume that the executable file is in ELF format - it is the most popular, but not the only choice) At the end of load_elf_binary method (actually here ) there is a call to architecture specific start_thread function. I used it as a prototype for the RPi OS move_to_user_mode routine, and this is the code that we mostly care about. Here it is. static inline void start_thread_common(struct pt_regs *regs, unsigned long pc) { memset(regs, 0, sizeof(*regs)); forget_syscall(regs); regs->pc = pc; } static inline void start_thread(struct pt_regs *regs, unsigned long pc, unsigned long sp) { start_thread_common(regs, pc); regs->pstate = PSR_MODE_EL0t; regs->sp = sp; } By the time start_thread is executed, the current process operates in the kernel mode. start_thread is given access to the current pt_regs struct, which is used to set saved pstate , sp and pc fields. The logic here is exactly the same as in the RPi OS move_to_user_mode function, so I don't want to repeat it one more time. An important thing to remember is that start_thread prepares saved processor state in such a way that kernel_exit macro will eventually move the process to user mode.","title":"Creating first user process"},{"location":"lesson05/linux/#linux-syscalls","text":"It would be no surprise to you that the primary syscall mechanism is exactly the same in Linux and RPi OS. Now we are going to use already familiar clone syscall to understand the details of this mechanism. It would make sense to start our exploration with the glibc clone wrapper function . It works exactly the same as call_sys_clone function in the RPi OS, with the exception that the former function performers arguments sanity check and handles exceptions properly. An important thing to remember and understand is that in both cases we are using svc instruction to generate a synchronous exception, syscall number is passed using x8 register and all arguments are passed in registers x0 - x7 . Next, let's take a look at how clone syscall is defined. The definition can be found here and looks like this. SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp, int __user *, parent_tidptr, int __user *, child_tidptr, unsigned long, tls) { return _do_fork(clone_flags, newsp, 0, parent_tidptr, child_tidptr, tls); } SYSCALL_DEFINE5 macro has number 5 in its name indicating that we are defining a syscall with 5 parameters. The macro allocates and populates new syscall_metadata struct and creates sys_<syscall name> function. For example, in case of the clone syscall sys_clone function will be defined - this is the actuall syscall handler that will be called from the low-level architecture code. When a syscall is executed, the kernel needs a way to find syscall handler by the syscall number. The easiest way to achieve this is to create an array of pointers to syscall handlers and use syscall number as an index in this array. This is the approach we used in the RPi OS and exactly the same approach is used in the Linux kernel. The array of pointers to syscall handlers is called sys_call_table and is defined like this. void * const sys_call_table[__NR_syscalls] __aligned(4096) = { [0 ... __NR_syscalls - 1] = sys_ni_syscall, #include <asm/unistd.h> }; All syscalls are initially allocated to point to sys_ni_syscall function (\"ni\" here means \"non-existent\"). Syscall with number 0 and all syscalls that aren't defined for the current architecture will keep this handler. All other syscall handlers in the sys_call_table array are rewritten in the asm/unistd.h header file. As you might see, this file simply provides a mapping between syscall number and syscall handler function.","title":"Linux syscalls"},{"location":"lesson05/linux/#low-level-syscall-handling-code","text":"Ok, we've seen how sys_call_table is created and populated, now it is time to investigate how it is used by the low-level syscall handling code. And once again the basic mechanism here will be almost exactly the same as in the RPi OS. We know that any syscall is a synchronous exception and all exception handlers are defined in the exception vector table (it is our favorite vectors array). The handler that we are interested in should be the one that handles synchronous exceptions generated at EL0. All of this makes it trivial to find the right handler, it is called el0_sync and looks like the following. el0_sync: kernel_entry 0 mrs x25, esr_el1 // read the syndrome register lsr x24, x25, #ESR_ELx_EC_SHIFT // exception class cmp x24, #ESR_ELx_EC_SVC64 // SVC in 64-bit state b.eq el0_svc cmp x24, #ESR_ELx_EC_DABT_LOW // data abort in EL0 b.eq el0_da cmp x24, #ESR_ELx_EC_IABT_LOW // instruction abort in EL0 b.eq el0_ia cmp x24, #ESR_ELx_EC_FP_ASIMD // FP/ASIMD access b.eq el0_fpsimd_acc cmp x24, #ESR_ELx_EC_FP_EXC64 // FP/ASIMD exception b.eq el0_fpsimd_exc cmp x24, #ESR_ELx_EC_SYS64 // configurable trap b.eq el0_sys cmp x24, #ESR_ELx_EC_SP_ALIGN // stack alignment exception b.eq el0_sp_pc cmp x24, #ESR_ELx_EC_PC_ALIGN // pc alignment exception b.eq el0_sp_pc cmp x24, #ESR_ELx_EC_UNKNOWN // unknown exception in EL0 b.eq el0_undef cmp x24, #ESR_ELx_EC_BREAKPT_LOW // debug exception in EL0 b.ge el0_dbg b el0_inv Here esr_el1 (exception syndrome register) is used to figure out whether the current exception is a system call. If this is the case el0_svc function is called. This function is listed below. el0_svc: adrp stbl, sys_call_table // load syscall table pointer mov wscno, w8 // syscall number in w8 mov wsc_nr, #__NR_syscalls el0_svc_naked: // compat entry point stp x0, xscno, [sp, #S_ORIG_X0] // save the original x0 and syscall number enable_dbg_and_irq ct_user_exit 1 ldr x16, [tsk, #TSK_TI_FLAGS] // check for syscall hooks tst x16, #_TIF_SYSCALL_WORK b.ne __sys_trace cmp wscno, wsc_nr // check upper syscall limit b.hs ni_sys ldr x16, [stbl, xscno, lsl #3] // address in the syscall table blr x16 // call sys_* routine b ret_fast_syscall ni_sys: mov x0, sp bl do_ni_syscall b ret_fast_syscall ENDPROC(el0_svc) Now, let's examine it line by line. el0_svc: adrp stbl, sys_call_table // load syscall table pointer mov wscno, w8 // syscall number in w8 mov wsc_nr, #__NR_syscalls The first 3 lines initialize stbl , wscno and wsc_nr variables that are just aliases for some registers. stbl holds the address of the syscall table, wsc_nr contains the total number of system calls and wscno is the current syscall number from w8 register. stp x0, xscno, [sp, #S_ORIG_X0] // save the original x0 and syscall number As you might remember from our RPi OS syscall discussion, x0 is overwritten in the pt_regs area after a syscall finishes. In case when original value of the x0 register might be needed, it is saved in the separate field of the pt_regs struct. Similarly syscall number is also saved in the pt_regs . enable_dbg_and_irq Interrupts and debug exceptions are enabled. ct_user_exit 1 The event of switching from the user mode to the kernel mode is recorded. ldr x16, [tsk, #TSK_TI_FLAGS] // check for syscall hooks tst x16, #_TIF_SYSCALL_WORK b.ne __sys_trace In case the current task is executed under a syscall tracer _TIF_SYSCALL_WORK flag should be set. In this case, __sys_trace function will be called. As our discussion is only focused on the general case, we are going to skip this function. cmp wscno, wsc_nr // check upper syscall limit b.hs ni_sys If current syscall number is greater then total syscall count an error is returned to the user. ldr x16, [stbl, xscno, lsl #3] // address in the syscall table blr x16 // call sys_* routine b ret_fast_syscall Syscall number is used as an index in the syscall table array to find the handler. Then handler address is loaded into x16 register and it is executed. Finally ret_fast_syscall function is called. ret_fast_syscall: disable_irq // disable interrupts str x0, [sp, #S_X0] // returned x0 ldr x1, [tsk, #TSK_TI_FLAGS] // re-check for syscall tracing and x2, x1, #_TIF_SYSCALL_WORK cbnz x2, ret_fast_syscall_trace and x2, x1, #_TIF_WORK_MASK cbnz x2, work_pending enable_step_tsk x1, x2 kernel_exit 0 The important things here are the first line, were interrupts are disabled, and the last line, were kernel_exit is called - everything else is related to special case processing. So as you might guess this is the place where a system call actually finishes and execution is transfered to user process.","title":"Low-level syscall handling code"},{"location":"lesson05/linux/#conclusion","text":"We've now gone through the process of generating and processing a system call. This process is relatively simple, but it is vital for the OS, because it allows the kernel to set up an API and make sure that this API is the only mean of communication between a user program and the kernel.","title":"Conclusion"},{"location":"lesson05/linux/#previous-page","text":"5.1 User processes and system calls: RPi OS","title":"Previous Page"},{"location":"lesson05/linux/#next-page","text":"5.3 User processes and system calls: Exercises","title":"Next Page"},{"location":"lesson05/rpi-os/","text":"5: User processes and system calls Objectives Our kernel is evolving from an \"embedded\" kernel which often lacks user/kernel separation to a multiprogrammed kernel. Run tasks in EL0 Add the syscall mechanism Implement a few basic syscalls NOTE: this experiment enables running user/kernel at different ELs. Yet, it does not NOT give each task its own address space \u2014 we are going to tackle this issue in lesson 6! Roadmap We will: Implement the syscall mechanism, in particular switch between EL0 and EL1 (you have already done something similar in previous experiments!) Implement two mechanisms that put user tasks to EL0: moving a kernel task EL1 -> EL0 forking an existing user task at EL0 Syscall implementation Each system call is a synchronous exception. A user program prepares all necessary arguments, and then run svc instruction. Such exceptions are handled at EL1 by the kernel. The kernel validates all arguments, does the syscall, and exists from the exception. After that, the user task resumes at EL0 right after the svc instruction. We have 4 simple syscalls: write outputs to UART. It accepts a buffer with the text to be printed as the first argument. clone creates a new user thread. The location of the stack for the newly created thread is passed as the first argument. malloc allocates a memory page for a user process. There is no analog of this syscall in Linux (and I think in any other OS as well.) The only reason that we have no virtual memory yet, and all user processes work with physical memory. Each process needs a way to figure out which memory page can be used. malloc syscall return pointer to the newly allocated page or -1 in case of an error. exit Each process must call this syscall after it finishes execution. It will do cleanup. All syscalls are defined in the sys.c file. There is also an array sys_call_table that contains pointers to all syscall handlers. Each syscall has a \"syscall number\" \u2014 this is just an index in the sys_call_table array. All syscall numbers are defined here \u2014 they are used by the assembler code to look up syscall. Let's use write syscall as an example: //sys.S .globl call_sys_write call_sys_write: mov w8, #SYS_WRITE_NUMBER svc #0 ret Simple -- the wrapper stores the syscall number in the w8 register and does svc . Convention: registers x0 \u2014 x7 are used for syscall arguments and x8 is used to store syscall number. This allows a syscall to have up to 8 arguments. In commodity OSes, such wrapper functions are usually in user library such as glibc but not in the kernel. Switching between EL0 and EL1 We need this new mechanism. It's in the same spirit as we move from EL2/3 to EL1. (Recall: how did we do it?) Previously, our kernel may take interrupt EL1. Now, it has to take exception (svc) at EL0. To accommodate this, both kernel_entry and kernel_exit macros accepts an additional argument el , indicating which exception level an exception is taken from. The information is required to properly save/restore stack pointer. Here are the two relevant parts from the kernel_entry and kernel_exit macros. .if \\el == 0 mrs x21, sp_el0 .else add x21, sp, #S_FRAME_SIZE .endif /* \\el == 0 */ .if \\el == 0 msr sp_el0, x21 .endif /* \\el == 0 */ We are using 2 distinct stack pointers (SP) for EL0 and EL1. This is a common design because we want to separate user/kernel. Supported by CPU hardware, after taking an exception from EL0 to EL1, the CPU is using the SP for EL1. The SP for EL0 can be found in the sp_el0 register. The value of this register must be stored and restored upon entering/exiting the kernel, even if the kernel does not sp_el0 in the exception handler. Reason: we need to virtualize sp_el0 for each task because each task has its own user stack. Try to visualize this in your mind. When we do kernel_exit , how do we specify which EL to return to, EL0 or EL1? We don't have to explicitly specify so in eret . This EL level is encoded in the spsr_el1 register that was saved, e.g. when syscall enters the kernel. So we always return to the level from which the exception was taken. How did we treat SP when taking interrupts (from EL1)? Revisit the figures in previous experiments. Handling synchronous exceptions In the exception table, el0_sync is registered as the handler for sync exception taken at EL0. // entry.S el0_sync: kernel_entry 0 mrs x25, esr_el1 // read the syndrome register lsr x24, x25, #ESR_ELx_EC_SHIFT // exception class cmp x24, #ESR_ELx_EC_SVC64 // SVC in 64-bit state b.eq el0_svc handle_invalid_entry 0, SYNC_ERROR As for all exception handlers, kernel_entry macro is called. esr_el1 (Exception Syndrome Register) is checked. This register contains \"exception class\" field at offset ESR_ELx_EC_SHIFT . If exception class is equal to ESR_ELx_EC_SVC64 this means that the current exception is caused by the svc instruction and it is a system call. In this case, we jump to el0_svc label and show an error message otherwise. sc_nr .req x25 // number of system calls scno .req x26 // syscall number stbl .req x27 // syscall table pointer el0_svc: adr stbl, sys_call_table // load syscall table pointer uxtw scno, w8 // syscall number in w8 mov sc_nr, #__NR_syscalls bl enable_irq cmp scno, sc_nr // check upper syscall limit b.hs ni_sys ldr x16, [stbl, scno, lsl #3] // address in the syscall table blr x16 // call sys_* routine b ret_from_syscall ni_sys: handle_invalid_entry 0, SYSCALL_ERROR el0_svc first loads the address of the syscall table in the stbl (it is just an alias to the x27 register.) and syscall number in the scno variable. Then interrupts are enabled and syscall number is compared to the total number of syscalls in the system \u2014 if it is greater or equal an error message is shown. If syscall number falls within the required range, it is used as an index in the syscall table array to obtain a pointer to the syscall handler. Next, the handler is executed and after it finishes ret_from_syscall is called. Note, that we don't touch here registers x0 \u2013 x7 \u2014 they are transparently passed to the handler. (Why?) Fast forward to the completion of syscall. ret_from_syscall: bl disable_irq str x0, [sp, #S_X0] // returned x0 kernel_exit 0 ret_from_syscall first disables interrupts. Then it saves the value of x0 register on the stack. This is required because kernel_exit will restore all general purpose registers from their saved values, but x0 now contains return value of the syscall handler and we want this value to be passed to the user code. Finally kernel_exit is called, which returns to the user code. Executing a task in user mode Atop that, the kernel implements two complementary ways for launching a user process. Overview: Method 1: Moving a kernel task to EL0 Overview: upon its creation, the kernel task calls its main function, kernel_process() , which calls move_to_user_mode() . The later prepares CPU context for exiting to EL0. Then kernel_process() returns to ret_from_fork which invokes the familiar kernel_exit . Eventually, an eret instruction - boom! We land in EL0. Code walkthrough First create a process (i.e. a task) as we did before. This is a \"kernel\" process to execute at EL1. // kernel.c int res = copy_process(PF_KTHREAD, (unsigned long)&kernel_process, 0, 0); The kernel process invokes move_to_user_mode() , passing a function pointer to the user_process as the first argument. void kernel_process(){ printf(\"Kernel process started. EL %d\\r\\n\", get_el()); int err = move_to_user_mode((unsigned long)&user_process); Now let's see what move_to_user_mode function is doing. int move_to_user_mode(unsigned long pc) { struct pt_regs *regs = task_pt_regs(current); memzero((unsigned long)regs, sizeof(*regs)); regs->pc = pc; regs->pstate = PSR_MODE_EL0t; unsigned long stack = get_free_page(); //allocate new user stack if (!stack) { return -1; } regs->sp = stack + PAGE_SIZE; current->stack = stack; return 0; } pt_regs: the exception stack frame In the previous experiment: when an interrupt happens, kernel_entry saves CPU context to a stack frame marked as \"saved regs\", which is somewhere in the middle of a kernel task's stack. In this experiment, our kernel will additionally handle sync exceptions (syscalls). When a syscall happens, the CPU will create a stack frame in the same format called pt_regs . The name comes from Linux again. When syscall returns, the kernel unwinds pt_regs . For the first time return to EL0, move_to_user_mode() sets up pt_regs : pt_regs.pc This is the first instruction to be executed by the task once it lands in user mode via eret . pstate . This specifies the CPU state for the task. Later, kernel_exit copies this field to spsr_el1 . eret restores the CPU state from pstate . PSR_MODE_EL0t constant specifies that we will go to EL0. See manual . Furthermore, move_to_user_mode allocates a new page for the user stack and sets sp field to point to the page top. Where is pt_regs? It is at the top of the stack. See the figure above. Right before kernel_exit() , the task's stack is unwound just to the beginning of pt_regs . Therefore, kernel_exit() will restore CPU regs from the stack. task_pt_regs() calculates the address of a task's pt_regs . See the code below which is self-evident. Recall that THREAD_SIZE == 4KB which is the memory size for the task. struct pt_regs * task_pt_regs(struct task_struct *tsk){ unsigned long p = (unsigned long)tsk + THREAD_SIZE - sizeof(struct pt_regs); return (struct pt_regs *)p; } Addition to ret_from_fork() As you might notice ret_from_fork has been updated. This happens in the middle of the ret_from_fork function. .globl ret_from_fork ret_from_fork: bl schedule_tail cbz x19, ret_to_user // not a kernel thread mov x0, x20 blr x19 ret_to_user: bl disable_irq kernel_exit 0 Now, after a kernel thread finishes, the execution goes to the ret_to_user label, here we disable interrupts and perform normal exception return, using previously prepared processor state. If you get confused, revisit the \"overview\" figure above. Method 2: Forking user processes The clone syscall, named call_sys_clone() in our kernel, forks user task. For instance, user_process function calls call_sys_clone to spawn two user tasks. // sys.S .globl call_sys_clone call_sys_clone: /* Save args for the child. They will be preserved throughout syscall */ mov x10, x0 /*fn*/ mov x11, x1 /*arg*/ mov x12, x2 /*stack*/ /* Prep syscall args. Do the system call. */ mov x0, x2 /* stack */ mov x8, #SYS_CLONE_NUMBER svc 0x0 cmp x0, #0 beq thread_start ret thread_start: mov x29, 0 /* Pick the function arg and execute. */ mov x0, x11 blr x10 /* We are done, pass the return value through x0. */ mov x8, #SYS_EXIT_NUMBER svc 0x0 The clone wrapper above mimics the coresponding function from in the glibc library. This function does the following. x0-x3 contain syscall arguments. They are intended for the child task. Save them to x10-x11. Why? The new task will pick them in thread_start . (NB: our syscall handler el0_svc does not save x0-x3. Read on.) Put the stack point for the new task in x0, as expected by sys_clone(unsigned long stack) . Starts syscall via svc . Do syscall ... Upon returning from syscall, checks return value in x0: if 0, we are executing inside of the newly created task. In this case, execution goes to thread_start label. If not 0, x0 is the PID of the new task. This means that we are executing inside the original thread \u2014 just return to the caller in this case. thread_start executes in the new task with the give entry function (x10) and the arg to the function (x11). Note x29 (FP) is cleared for correct stack unwinding. See here . After the function finishes, exit syscall is performed \u2014 it never returns. Implementing clone in kernel Clone syscall handler can be found here . It is very simple and it just calls already familiar copy_process function. This function, however, has been modified since the last lesson \u2014 now it supports cloning user threads as well as kernel threads. The source of the function is listed below. int copy_process(unsigned long clone_flags, unsigned long fn, unsigned long arg, unsigned long stack) { preempt_disable(); struct task_struct *p; p = (struct task_struct *) get_free_page(); if (!p) { return -1; } struct pt_regs *childregs = task_pt_regs(p); memzero((unsigned long)childregs, sizeof(struct pt_regs)); memzero((unsigned long)&p->cpu_context, sizeof(struct cpu_context)); // new addition if (clone_flags & PF_KTHREAD) { p->cpu_context.x19 = fn; p->cpu_context.x20 = arg; } else { struct pt_regs * cur_regs = task_pt_regs(current); *childregs = *cur_regs; childregs->regs[0] = 0; childregs->sp = stack + PAGE_SIZE; p->stack = stack; } p->flags = clone_flags; p->priority = current->priority; p->state = TASK_RUNNING; p->counter = p->priority; p->preempt_count = 1; //disable preemtion until schedule_tail p->cpu_context.pc = (unsigned long)ret_from_fork; p->cpu_context.sp = (unsigned long)childregs; int pid = nr_tasks++; task[pid] = p; preempt_enable(); return pid; } In case, when we are creating a new kernel thread, the function behaves exactly the same, as described in the previous lesson. In the other case, when we are cloning a user thread, this part of the code is executed. struct pt_regs * cur_regs = task_pt_regs(current); *childregs = *cur_regs; childregs->regs[0] = 0; childregs->sp = stack + PAGE_SIZE; p->stack = stack; We populate the CPU context, i.e. pt_regs , for the new task. Note that pt_regs is always at the top of the stack page, because when syscall enters/exits the kernel, the kernel stack is empty. x0 in the new state is set to 0 , because x0 will be interpreted by the caller as a return value of the syscall. We've just seen how clone wrapper function uses this value to determine whether we are still executing as a part of the original thread or a new one. Next sp for the new task is set to point to the top of the new user stack page. We also save the pointer to the stack page in order to do a cleanup after the task finishes. Exiting a task After each user tasks finishes it should call exit syscall (In the current implementation exit is called implicitly by the clone wrapper function.). exit syscall then calls exit_process function, which is responsible for deactivating a task. The function is listed below. void exit_process(){ preempt_disable(); for (int i = 0; i < NR_TASKS; i++){ if (task[i] == current) { task[i]->state = TASK_ZOMBIE; break; } } if (current->stack) { free_page(current->stack); } preempt_enable(); schedule(); } Following Linux convention, we are not deleting the task at once but set its state to TASK_ZOMBIE instead. This prevents the task from being selected and executed by the scheduler. In Linux such approach is used to allow parent process to query information about the child even after it finishes. exit_process also deletes now unnecessary user stack and calls schedule . After schedule is called new task will be selected, that's why this system call never returns. Conclusion Now that the RPi OS can manage user tasks, we become much closer to the full process isolation. But one important step is still missing: all user tasks share the same physical memory and can easily read one another's data. In the next lesson, we are going to introduce virtual memory and fix this issue. Previous Page 4.5 Process scheduler: Exercises Next Page 5.2 User processes and system calls: Linux","title":"5: User processes and system calls"},{"location":"lesson05/rpi-os/#5-user-processes-and-system-calls","text":"","title":"5: User processes and system calls"},{"location":"lesson05/rpi-os/#objectives","text":"Our kernel is evolving from an \"embedded\" kernel which often lacks user/kernel separation to a multiprogrammed kernel. Run tasks in EL0 Add the syscall mechanism Implement a few basic syscalls NOTE: this experiment enables running user/kernel at different ELs. Yet, it does not NOT give each task its own address space \u2014 we are going to tackle this issue in lesson 6!","title":"Objectives"},{"location":"lesson05/rpi-os/#roadmap","text":"We will: Implement the syscall mechanism, in particular switch between EL0 and EL1 (you have already done something similar in previous experiments!) Implement two mechanisms that put user tasks to EL0: moving a kernel task EL1 -> EL0 forking an existing user task at EL0","title":"Roadmap"},{"location":"lesson05/rpi-os/#syscall-implementation","text":"Each system call is a synchronous exception. A user program prepares all necessary arguments, and then run svc instruction. Such exceptions are handled at EL1 by the kernel. The kernel validates all arguments, does the syscall, and exists from the exception. After that, the user task resumes at EL0 right after the svc instruction. We have 4 simple syscalls: write outputs to UART. It accepts a buffer with the text to be printed as the first argument. clone creates a new user thread. The location of the stack for the newly created thread is passed as the first argument. malloc allocates a memory page for a user process. There is no analog of this syscall in Linux (and I think in any other OS as well.) The only reason that we have no virtual memory yet, and all user processes work with physical memory. Each process needs a way to figure out which memory page can be used. malloc syscall return pointer to the newly allocated page or -1 in case of an error. exit Each process must call this syscall after it finishes execution. It will do cleanup. All syscalls are defined in the sys.c file. There is also an array sys_call_table that contains pointers to all syscall handlers. Each syscall has a \"syscall number\" \u2014 this is just an index in the sys_call_table array. All syscall numbers are defined here \u2014 they are used by the assembler code to look up syscall. Let's use write syscall as an example: //sys.S .globl call_sys_write call_sys_write: mov w8, #SYS_WRITE_NUMBER svc #0 ret Simple -- the wrapper stores the syscall number in the w8 register and does svc . Convention: registers x0 \u2014 x7 are used for syscall arguments and x8 is used to store syscall number. This allows a syscall to have up to 8 arguments. In commodity OSes, such wrapper functions are usually in user library such as glibc but not in the kernel.","title":"Syscall implementation"},{"location":"lesson05/rpi-os/#switching-between-el0-and-el1","text":"We need this new mechanism. It's in the same spirit as we move from EL2/3 to EL1. (Recall: how did we do it?) Previously, our kernel may take interrupt EL1. Now, it has to take exception (svc) at EL0. To accommodate this, both kernel_entry and kernel_exit macros accepts an additional argument el , indicating which exception level an exception is taken from. The information is required to properly save/restore stack pointer. Here are the two relevant parts from the kernel_entry and kernel_exit macros. .if \\el == 0 mrs x21, sp_el0 .else add x21, sp, #S_FRAME_SIZE .endif /* \\el == 0 */ .if \\el == 0 msr sp_el0, x21 .endif /* \\el == 0 */ We are using 2 distinct stack pointers (SP) for EL0 and EL1. This is a common design because we want to separate user/kernel. Supported by CPU hardware, after taking an exception from EL0 to EL1, the CPU is using the SP for EL1. The SP for EL0 can be found in the sp_el0 register. The value of this register must be stored and restored upon entering/exiting the kernel, even if the kernel does not sp_el0 in the exception handler. Reason: we need to virtualize sp_el0 for each task because each task has its own user stack. Try to visualize this in your mind. When we do kernel_exit , how do we specify which EL to return to, EL0 or EL1? We don't have to explicitly specify so in eret . This EL level is encoded in the spsr_el1 register that was saved, e.g. when syscall enters the kernel. So we always return to the level from which the exception was taken. How did we treat SP when taking interrupts (from EL1)? Revisit the figures in previous experiments.","title":"Switching between EL0 and EL1"},{"location":"lesson05/rpi-os/#handling-synchronous-exceptions","text":"In the exception table, el0_sync is registered as the handler for sync exception taken at EL0. // entry.S el0_sync: kernel_entry 0 mrs x25, esr_el1 // read the syndrome register lsr x24, x25, #ESR_ELx_EC_SHIFT // exception class cmp x24, #ESR_ELx_EC_SVC64 // SVC in 64-bit state b.eq el0_svc handle_invalid_entry 0, SYNC_ERROR As for all exception handlers, kernel_entry macro is called. esr_el1 (Exception Syndrome Register) is checked. This register contains \"exception class\" field at offset ESR_ELx_EC_SHIFT . If exception class is equal to ESR_ELx_EC_SVC64 this means that the current exception is caused by the svc instruction and it is a system call. In this case, we jump to el0_svc label and show an error message otherwise. sc_nr .req x25 // number of system calls scno .req x26 // syscall number stbl .req x27 // syscall table pointer el0_svc: adr stbl, sys_call_table // load syscall table pointer uxtw scno, w8 // syscall number in w8 mov sc_nr, #__NR_syscalls bl enable_irq cmp scno, sc_nr // check upper syscall limit b.hs ni_sys ldr x16, [stbl, scno, lsl #3] // address in the syscall table blr x16 // call sys_* routine b ret_from_syscall ni_sys: handle_invalid_entry 0, SYSCALL_ERROR el0_svc first loads the address of the syscall table in the stbl (it is just an alias to the x27 register.) and syscall number in the scno variable. Then interrupts are enabled and syscall number is compared to the total number of syscalls in the system \u2014 if it is greater or equal an error message is shown. If syscall number falls within the required range, it is used as an index in the syscall table array to obtain a pointer to the syscall handler. Next, the handler is executed and after it finishes ret_from_syscall is called. Note, that we don't touch here registers x0 \u2013 x7 \u2014 they are transparently passed to the handler. (Why?) Fast forward to the completion of syscall. ret_from_syscall: bl disable_irq str x0, [sp, #S_X0] // returned x0 kernel_exit 0 ret_from_syscall first disables interrupts. Then it saves the value of x0 register on the stack. This is required because kernel_exit will restore all general purpose registers from their saved values, but x0 now contains return value of the syscall handler and we want this value to be passed to the user code. Finally kernel_exit is called, which returns to the user code.","title":"Handling synchronous exceptions"},{"location":"lesson05/rpi-os/#executing-a-task-in-user-mode","text":"Atop that, the kernel implements two complementary ways for launching a user process. Overview:","title":"Executing a task in user mode"},{"location":"lesson05/rpi-os/#method-1-moving-a-kernel-task-to-el0","text":"Overview: upon its creation, the kernel task calls its main function, kernel_process() , which calls move_to_user_mode() . The later prepares CPU context for exiting to EL0. Then kernel_process() returns to ret_from_fork which invokes the familiar kernel_exit . Eventually, an eret instruction - boom! We land in EL0. Code walkthrough First create a process (i.e. a task) as we did before. This is a \"kernel\" process to execute at EL1. // kernel.c int res = copy_process(PF_KTHREAD, (unsigned long)&kernel_process, 0, 0); The kernel process invokes move_to_user_mode() , passing a function pointer to the user_process as the first argument. void kernel_process(){ printf(\"Kernel process started. EL %d\\r\\n\", get_el()); int err = move_to_user_mode((unsigned long)&user_process); Now let's see what move_to_user_mode function is doing. int move_to_user_mode(unsigned long pc) { struct pt_regs *regs = task_pt_regs(current); memzero((unsigned long)regs, sizeof(*regs)); regs->pc = pc; regs->pstate = PSR_MODE_EL0t; unsigned long stack = get_free_page(); //allocate new user stack if (!stack) { return -1; } regs->sp = stack + PAGE_SIZE; current->stack = stack; return 0; } pt_regs: the exception stack frame In the previous experiment: when an interrupt happens, kernel_entry saves CPU context to a stack frame marked as \"saved regs\", which is somewhere in the middle of a kernel task's stack. In this experiment, our kernel will additionally handle sync exceptions (syscalls). When a syscall happens, the CPU will create a stack frame in the same format called pt_regs . The name comes from Linux again. When syscall returns, the kernel unwinds pt_regs . For the first time return to EL0, move_to_user_mode() sets up pt_regs : pt_regs.pc This is the first instruction to be executed by the task once it lands in user mode via eret . pstate . This specifies the CPU state for the task. Later, kernel_exit copies this field to spsr_el1 . eret restores the CPU state from pstate . PSR_MODE_EL0t constant specifies that we will go to EL0. See manual . Furthermore, move_to_user_mode allocates a new page for the user stack and sets sp field to point to the page top. Where is pt_regs? It is at the top of the stack. See the figure above. Right before kernel_exit() , the task's stack is unwound just to the beginning of pt_regs . Therefore, kernel_exit() will restore CPU regs from the stack. task_pt_regs() calculates the address of a task's pt_regs . See the code below which is self-evident. Recall that THREAD_SIZE == 4KB which is the memory size for the task. struct pt_regs * task_pt_regs(struct task_struct *tsk){ unsigned long p = (unsigned long)tsk + THREAD_SIZE - sizeof(struct pt_regs); return (struct pt_regs *)p; } Addition to ret_from_fork() As you might notice ret_from_fork has been updated. This happens in the middle of the ret_from_fork function. .globl ret_from_fork ret_from_fork: bl schedule_tail cbz x19, ret_to_user // not a kernel thread mov x0, x20 blr x19 ret_to_user: bl disable_irq kernel_exit 0 Now, after a kernel thread finishes, the execution goes to the ret_to_user label, here we disable interrupts and perform normal exception return, using previously prepared processor state. If you get confused, revisit the \"overview\" figure above.","title":"Method 1: Moving a kernel task to EL0"},{"location":"lesson05/rpi-os/#method-2-forking-user-processes","text":"The clone syscall, named call_sys_clone() in our kernel, forks user task. For instance, user_process function calls call_sys_clone to spawn two user tasks. // sys.S .globl call_sys_clone call_sys_clone: /* Save args for the child. They will be preserved throughout syscall */ mov x10, x0 /*fn*/ mov x11, x1 /*arg*/ mov x12, x2 /*stack*/ /* Prep syscall args. Do the system call. */ mov x0, x2 /* stack */ mov x8, #SYS_CLONE_NUMBER svc 0x0 cmp x0, #0 beq thread_start ret thread_start: mov x29, 0 /* Pick the function arg and execute. */ mov x0, x11 blr x10 /* We are done, pass the return value through x0. */ mov x8, #SYS_EXIT_NUMBER svc 0x0 The clone wrapper above mimics the coresponding function from in the glibc library. This function does the following. x0-x3 contain syscall arguments. They are intended for the child task. Save them to x10-x11. Why? The new task will pick them in thread_start . (NB: our syscall handler el0_svc does not save x0-x3. Read on.) Put the stack point for the new task in x0, as expected by sys_clone(unsigned long stack) . Starts syscall via svc . Do syscall ... Upon returning from syscall, checks return value in x0: if 0, we are executing inside of the newly created task. In this case, execution goes to thread_start label. If not 0, x0 is the PID of the new task. This means that we are executing inside the original thread \u2014 just return to the caller in this case. thread_start executes in the new task with the give entry function (x10) and the arg to the function (x11). Note x29 (FP) is cleared for correct stack unwinding. See here . After the function finishes, exit syscall is performed \u2014 it never returns.","title":"Method 2: Forking user processes"},{"location":"lesson05/rpi-os/#implementing-clone-in-kernel","text":"Clone syscall handler can be found here . It is very simple and it just calls already familiar copy_process function. This function, however, has been modified since the last lesson \u2014 now it supports cloning user threads as well as kernel threads. The source of the function is listed below. int copy_process(unsigned long clone_flags, unsigned long fn, unsigned long arg, unsigned long stack) { preempt_disable(); struct task_struct *p; p = (struct task_struct *) get_free_page(); if (!p) { return -1; } struct pt_regs *childregs = task_pt_regs(p); memzero((unsigned long)childregs, sizeof(struct pt_regs)); memzero((unsigned long)&p->cpu_context, sizeof(struct cpu_context)); // new addition if (clone_flags & PF_KTHREAD) { p->cpu_context.x19 = fn; p->cpu_context.x20 = arg; } else { struct pt_regs * cur_regs = task_pt_regs(current); *childregs = *cur_regs; childregs->regs[0] = 0; childregs->sp = stack + PAGE_SIZE; p->stack = stack; } p->flags = clone_flags; p->priority = current->priority; p->state = TASK_RUNNING; p->counter = p->priority; p->preempt_count = 1; //disable preemtion until schedule_tail p->cpu_context.pc = (unsigned long)ret_from_fork; p->cpu_context.sp = (unsigned long)childregs; int pid = nr_tasks++; task[pid] = p; preempt_enable(); return pid; } In case, when we are creating a new kernel thread, the function behaves exactly the same, as described in the previous lesson. In the other case, when we are cloning a user thread, this part of the code is executed. struct pt_regs * cur_regs = task_pt_regs(current); *childregs = *cur_regs; childregs->regs[0] = 0; childregs->sp = stack + PAGE_SIZE; p->stack = stack; We populate the CPU context, i.e. pt_regs , for the new task. Note that pt_regs is always at the top of the stack page, because when syscall enters/exits the kernel, the kernel stack is empty. x0 in the new state is set to 0 , because x0 will be interpreted by the caller as a return value of the syscall. We've just seen how clone wrapper function uses this value to determine whether we are still executing as a part of the original thread or a new one. Next sp for the new task is set to point to the top of the new user stack page. We also save the pointer to the stack page in order to do a cleanup after the task finishes.","title":"Implementing clone in kernel"},{"location":"lesson05/rpi-os/#exiting-a-task","text":"After each user tasks finishes it should call exit syscall (In the current implementation exit is called implicitly by the clone wrapper function.). exit syscall then calls exit_process function, which is responsible for deactivating a task. The function is listed below. void exit_process(){ preempt_disable(); for (int i = 0; i < NR_TASKS; i++){ if (task[i] == current) { task[i]->state = TASK_ZOMBIE; break; } } if (current->stack) { free_page(current->stack); } preempt_enable(); schedule(); } Following Linux convention, we are not deleting the task at once but set its state to TASK_ZOMBIE instead. This prevents the task from being selected and executed by the scheduler. In Linux such approach is used to allow parent process to query information about the child even after it finishes. exit_process also deletes now unnecessary user stack and calls schedule . After schedule is called new task will be selected, that's why this system call never returns.","title":"Exiting a task"},{"location":"lesson05/rpi-os/#conclusion","text":"Now that the RPi OS can manage user tasks, we become much closer to the full process isolation. But one important step is still missing: all user tasks share the same physical memory and can easily read one another's data. In the next lesson, we are going to introduce virtual memory and fix this issue. Previous Page 4.5 Process scheduler: Exercises Next Page 5.2 User processes and system calls: Linux","title":"Conclusion"},{"location":"lesson06/exercises/","text":"Exercise: make the ARM generic timer work Problem symptom Build and run the given code on the Rpi3 hardware , it works as expected: kernel boots ... Kernel process started. EL 1 User process 123451234512345abcdeabcdeabcdeabcd12345123451234eabcdeabcdeabcdeab512345123451234cdeabcdeabcdeabcde51234512345123abcdeabcdeabcdeabc451234512345123deabcdeabcdeabcdea45123451234512bcdeabcdeabcdeabcd345123451234512eabcdeabcdeabcdeab34512345123451cdeabcdeabcdeabcde234512345123451abcdeabcdeabcdeabc23451234512345deabcdea Now build and run it on QEMU . What have you observed? VNC server running on 127.0.0.1:5900 kernel boots ... Kernel process started. EL 1 User process 12345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123 Problem cause Why is our kernel not preempting user tasks on QEMU? The kernel's preemptive scheduling is driven by timer interrupts. Turns out that the given kernel source code only includes a driver ( timer.c ) for Rpi3's \"system timer\" (which we briefly mentioned in experiment 3 when introducing interrupts). However, QEMU does NOT emulate the system timer; it emulates the ARM generic timer , which we have been using since experiment 3. Fix it! We will make the kernel (which implements user-level process support) work with the ARM generic timer. If successful, our kernel should be able to preempt user tasks on QEMU; it will work as usual on the Rpi3 hardware, i.e. using the system timer as scheduling ticks. The first step is to port the driver for the generic timer from our previous kernel versions to this one. This is easy -- mostly copy & paste, update some macros, etc. The challenge is the memory mapping for registers of the generic timer. In previous experiments, this was not a problem because MMU was off and our kernel uses physical memory. In this experiment, we turned on MMU so the kernel must establish memory mapping for IO registers before accessing them. Check out the table \"Other timers on Rpi3\" in experiment 3: the registers for the generic timers are above address 0x40000040. Unfortunately, our current kernel only maps up to 1GB (0x40000000) physical memory. Additional mapping is needed for these registers. Now you will have to figure out how to allocate and populate the needed additional pgtable and eventually get everything work. Deliverable A code tarball implementing the task above.","title":"Exercises"},{"location":"lesson06/exercises/#exercise-make-the-arm-generic-timer-work","text":"","title":"Exercise: make the ARM generic timer work"},{"location":"lesson06/exercises/#problem-symptom","text":"Build and run the given code on the Rpi3 hardware , it works as expected: kernel boots ... Kernel process started. EL 1 User process 123451234512345abcdeabcdeabcdeabcd12345123451234eabcdeabcdeabcdeab512345123451234cdeabcdeabcdeabcde51234512345123abcdeabcdeabcdeabc451234512345123deabcdeabcdeabcdea45123451234512bcdeabcdeabcdeabcd345123451234512eabcdeabcdeabcdeab34512345123451cdeabcdeabcdeabcde234512345123451abcdeabcdeabcdeabc23451234512345deabcdea Now build and run it on QEMU . What have you observed? VNC server running on 127.0.0.1:5900 kernel boots ... Kernel process started. EL 1 User process 12345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123451234512345123","title":"Problem symptom"},{"location":"lesson06/exercises/#problem-cause","text":"Why is our kernel not preempting user tasks on QEMU? The kernel's preemptive scheduling is driven by timer interrupts. Turns out that the given kernel source code only includes a driver ( timer.c ) for Rpi3's \"system timer\" (which we briefly mentioned in experiment 3 when introducing interrupts). However, QEMU does NOT emulate the system timer; it emulates the ARM generic timer , which we have been using since experiment 3.","title":"Problem cause"},{"location":"lesson06/exercises/#fix-it","text":"We will make the kernel (which implements user-level process support) work with the ARM generic timer. If successful, our kernel should be able to preempt user tasks on QEMU; it will work as usual on the Rpi3 hardware, i.e. using the system timer as scheduling ticks. The first step is to port the driver for the generic timer from our previous kernel versions to this one. This is easy -- mostly copy & paste, update some macros, etc. The challenge is the memory mapping for registers of the generic timer. In previous experiments, this was not a problem because MMU was off and our kernel uses physical memory. In this experiment, we turned on MMU so the kernel must establish memory mapping for IO registers before accessing them. Check out the table \"Other timers on Rpi3\" in experiment 3: the registers for the generic timers are above address 0x40000040. Unfortunately, our current kernel only maps up to 1GB (0x40000000) physical memory. Additional mapping is needed for these registers. Now you will have to figure out how to allocate and populate the needed additional pgtable and eventually get everything work.","title":"Fix it!"},{"location":"lesson06/exercises/#deliverable","text":"A code tarball implementing the task above.","title":"Deliverable"},{"location":"lesson06/rpi-os/","text":"6: Virtual memory (VM) Objectives Make our tiny kernel capable of: enforcing separate virtual address spaces, and user-level demand paging. Roadmap Prior to this experiment, our kernel can run and schedule user processes, but the isolation between them is not complete - all processes and the kernel itself share the same memory. This allows any process to easily access somebody else's data and even kernel data. And even if we assume that all our processes are not malicious, there is another drawback: before allocating memory each process need to know which memory regions are already occupied - this makes memory allocation for a process more complicated. We will take the following steps. Set up a pgtable for kernel. Using linear mapping. Turn on MMU shortly after kernel boots. This is a common kernel design. Set up pgtables for user processes Implement fork() for user processes Implement demand paging Background: ARM64 translation process Page table format This experiment introduces VM to our kernel. With VM, we can formally call tasks \"processes\". Each task will have its own address space. They issue memory access with virtual addresses. The MMU transparently translates virtual addresses to physical addresses. The MMU uses page table (or pgtable, or \"translation table\" in ARM's manual). The following diagram summarizes ARM64 address translation with uses 4-level pgtables. Virtual address Physical Memory +-----------------------------------------------------------------------+ +-----------------_+ | | PGD Index | PUD Index | PMD Index | PTE Index | Page offset | | | +-----------------------------------------------------------------------+ | | 63 47 | 38 | 29 | 20 | 11 | 0 | Page N | | | | | +--------------------+ +---->+------------------+ | | | +---------------------+ | | | | +------+ | | | | | | | | | +----------+ | | | |------------------| +------+ | PGD | | | +---------------->| Physical address | | ttbr |---->+-------------+ | PUD | | | |------------------| +------+ | | | | +->+-------------+ | PMD | | | | | +-------------+ | | | | | +->+-------------+ | PTE | +------------------+ +->| PUD address |----+ +-------------+ | | | | | +->+--------------+ | | | +-------------+ +--->| PMD address |----+ +-------------+ | | | | | | | | | +-------------+ +--->| PTE address |----+ +-------------_+ | | | +-------------+ | | +-------------+ +--->| Page address |----+ | | +-------------+ | | +--------------+ | | +-------------+ | | | | +--------------+ +------------------+ Notable points: Page tables have a hierarchical structure, i.e. a tree. An item in any of the tables contains an address of the next table in the hierarchy. Note: strictly speaking a pgtable means a contiguous array of entries at any of the four levels. So a tree has many pgtables. Some documents casually use \"pgtable\" to refer to an entire pgtable tree. Be careful. There are 4 levels in the table hierarchy: PGD (Page Global Directory), PUD (Page Upper Directory), PMD (Page Middle Directory), PTE (Page Table Entry). PTE is the last table in the hierarchy and it points to the actual page in the physical memory. Don't read too much into the terms, which just represent lv1, 2, ... pgtables. Them terms come from the Linux kernel (I guess x86), not ARM64. Over years, they became common lingo among kernel hackers. Besides holding a physical address, each pgtable item holds extra bits crucial for translation. Will examine the format below. MMU starts memory translation process by locating the base address of PGD. MMU locates the base address from the ttbr0_el1 register which should be set by the kernel. ttbr=translation table base register. MMU walks the pgtable tree to look up the physical address. A virtual address uses only 48 out of 64 available bits. When doing a translation, MMU splits an address into 4 parts: 9 bits [39 - 47] contain an index in the PGD table. MMU uses this index to find the location of the PUD. 9 bits [30 - 38] contain an index in the PUD table. MMU uses this index to find the location of the PMD. 9 bits [21 - 29] contain an index in the PMD table. MMU uses this index to find the location of the PTE. 9 bits [12 - 20] contain an index in the PTE table. MMU uses this index to find a page in the physical memory. Bits [0 - 11] contain an offset in the physical page. MMU uses this offset to determine the exact position in the previously found page that corresponds to the original virtual address. Each process has its own address space. Therefore, it has its own copy of page table tree, starting from PGD. Therefore, the kernel keeps a separate PGD base address for each process. That is, the kernel virtualizes PGD for processes. During a context switch, the kernel loads the PGD base of the next process to ttbr0_el1 . Memory for a user process is always allocated in pages. A page is a contiguous memory region 4KB in size (ARM processors support larger pages, but 4KB is the most common case and we are going to limit our discussion only to this page size). Exercise: how large is a page table? From the diagram above we know that index in a page table occupies 9 bits (this is true for all page table levels). This means that each page table contains 2^9 = 512 items. Each item in a page table is an address of either the next page table in the hierarchy or a physical page in case of PTE. As we are using a 64-bit processor, each address must be 64 bit or 8 bytes in size. This means that each pgtable is 512 * 8 = 4096 bytes or 4 KB. A pgtable is exactly a page! This might give you an intuition why MMU designers chose such numbers. Section (2MB) mapping This is specific to ARM64 for mapping large parts of continuous physical memory. In this case, instead of 4 KB pages, we can directly map 2 MB blocks that are called sections. This allows to eliminate one single level of translation. The translation diagram, in this case, looks like the following. Virtual address Physical Memory +-----------------------------------------------------------------------+ +-----------------_+ | | PGD Index | PUD Index | PMD Index | Section offset | | | +-----------------------------------------------------------------------+ | | 63 47 | 38 | 29 | 20 | 0 | Section N | | | | | +---->+------------------+ | | | | | | | +------+ | | | | | | | | +----------+ | | |------------------| +------+ | PGD | | +------------------------->| Physical address | | ttbr |---->+-------------+ | PUD | | |------------------| +------+ | | | | +->+-------------+ | PMD | | | | +-------------+ | | | | | +->+-----------------+ | +------------------+ +->| PUD address |----+ +-------------+ | | | | | | | +-------------+ +--->| PMD address |----+ +-----------------+ | | | | | +-------------+ +--->| Section address |-----+ | | +-------------+ | | +-----------------+ | | +-------------+ | | | | +-----------------+ | | +------------------+ As you can see the difference here is that now PMD contains a pointer to the physical section. Also, the offset occupies 21 bits instead of 12 bits (this is because we need 21 bits to encode a 2MB range) Page descriptor format An item in a page table is called \"descriptor\". A description has a special format as mandated by MMU hardware. A descriptor contains an address of either next page table or a physical page. The key thing to understand : each descriptor always points to something that is page-aligned (either a physical page, a section or the next page table in the hierarchy). This means that last 12 bits of the address, stored in a descriptor, will always be 0. MMU uses those bits to store additional information (\"attributes\") for translation. Descriptor format `+------------------------------------------------------------------------------------------+ | Upper attributes | Address (bits 47:12) | Lower attributes | Block/table bit | Valid bit | +------------------------------------------------------------------------------------------+ 63 47 11 2 1 0 Bit 0 This bit must be set to 1 for all valid descriptors. If MMU encounter non-valid descriptor during translation process a synchronous exception is generated. If this invalid bit was set by kernel on purpose, the kernel shall handle this exception, allocate a new page, and prepare a correct descriptor (We will look in details on how this works a little bit later) Bit 1 This bit indicates whether the current descriptor points to a next page table in the hierarchy (we call such descriptor a \" table descriptor \") or it points instead to a physical page or a section (such descriptors are called \" block descriptors \"). Bits [11:2] Those bits are ignored for table descriptors. For block descriptors they contain some attributes that control, for example, whether the mapped page is cachable, executable, etc. Bits [47:12] . This is the place where the address that a descriptor points to is stored. As I mentioned previously, only bits [47:12] of the address need to be stored, because all other bits are always 0. Bits [63:48] Another set of attributes. Configuring page attributes As I mentioned in the previous section, each block descriptor contains a set of attributes that controls various virtual page parameters. However, the attributes that are most important for our discussion are NOT configured directly in the descriptor. Instead, ARM processors implement a trick for compressing descriptor attributes commonly used. (The days when ARM hardware was way simpler were gone) Memory attribute indirection ARMv8 architecture introduces mair_el1 register. See its definition . This register consists of 8 slots, each spanning 8 bits. Each slot configures a common set of attributes. A descriptor then specifies just an index of the mair slot, instead of specifying all attributes directly. This allows using only 3 bits in the descriptor to reference a mair slot. We are using only a few of available attribute options. Here is the code that prepares values for the mair register. /* * Memory region attributes: * * n = AttrIndx[2:0] * n MAIR * DEVICE_nGnRnE 000 00000000 * NORMAL_NC 001 01000100 */ #define MT_DEVICE_nGnRnE 0x0 #define MT_NORMAL_NC 0x1 #define MT_DEVICE_nGnRnE_FLAGS 0x00 #define MT_NORMAL_NC_FLAGS 0x44 #define MAIR_VALUE (MT_DEVICE_nGnRnE_FLAGS << (8 * MT_DEVICE_nGnRnE)) | (MT_NORMAL_NC_FLAGS << (8 * MT_NORMAL_NC)) Here we are using only 2 out of 8 available slots in the mair registers. The first one corresponds to device memory (IO registers) and second to normal non-cacheable memory. MT_DEVICE_nGnRnE and MT_NORMAL_NC are indexes that we are going to use in block descriptors, MT_DEVICE_nGnRnE_FLAGS and MT_NORMAL_NC_FLAGS are values that we are storing in the first 2 slots of the mair_el1 register. Kernel vs user virtual memory After the MMU is switched on, each memory access issued by kernel must use virtual address instead of physical. One consequence is that the kernel itself must maintain its own set of page tables. One possible solution could be to reload ttbr (pointing to the PGD base) each time we switch from user to kernel mode. Reloading ttbr can be costly. (Why?) This makes syscalls and page faults expensive. Commodity kernels therefore avoid frequent reloads of PGD base. A kernel splits the virtual address space into 2 parts: user portion and kernel portion. When switching among user tasks, the kernel only changes the mapping of the user portion while keeping the kernel mapping unchanged. This classic kernel design turns out to lead to most severe security holes in recent years. Google \"spectre and meltdown\". On 32-bit CPUs, a kernel usually allocate first 3 GB of the address space for user and reserve last 1 GB for the kernel. 64-bit architectures are much more favorable in this regard because of huge virtual address space (how large?). And even more: ARMv8 architecture comes with a native feature that can be used to easily implement user/kernel address split. ARM64 defines 2 ttbr registers for holding PGD base addresses: ttbr0_el1 and ttbr1_el1 , pointing to the user PGD and the kernel PGD, respectively. As you might remember MMU uses only 48 bits out of 64 bits in the virtual addresses for translation, so the upper 16 bits can be used to distinguish between ttbr0 and ttbr1 translation processes. User virtual addresses : upper 16 bits == 0. MMU uses the PGD base stored in ttbr0_el1 . This value shall be changed according to process switch. Kernel virtual addresses : upper 16 bits == 0xffff . MMU uses the PGD base stored in ttbr0_el1 . This value shall remain unchanged throughout the life of the kernel. The CPU also enforces that software at EL0 can never access virtual addresses started with 0xffff . Doing so triggers a synchronous exception. Adjusting kernel addresses All absolute kernel addresses must start with 0xffff... . There are 2 places in the kernel source code shall be changed. In the linker script we specify base address of the image as 0xffff000000000000 . This will make the linker think that our image is going to be loaded at 0xffff000000000000 address, and therefore whenever it needs to generate an absolute address it will make it right. (There are a few more changes to the linker script, but we will discuss them later.) We hardcode absolute kernel base addresses in the header where we define device base address. After switching on MMU, kernel has to access all IO via virtual addresses. We can map them starting from 0xffff00003F000000 . In the next section we will explore in detail the code that creates this mapping. Kernel boot: initializing kernel page tables Important: the linker is completely oblivious to kernel physical address , e.g. the physical base (0x0 or 0x80000) where the kernel will be loaded. Two Implications: the linker links all kernel symbols at virtual addresses starting from 0xffff000000000000 ; Before kernel boots and before it turns on MMU, the kernel will operate on physical addresses starting from 0x0 (or 0x80000 for QEMU). Keep this key constraint in mind. See below. Right after kernel switches to EL1 and clears the BSS, the kernel populates its pgtables via __create_page_tables function. // boot.S __create_page_tables: mov x29, x30 // save return address First, the function saves x30 (LR). As we are going to call other functions from __create_page_tables , x30 will be overwritten. Usually x30 is saved on the stack but, as we know that we are not going to use recursion and nobody else will use x29 during __create_page_tables execution, this simple method of preserving link register also works fine. Q: What could go wrong if we push x30 to stack here? adrp x0, pg_dir // adrp: form PC-relative address to 4KB page mov x1, #PG_DIR_SIZE bl memzero Next, we clear the initial page tables area. An important thing to understand here is where this area is located (x0) and how do we know its size (x1)? Initial page tables area is defined in the linker script - this means that we are allocating the spot for this area in the kernel image itself. Calculating the size of this area is a little bit trickier. First, we need to understand the structure of the initial kernel page tables. We know that all our mappings are all inside 1 GB region (this is the size of RPi3 physical memory). One PGD descriptor can cover 2^39 = 512 GB and one PUD descriptor can cover 2^30 = 1 GB of continuous virtual mapping area. (Those values are calculated based on the PGD and PUD indexes location in the virtual address.) This means that we need just one PGD and one PUD to map the whole RPi memory, and even more - both PGD and PUD will contain a single descriptor (of course we still need to allocate at least one page for them each). If we have a single PUD entry there also must be a single PMD table, to which this entry will point. (Single PMD entry covers 2 MB, there are 512 items in a PMD, so in total the whole PMD table covers the same 1 GB of memory that is covered by a single PUD descriptor.) Next, we know that we need to map 1 GB region of memory, which is a multiple of 2 MB. This allows us to keep things simple -- using section mapping. This means that we don't need PTE at all. So in total, we need 3 pages: one for PGD, PUD and PMD - this is precisely the size of the initial page table area. Q: here, MMU is off and everything should be physical address. How could the kernel possibly address functions/variables like memzero, which are linked at virtual addresses? (Hint: check the disassembly of the kernel binary) Allocating & installing a new pgtable Now we are going to step outside __create_page_tables function and take a look on 2 essential macros: create_table_entry and create_block_map . create_table_entry is responsible for allocating a new page table (In our case either PGD or PUD) The source code is listed below. .macro create_table_entry, tbl, virt, shift, tmp1, tmp2 lsr \\tmp1, \\virt, #\\shift and \\tmp1, \\tmp1, #PTRS_PER_TABLE - 1 // table index add \\tmp2, \\tbl, #PAGE_SIZE orr \\tmp2, \\tmp2, #MM_TYPE_PAGE_TABLE str \\tmp2, [\\tbl, \\tmp1, lsl #3] add \\tbl, \\tbl, #PAGE_SIZE // next level table page .endm This macro accepts the following arguments. tbl - a pointer to a memory region were new table has to be allocated. virt - virtual address that we are currently mapping. shift - shift that we need to apply to the virtual address in order to extract current table index. (39 in case of PGD and 30 in case of PUD) tmp1 , tmp2 - temporary registers. This macro is very important, so we are going to spend some time understanding it. lsr \\tmp1, \\virt, #\\shift and \\tmp1, \\tmp1, #PTRS_PER_TABLE - 1 // table index The first two lines of the macro are responsible for extracting table index from the virtual address. We are applying right shift first to strip everything to the right of the index and then using and operation to strip everything to the left. add \\tmp2, \\tbl, #PAGE_SIZE Then the address of the next page table is calculated. Here we are using the convention that all our initial page tables are located in one continuous memory region. We simply assume that the next page table in the hierarchy will be adjacent to the current page table. orr \\tmp2, \\tmp2, #MM_TYPE_PAGE_TABLE Next, a pointer to the next page table in the hierarchy is converted to a table descriptor. (A descriptor must have 2 lower bits set to 1 ) str \\tmp2, [\\tbl, \\tmp1, lsl #3] Then the descriptor is stored in the current page table. We use previously calculated index to find the right spot in the table. add \\tbl, \\tbl, #PAGE_SIZE // next level table page Finally, we change tbl parameter to point to the next page table in the hierarchy. This is convenient because now we can call create_table_entry one more time for the next table in the hierarchy without making any adjustments to the tbl parameter. This is precisely what we are doing in the create_pgd_entry macro, which is just a wrapper that allocates both PGD and PUD. Populating a PMD table Next important macro is create_block_map . As you might guess this macro is responsible for populating entries of the PMD table. It looks like the following. .macro create_block_map, tbl, phys, start, end, flags, tmp1 lsr \\start, \\start, #SECTION_SHIFT and \\start, \\start, #PTRS_PER_TABLE - 1 // table index lsr \\end, \\end, #SECTION_SHIFT and \\end, \\end, #PTRS_PER_TABLE - 1 // table end index lsr \\phys, \\phys, #SECTION_SHIFT mov \\tmp1, #\\flags orr \\phys, \\tmp1, \\phys, lsl #SECTION_SHIFT // table entry 9999: str \\phys, [\\tbl, \\start, lsl #3] // store the entry add \\start, \\start, #1 // next entry add \\phys, \\phys, #SECTION_SIZE // next block cmp \\start, \\end b.ls 9999b .endm Parameters here are a little bit different. tbl - a pointer to the PMD table. phys - the start of the physical region to be mapped. start - virtual address of the first section to be mapped. end - virtual address of the last section to be mapped. flags - flags that need to be copied into lower attributes of the block descriptor. tmp1 - temporary register. Now, let's examine the source. lsr \\start, \\start, #SECTION_SHIFT and \\start, \\start, #PTRS_PER_TABLE - 1 // table index Those 2 lines extract the table index from start virtual address. This is done exactly in the same way as we did it before in the create_table_entry macro. lsr \\end, \\end, #SECTION_SHIFT and \\end, \\end, #PTRS_PER_TABLE - 1 // table end index The same thing is repeated for the end address. Now both start and end contains not virtual addresses, but indexes in the PMD table, corresponding to the original addresses. lsr \\phys, \\phys, #SECTION_SHIFT mov \\tmp1, #\\flags orr \\phys, \\tmp1, \\phys, lsl #SECTION_SHIFT // table entry Next, block descriptor is prepared and stored in the tmp1 variable. In order to prepare the descriptor phys parameter is first shifted to right then shifted back and merged with the flags parameter using orr instruction. If you wonder why do we have to shift the address back and forth - the answer is that this clears first 21 bit in the phys address and makes our macro universal, allowing it to be used with any address, not just the first address of the section. 9999: str \\phys, [\\tbl, \\start, lsl #3] // store the entry add \\start, \\start, #1 // next entry add \\phys, \\phys, #SECTION_SIZE // next block cmp \\start, \\end b.ls 9999b // jump back if \"Unsigned Less than or equal\" The final part of the function is executed inside a loop. Here we first store current descriptor at the right index in the PMD table. Next, we increase current index by 1 and update the descriptor to point to the next section. We repeat the same process until current index becomes equal to the last index. Putting it together: __create_page_tables() Now, when you understand how create_table_entry and create_block_map macros work, it will be straightforward to understand the rest of the __create_page_tables function. adrp x0, pg_dir mov x1, #VA_START create_pgd_entry x0, x1, x2, x3 Here we create both PGD and PUD. We configure them to start mapping from VA_START virtual address. Because of the semantics of the create_table_entry macro, after create_pgd_entry finishes x0 will contain the address of the next table in the hierarchy - namely PMD. /* Mapping kernel and init stack*/ mov x1, xzr // start mapping from physical offset 0 mov x2, #VA_START // first virtual address ldr x3, =(VA_START + DEVICE_BASE - SECTION_SIZE) // last virtual address create_block_map x0, x1, x2, x3, MMU_FLAGS, x4 Next, we create virtual mapping of the whole memory, excluding device registers region. We use MMU_FLAGS constant as flags parameter - this marks all sections to be mapped as normal noncacheable memory. (Note, that MM_ACCESS flag is also specified as part of MMU_FLAGS constant. Without this flag each memory access will generate a synchronous exception.) /* Mapping device memory*/ mov x1, #DEVICE_BASE // start mapping from device base address ldr x2, =(VA_START + DEVICE_BASE) // first virtual address ldr x3, =(VA_START + PHYS_MEMORY_SIZE - SECTION_SIZE) // last virtual address create_block_map x0, x1, x2, x3, MMU_DEVICE_FLAGS, x4 Then device registers region is mapped. This is done exactly in the same way as in the previous code sample, with the exception that we are now using different start and end addresses and different flags. mov x30, x29 // restore return address ret Finally, the function restored link register and returns to the caller. Configuring page translation Now page tables are created and we are back to the el1_entry function. But there is still some work to be done before we can switch on the MMU. mov x0, #VA_START add sp, x0, #LOW_MEMORY We are updating init task stack pointer. Now it uses a virtual address, instead of a physical one. Therefore it could be used only after MMU is on. Recall that our kernel uses linear mapping therefore an offset is simply applied. adrp x0, pg_dir msr ttbr1_el1, x0 ttbr1_el1 is updated to point to the previously populated PGD table. Q: Isn't pg_dir a virtual address (e.g. ffff000000083000) set by the linker? ttbr shall expect a physical address, right? How could this work? ldr x0, =(TCR_VALUE) msr tcr_el1, x0 tcr_el1 of Translation Control Register is responsible for configuring some general parameters of the MMU. (For example, here we configure that both kernel and user page tables should use 4 KB pages.) ldr x0, =(MAIR_VALUE) msr mair_el1, x0 We already discussed mair register in the \"Configuring page attributes\" section. Here we just set its value. ldr x2, =kernel_main mov x0, #SCTLR_MMU_ENABLED msr sctlr_el1, x0 // BOOM! br x2 msr sctlr_el1, x0 is the line where MMU is actually enabled. Now we can jump to the kernel_main function. From this moment onward kernel runs on virtual addresses completely. An interesting question is why can't we just execute br kernel_main instruction? Indeed, we can't. Before the MMU was enabled we have been working with physical memory, the kernel is loaded at a physical offset 0 - this means that current program counter (PC) is very close to 0. Switching on the MMU doesn't update PC. br kernel_main uses offset relative to the current PC and jumps to the place were kernel_main would have been if we don't turn on the MMU. Example: in generating the kernel binary, the linker starts from base address 0xffff000000000000 as controlled by our linker script. It assigns the instruction \"br kernel_main\" to address 0xffff000000000080; it assigns kernel_main to 0xffff000000003190. The instruction \"br kernel_main\" will be a relative jump, and will be emitted as \"br #0x3110\" (we can verify this by disassembling the kernel binary). At run time, when we reach \"br kernel_main\", PC is 0x80. Executing the instruction will update PC is to 0x3190. As MMU is on now, CPU fetches instruction at 0x3190 via MMU. A translation fault! ldr x2, =kernel_main does not suffer from the problem. CPU loads x2 with the link address of kernel_main , e.g. 0xffff000000003190. Different from br kernel_main which uses PC-based offset, br x2 jumps to an absolute address stored in x2 (this is called long jmp). Therefore, PC will be updated with the link address of kernel_main which can be translated via MMU. In other words, by executing a long jmp, we \"synchronize\" the PC value with virtual addresses. Another question: why ldr x2, =kernel_main itself must be executed before we turn on the MMU? The reason is that ldr also uses pc relative offset. See the manual . On my build, it emitted as ldr x2, #0x10c . So if we execute this instruction after MMU is on but before we \"synchronize\" PC, MMU will give another translation fault. Compiling & loading user programs Commodity kernels load user programs as ELF from filesystems. We won't be building a filesystem or ELF loader in this experiment. As a workaround, we will embed user programs in the kernel binary at link time, and load them at run time. For easy loading, we will store the user program in a separate ELF section of the kernel binary. Here is the relevant section of the linker script that is responsible for doing this. . = ALIGN(0x00001000); user_begin = .; .text.user : { build/user* (.text) } .rodata.user : { build/user* (.rodata) } .data.user : { build/user* (.data) } .bss.user : { build/user* (.bss) } user_end = .; I made a convention: user level source code should be defined in C source files named as \"userXXX\". The linker script then can isolate all user related code in a continuous region, of which the start and end are marked with user_begin and user_end symbols. At run time, the kernel simply copies everything between user_begin and user_end to the newly allocated process address space, thus simulating loading a user program. A simple hack, but suffice for our current purpose. Aside: our user symbol addresses As user programs will be linked as part the kernel binary, the linker will place all user symbols (functions & variables) in the kernel's address space (0xffff000000000000 onwards). You can verify this by, e.g. nm kernel8.elf|grep \" user_\" . How could such user programs work? We rely on an assumption: our user programs are simple enough; they always address memory with register-relative offsets but not absolute address. You can verify this by disassembly. However, if our programs, e.g. call functions via pointers, the entailed long jmp will target absolute virtual address inside kernel and will trigger exception. This assumption can't go a long way. The right solution would be link user programs and kernel separately. Right now there are 2 files that are compiled in the user region. user_sys.S This file contains definitions of the syscall wrapper functions. The RPi OS still supports the same syscalls as in the previous lesson, with the exception that now instead of clone syscall we are going to use fork syscall. The difference is that fork copies process virtual memory, and that is something we want to try doing. user.c User program source code. Almost the same as we've used in the previous lesson. Creating first user process As it was the case in the previous lesson, move_to_user_mode function is responsible for creating the first user process. We call this function from a kernel thread. Here is how we do this. void kernel_process(){ printf(\"Kernel process started. EL %d\\r\\n\", get_el()); unsigned long begin = (unsigned long)&user_begin; unsigned long end = (unsigned long)&user_end; unsigned long process = (unsigned long)&user_process; int err = move_to_user_mode(begin, end - begin, process - begin); if (err < 0){ printf(\"Error while moving process to user mode\\n\\r\"); } } Now we need 3 arguments to call move_to_user_mode : a pointer to the beginning of the user code area, size of the area and offset of the startup function inside it. This information is calculated based on the previously discussed user_begin and user_end symbols (as global variables). move_to_user_mode function is listed below. int move_to_user_mode(unsigned long start, unsigned long size, unsigned long pc) { struct pt_regs *regs = task_pt_regs(current); regs->pstate = PSR_MODE_EL0t; regs->pc = pc; regs->sp = 2 * PAGE_SIZE; unsigned long code_page = allocate_user_page(current, 0); if (code_page == 0) { return -1; } memcpy(code_page, start, size); set_pgd(current->mm.pgd); return 0; } Now let's try to inspect in details what is going on here. struct pt_regs *regs = task_pt_regs(current); As it was the case in the previous lesson, we obtain a pointer to pt_regs area and set pstate , so that after kernel_exit we will end up in EL0. regs->pc = pc; pc now points to the offset of the startup function in the user region. regs->sp = 2 * PAGE_SIZE; We made a simple convention that our user program will not exceed 1 page in size. We allocate the second page to the stack. unsigned long code_page = allocate_user_page(current, 0); if (code_page == 0) { return -1; } allocate_user_page reserves 1 memory page and maps it to the virtual address, provided as a second argument. In the process of mapping it populates page tables, associated with the current process. We will investigate in details how this function works later in this chapter. memcpy(code_page, start, size); Next, we are going to copy the whole user region to the new address space (in the page that we have just mapped), starting from offset 0, so the offset in the user region will become an actual virtual address of the starting point. set_pgd(current->mm.pgd); Finally, we call set_pgd , which updates ttbr0_el1 register and thus activate cu4rrent process translation tables. Aside: TLB If you take a look at the set_pgd function you will see that after it sets ttbr0_el1 it also clears TLB (Translation lookaside buffer). TLB is a cache that is designed specifically to store the mapping between physical and virtual pages. The first time some virtual address is mapped into a physical one this mapping is stored in TLB. Next time we need to access the same page we no longer need to perform full page table walk. Therefore it makes perfect sense that we invalidate TLB after updating page tables - otherwise our change will not be applied for the pages already stored in the TLB. Usually, we try to avoid using all caches for simplicity, but without TLB any memory access would become extremely inefficient, and I don't think that it is even possible to completely disable TLB. Besides, TLB doesn't add any other complexity to the OS, in spite of the fact that we must clean it after switching ttbr0_el1 . Mapping a virtual page to user We have seen previously how allocate_user_page function is used - now it is time to see what is inside it. unsigned long allocate_user_page(struct task_struct *task, unsigned long va) { unsigned long page = get_free_page(); if (page == 0) { return 0; } map_page(task, va, page); return page + VA_START; } This function allocates a new page, maps it to the provided virtual address and returns a pointer to the page. When we say \"a pointer\" now we need to distinguish between 3 things: a pointer to a physical page, a pointer inside kernel address space and a pointer inside user address space - all these 3 different pointers can lead to the same location in memory. In our case page variable is a physical pointer (note its \"unsigned long\" type -- not a C pointer!) and the return value is a pointer inside kernel address space. This pointer can be easily calculated because we linearly map the whole physical memory starting at VA_START virtual address. Through the pointer, our kernel copies the user program to the page. Does our kernel have a virtual mapping for the new page? We do not have to worry, because kernel maps the entire physical memory in boot.S . User mapping is still required to be created and this happens in the map_page function, which we will explore next. void map_page(struct task_struct *task, unsigned long va, unsigned long page){ unsigned long pgd; if (!task->mm.pgd) { task->mm.pgd = get_free_page(); task->mm.kernel_pages[++task->mm.kernel_pages_count] = task->mm.pgd; } pgd = task->mm.pgd; int new_table; unsigned long pud = map_table((unsigned long *)(pgd + VA_START), PGD_SHIFT, va, &new_table); if (new_table) { task->mm.kernel_pages[++task->mm.kernel_pages_count] = pud; } unsigned long pmd = map_table((unsigned long *)(pud + VA_START) , PUD_SHIFT, va, &new_table); if (new_table) { task->mm.kernel_pages[++task->mm.kernel_pages_count] = pmd; } unsigned long pte = map_table((unsigned long *)(pmd + VA_START), PMD_SHIFT, va, &new_table); if (new_table) { task->mm.kernel_pages[++task->mm.kernel_pages_count] = pte; } map_table_entry((unsigned long *)(pte + VA_START), va, page); struct user_page p = {page, va}; task->mm.user_pages[task->mm.user_pages_count++] = p; } map_page in some way duplicates what we've been doing in the __create_page_tables function: it allocates and populates a page table hierarchy. There are 3 important difference, however: now we are doing this in C, instead of assembler. map_page maps a single page, instead of the whole memory, and use normal page mapping, instead of section mapping. There are 2 important functions involved in the process: map_table and map_table_entry . map_table is listed below. unsigned long map_table(unsigned long *table, unsigned long shift, unsigned long va, int* new_table) { unsigned long index = va >> shift; index = index & (PTRS_PER_TABLE - 1); if (!table[index]){ *new_table = 1; unsigned long next_level_table = get_free_page(); unsigned long entry = next_level_table | MM_TYPE_PAGE_TABLE; table[index] = entry; return next_level_table; } else { *new_table = 0; } return table[index] & PAGE_MASK; } This function has the following arguments. table This is a pointer to the parent page table. This page table is assumed to be already allocated, but might be empty. shift This argument is used to extract table index from the provided virtual address. va Virtual address itself. new_table This is an output parameter. It is set to 1 if a new child table has been allocated and left 0 otherwise. You can think of this function as an analog of the create_table_entry macro. It extracts table index from the virtual address and prepares a descriptor in the parent table that points to the child table. Unlike create_table_entry macro we don't assume that the child table should be adjacent into memory with the parent table - instead, we rely on get_free_table function to return whatever page is available. It also might be the case that child table was already allocated (This might happen if child page table covers the region where another page has been allocated previously.). In this case we set new_table to 0 and read child page table address from the parent table. map_page calls map_table 3 times: once for PGD, PUD and PMD. The last call allocates PTE and sets a descriptor in the PMD. Next, map_table_entry is called. You can see this function below. void map_table_entry(unsigned long *pte, unsigned long va, unsigned long pa) { unsigned long index = va >> PAGE_SHIFT; index = index & (PTRS_PER_TABLE - 1); unsigned long entry = pa | MMU_PTE_FLAGS; pte[index] = entry; } map_table_entry extracts PTE index from the virtual address and then prepares and sets PTE descriptor. It is similar to what we've been doing in the create_block_map macro. That's it about user page tables allocation, but map_page is responsible for one more important role: it keeps track of the pages that have been allocated during the process of virtual address mapping. All such pages are stored in the kernel_pages array. We need this array to be able to clean up allocated pages after a task exits. There is also user_pages array, which is also populated by the map_page function. This array store information about the correspondence between process virtual pages any physical pages. We need this information in order to be able to copy process virtual memory during fork (More on this later). Forking a user process Let's summarize where we are so far: we've seen how first user process is created, its page tables populated, code & data copied to the proper location and stack initialized. After all of this preparation, the process is ready to run. The code that is executed inside user process is listed below. void loop(char* str) { char buf[2] = {\"\"}; while (1){ for (int i = 0; i < 5; i++){ buf[0] = str[i]; call_sys_write(buf); user_delay(1000000); } } } void user_process() { call_sys_write(\"User process\\n\\r\"); int pid = call_sys_fork(); if (pid < 0) { call_sys_write(\"Error during fork\\n\\r\"); call_sys_exit(); return; } if (pid == 0){ loop(\"abcde\"); } else { loop(\"12345\"); } } The familiar fork() semantics The code itself is very simple as we expect. Unlike clone , when doing fork we don't need to provide the function that needs to be executed in a new process. Also, the fork wrapper function is much easier than the clone one. All of this is possible because of the fact that fork make a full copy of the process virtual address space, so the fork wrapper function return twice: one time in the original process and one time in the new one. At this point, we have two identical processes, with identical stacks and pc positions. The only difference is the return value of the fork syscall: it returns child PID in the parent process and 0 in the child process. Starting from this point both processes begin completely independent life and can modify their stacks and write different things using same addresses in memory - all of this without affecting one another. Implementation Now let's see how fork system call is implemented. copy_process function does most of the job. int copy_process(unsigned long clone_flags, unsigned long fn, unsigned long arg) { preempt_disable(); struct task_struct *p; unsigned long page = allocate_kernel_page(); p = (struct task_struct *) page; struct pt_regs *childregs = task_pt_regs(p); if (!p) return -1; if (clone_flags & PF_KTHREAD) { p->cpu_context.x19 = fn; p->cpu_context.x20 = arg; } else { struct pt_regs * cur_regs = task_pt_regs(current); *childregs = *cur_regs; childregs->regs[0] = 0; copy_virt_memory(p); } p->flags = clone_flags; p->priority = current->priority; p->state = TASK_RUNNING; p->counter = p->priority; p->preempt_count = 1; //disable preemtion until schedule_tail p->cpu_context.pc = (unsigned long)ret_from_fork; p->cpu_context.sp = (unsigned long)childregs; int pid = nr_tasks++; task[pid] = p; preempt_enable(); return pid; } This function looks almost exactly the same as in the previous lesson with one exception: when copying user processes, now, instead of modifying new process stack pointer and program counter, we instead call copy_virt_memory . copy_virt_memory looks like this. int copy_virt_memory(struct task_struct *dst) { struct task_struct* src = current; for (int i = 0; i < src->mm.user_pages_count; i++) { unsigned long kernel_va = allocate_user_page(dst, src->mm.user_pages[i].virt_addr); if( kernel_va == 0) { return -1; } memcpy(kernel_va, src->mm.user_pages[i].virt_addr, PAGE_SIZE); } return 0; } It iterates over user_pages array, which contains all pages, allocated by the current process. Note, that in user_pages array we store only pages that are actually available to the process and contain its source code or data; we don't include here page table pages, which are stored in kernel_pages array. Next, for each page, we allocate another empty page and copy the original page content there. We also map the new page using the same virtual address, that is used by the original one. This is how we get the exact copy of the original process address space. All other details of the forking procedure work exactly in the same way, as they have been in the previous lesson. Q: does our fork() implement COW? Demand paging If you go back and take a look at the move_to_user_mode function, you may notice that we only map a single page, starting at offset 0. But we also assume that the second page will be used as a stack. Our kernel will map stack page, as well as any other page that a process needs to access as soon as it will be requested for the first time. Now we are going to explore the inner-workings of this mechanism. Setting up page faults When a process tries to access some address which belongs to the page that is not yet mapped, a synchronous exception is generated. This is the second type of synchronous exception that we are going to support (the first type is an exception generated by the svc instruction which is a system call). Synchronous exception handler now looks like the following. el0_sync: kernel_entry 0 mrs x25, esr_el1 // read the syndrome register lsr x24, x25, #ESR_ELx_EC_SHIFT // exception class cmp x24, #ESR_ELx_EC_SVC64 // SVC in 64-bit state b.eq el0_svc cmp x24, #ESR_ELx_EC_DABT_LOW // data abort in EL0 b.eq el0_da handle_invalid_entry 0, SYNC_ERROR Here we use esr_el1 register to determine exception type. If it is a page fault exception (or, which is the same, data access exception) el0_da function is called. el0_da: bl enable_irq mrs x0, far_el1 mrs x1, esr_el1 bl do_mem_abort cmp x0, 0 b.eq 1f handle_invalid_entry 0, DATA_ABORT_ERROR 1: bl disable_irq kernel_exit 0 el0_da redirects the main work to the do_mem_abort function. This function takes two arguments 1. The memory address which we tried to access. This address is taken from far_el1 register (Fault address register) 1. The content of the esr_el1 (Exception syndrome register) Handling page faults do_mem_abort is listed below. int do_mem_abort(unsigned long addr, unsigned long esr) { unsigned long dfs = (esr & 0b111111); if ((dfs & 0b111100) == 0b100) { unsigned long page = get_free_page(); if (page == 0) { return -1; } map_page(current, addr & PAGE_MASK, page); ind++; if (ind > 2){ return -1; } return 0; } return -1; } In order to understand this function, you need to know a little bit about the specifics of that esr_el1 register. Bits [32:26] of this register are called \"Exception Class\". We check those bits in the el0_sync handler to determine whether it is a syscall, or a data abort exception or potentially something else. Exception class determines the meaning of bits [24:0] - those bits are usually used to provide additional information about the exception. The meaning of [24:0] bits in case of the data abort exception is described on the page 2460 of the AArch64-Reference-Manual . In general, data abort exception can happen in many different scenarios (it could be a permission fault, or address size fault or a lot of other things). We are only interested in a translation fault which happens when some of the page tables for the current virtual address are not initialized. So in the first 2 lines of the do_mem_abort function, we check whether the current exception is actually a translation fault. If yes we allocate a new page and map it to the requested virtual address. All of this happens completely transparent for the user program - it doesn't notice that some of the memory accesses were interrupted and new page tables were allocated in the meantime. Conclusion This was a long and difficult chapter, but I hope it was useful as well. Virtual memory is really one of the most fundamental pieces of any operating system and I am glad we've passed through this chapter and, hopefully, started to understand how it works at the lowest level. With the introduction of virtual memory we now have full process isolation, but the RPi OS is still far from completion. It still doesn't support file systems, drivers, signals and interrupt waitlists, networking and a lot of other useful concepts, and we will continue to uncover them in the upcoming lessons. Previous Page 5.3 User processes and system calls: Exercises Next Page 6.2 Virtual memory management: Linux (in progress) 6.3 jump forward to Virtual memory management: Exercises","title":"6: Virtual memory (VM)"},{"location":"lesson06/rpi-os/#6-virtual-memory-vm","text":"","title":"6: Virtual memory (VM)"},{"location":"lesson06/rpi-os/#objectives","text":"Make our tiny kernel capable of: enforcing separate virtual address spaces, and user-level demand paging.","title":"Objectives"},{"location":"lesson06/rpi-os/#roadmap","text":"Prior to this experiment, our kernel can run and schedule user processes, but the isolation between them is not complete - all processes and the kernel itself share the same memory. This allows any process to easily access somebody else's data and even kernel data. And even if we assume that all our processes are not malicious, there is another drawback: before allocating memory each process need to know which memory regions are already occupied - this makes memory allocation for a process more complicated. We will take the following steps. Set up a pgtable for kernel. Using linear mapping. Turn on MMU shortly after kernel boots. This is a common kernel design. Set up pgtables for user processes Implement fork() for user processes Implement demand paging","title":"Roadmap"},{"location":"lesson06/rpi-os/#background-arm64-translation-process","text":"","title":"Background: ARM64 translation process"},{"location":"lesson06/rpi-os/#page-table-format","text":"This experiment introduces VM to our kernel. With VM, we can formally call tasks \"processes\". Each task will have its own address space. They issue memory access with virtual addresses. The MMU transparently translates virtual addresses to physical addresses. The MMU uses page table (or pgtable, or \"translation table\" in ARM's manual). The following diagram summarizes ARM64 address translation with uses 4-level pgtables. Virtual address Physical Memory +-----------------------------------------------------------------------+ +-----------------_+ | | PGD Index | PUD Index | PMD Index | PTE Index | Page offset | | | +-----------------------------------------------------------------------+ | | 63 47 | 38 | 29 | 20 | 11 | 0 | Page N | | | | | +--------------------+ +---->+------------------+ | | | +---------------------+ | | | | +------+ | | | | | | | | | +----------+ | | | |------------------| +------+ | PGD | | | +---------------->| Physical address | | ttbr |---->+-------------+ | PUD | | | |------------------| +------+ | | | | +->+-------------+ | PMD | | | | | +-------------+ | | | | | +->+-------------+ | PTE | +------------------+ +->| PUD address |----+ +-------------+ | | | | | +->+--------------+ | | | +-------------+ +--->| PMD address |----+ +-------------+ | | | | | | | | | +-------------+ +--->| PTE address |----+ +-------------_+ | | | +-------------+ | | +-------------+ +--->| Page address |----+ | | +-------------+ | | +--------------+ | | +-------------+ | | | | +--------------+ +------------------+ Notable points: Page tables have a hierarchical structure, i.e. a tree. An item in any of the tables contains an address of the next table in the hierarchy. Note: strictly speaking a pgtable means a contiguous array of entries at any of the four levels. So a tree has many pgtables. Some documents casually use \"pgtable\" to refer to an entire pgtable tree. Be careful. There are 4 levels in the table hierarchy: PGD (Page Global Directory), PUD (Page Upper Directory), PMD (Page Middle Directory), PTE (Page Table Entry). PTE is the last table in the hierarchy and it points to the actual page in the physical memory. Don't read too much into the terms, which just represent lv1, 2, ... pgtables. Them terms come from the Linux kernel (I guess x86), not ARM64. Over years, they became common lingo among kernel hackers. Besides holding a physical address, each pgtable item holds extra bits crucial for translation. Will examine the format below. MMU starts memory translation process by locating the base address of PGD. MMU locates the base address from the ttbr0_el1 register which should be set by the kernel. ttbr=translation table base register. MMU walks the pgtable tree to look up the physical address. A virtual address uses only 48 out of 64 available bits. When doing a translation, MMU splits an address into 4 parts: 9 bits [39 - 47] contain an index in the PGD table. MMU uses this index to find the location of the PUD. 9 bits [30 - 38] contain an index in the PUD table. MMU uses this index to find the location of the PMD. 9 bits [21 - 29] contain an index in the PMD table. MMU uses this index to find the location of the PTE. 9 bits [12 - 20] contain an index in the PTE table. MMU uses this index to find a page in the physical memory. Bits [0 - 11] contain an offset in the physical page. MMU uses this offset to determine the exact position in the previously found page that corresponds to the original virtual address. Each process has its own address space. Therefore, it has its own copy of page table tree, starting from PGD. Therefore, the kernel keeps a separate PGD base address for each process. That is, the kernel virtualizes PGD for processes. During a context switch, the kernel loads the PGD base of the next process to ttbr0_el1 . Memory for a user process is always allocated in pages. A page is a contiguous memory region 4KB in size (ARM processors support larger pages, but 4KB is the most common case and we are going to limit our discussion only to this page size). Exercise: how large is a page table? From the diagram above we know that index in a page table occupies 9 bits (this is true for all page table levels). This means that each page table contains 2^9 = 512 items. Each item in a page table is an address of either the next page table in the hierarchy or a physical page in case of PTE. As we are using a 64-bit processor, each address must be 64 bit or 8 bytes in size. This means that each pgtable is 512 * 8 = 4096 bytes or 4 KB. A pgtable is exactly a page! This might give you an intuition why MMU designers chose such numbers.","title":"Page table format"},{"location":"lesson06/rpi-os/#section-2mb-mapping","text":"This is specific to ARM64 for mapping large parts of continuous physical memory. In this case, instead of 4 KB pages, we can directly map 2 MB blocks that are called sections. This allows to eliminate one single level of translation. The translation diagram, in this case, looks like the following. Virtual address Physical Memory +-----------------------------------------------------------------------+ +-----------------_+ | | PGD Index | PUD Index | PMD Index | Section offset | | | +-----------------------------------------------------------------------+ | | 63 47 | 38 | 29 | 20 | 0 | Section N | | | | | +---->+------------------+ | | | | | | | +------+ | | | | | | | | +----------+ | | |------------------| +------+ | PGD | | +------------------------->| Physical address | | ttbr |---->+-------------+ | PUD | | |------------------| +------+ | | | | +->+-------------+ | PMD | | | | +-------------+ | | | | | +->+-----------------+ | +------------------+ +->| PUD address |----+ +-------------+ | | | | | | | +-------------+ +--->| PMD address |----+ +-----------------+ | | | | | +-------------+ +--->| Section address |-----+ | | +-------------+ | | +-----------------+ | | +-------------+ | | | | +-----------------+ | | +------------------+ As you can see the difference here is that now PMD contains a pointer to the physical section. Also, the offset occupies 21 bits instead of 12 bits (this is because we need 21 bits to encode a 2MB range)","title":"Section (2MB) mapping"},{"location":"lesson06/rpi-os/#page-descriptor-format","text":"An item in a page table is called \"descriptor\". A description has a special format as mandated by MMU hardware. A descriptor contains an address of either next page table or a physical page. The key thing to understand : each descriptor always points to something that is page-aligned (either a physical page, a section or the next page table in the hierarchy). This means that last 12 bits of the address, stored in a descriptor, will always be 0. MMU uses those bits to store additional information (\"attributes\") for translation. Descriptor format `+------------------------------------------------------------------------------------------+ | Upper attributes | Address (bits 47:12) | Lower attributes | Block/table bit | Valid bit | +------------------------------------------------------------------------------------------+ 63 47 11 2 1 0 Bit 0 This bit must be set to 1 for all valid descriptors. If MMU encounter non-valid descriptor during translation process a synchronous exception is generated. If this invalid bit was set by kernel on purpose, the kernel shall handle this exception, allocate a new page, and prepare a correct descriptor (We will look in details on how this works a little bit later) Bit 1 This bit indicates whether the current descriptor points to a next page table in the hierarchy (we call such descriptor a \" table descriptor \") or it points instead to a physical page or a section (such descriptors are called \" block descriptors \"). Bits [11:2] Those bits are ignored for table descriptors. For block descriptors they contain some attributes that control, for example, whether the mapped page is cachable, executable, etc. Bits [47:12] . This is the place where the address that a descriptor points to is stored. As I mentioned previously, only bits [47:12] of the address need to be stored, because all other bits are always 0. Bits [63:48] Another set of attributes.","title":"Page descriptor format"},{"location":"lesson06/rpi-os/#configuring-page-attributes","text":"As I mentioned in the previous section, each block descriptor contains a set of attributes that controls various virtual page parameters. However, the attributes that are most important for our discussion are NOT configured directly in the descriptor. Instead, ARM processors implement a trick for compressing descriptor attributes commonly used. (The days when ARM hardware was way simpler were gone) Memory attribute indirection ARMv8 architecture introduces mair_el1 register. See its definition . This register consists of 8 slots, each spanning 8 bits. Each slot configures a common set of attributes. A descriptor then specifies just an index of the mair slot, instead of specifying all attributes directly. This allows using only 3 bits in the descriptor to reference a mair slot. We are using only a few of available attribute options. Here is the code that prepares values for the mair register. /* * Memory region attributes: * * n = AttrIndx[2:0] * n MAIR * DEVICE_nGnRnE 000 00000000 * NORMAL_NC 001 01000100 */ #define MT_DEVICE_nGnRnE 0x0 #define MT_NORMAL_NC 0x1 #define MT_DEVICE_nGnRnE_FLAGS 0x00 #define MT_NORMAL_NC_FLAGS 0x44 #define MAIR_VALUE (MT_DEVICE_nGnRnE_FLAGS << (8 * MT_DEVICE_nGnRnE)) | (MT_NORMAL_NC_FLAGS << (8 * MT_NORMAL_NC)) Here we are using only 2 out of 8 available slots in the mair registers. The first one corresponds to device memory (IO registers) and second to normal non-cacheable memory. MT_DEVICE_nGnRnE and MT_NORMAL_NC are indexes that we are going to use in block descriptors, MT_DEVICE_nGnRnE_FLAGS and MT_NORMAL_NC_FLAGS are values that we are storing in the first 2 slots of the mair_el1 register.","title":"Configuring page attributes"},{"location":"lesson06/rpi-os/#kernel-vs-user-virtual-memory","text":"After the MMU is switched on, each memory access issued by kernel must use virtual address instead of physical. One consequence is that the kernel itself must maintain its own set of page tables. One possible solution could be to reload ttbr (pointing to the PGD base) each time we switch from user to kernel mode. Reloading ttbr can be costly. (Why?) This makes syscalls and page faults expensive. Commodity kernels therefore avoid frequent reloads of PGD base. A kernel splits the virtual address space into 2 parts: user portion and kernel portion. When switching among user tasks, the kernel only changes the mapping of the user portion while keeping the kernel mapping unchanged. This classic kernel design turns out to lead to most severe security holes in recent years. Google \"spectre and meltdown\". On 32-bit CPUs, a kernel usually allocate first 3 GB of the address space for user and reserve last 1 GB for the kernel. 64-bit architectures are much more favorable in this regard because of huge virtual address space (how large?). And even more: ARMv8 architecture comes with a native feature that can be used to easily implement user/kernel address split. ARM64 defines 2 ttbr registers for holding PGD base addresses: ttbr0_el1 and ttbr1_el1 , pointing to the user PGD and the kernel PGD, respectively. As you might remember MMU uses only 48 bits out of 64 bits in the virtual addresses for translation, so the upper 16 bits can be used to distinguish between ttbr0 and ttbr1 translation processes. User virtual addresses : upper 16 bits == 0. MMU uses the PGD base stored in ttbr0_el1 . This value shall be changed according to process switch. Kernel virtual addresses : upper 16 bits == 0xffff . MMU uses the PGD base stored in ttbr0_el1 . This value shall remain unchanged throughout the life of the kernel. The CPU also enforces that software at EL0 can never access virtual addresses started with 0xffff . Doing so triggers a synchronous exception.","title":"Kernel vs user virtual memory"},{"location":"lesson06/rpi-os/#adjusting-kernel-addresses","text":"All absolute kernel addresses must start with 0xffff... . There are 2 places in the kernel source code shall be changed. In the linker script we specify base address of the image as 0xffff000000000000 . This will make the linker think that our image is going to be loaded at 0xffff000000000000 address, and therefore whenever it needs to generate an absolute address it will make it right. (There are a few more changes to the linker script, but we will discuss them later.) We hardcode absolute kernel base addresses in the header where we define device base address. After switching on MMU, kernel has to access all IO via virtual addresses. We can map them starting from 0xffff00003F000000 . In the next section we will explore in detail the code that creates this mapping.","title":"Adjusting kernel addresses"},{"location":"lesson06/rpi-os/#kernel-boot-initializing-kernel-page-tables","text":"Important: the linker is completely oblivious to kernel physical address , e.g. the physical base (0x0 or 0x80000) where the kernel will be loaded. Two Implications: the linker links all kernel symbols at virtual addresses starting from 0xffff000000000000 ; Before kernel boots and before it turns on MMU, the kernel will operate on physical addresses starting from 0x0 (or 0x80000 for QEMU). Keep this key constraint in mind. See below. Right after kernel switches to EL1 and clears the BSS, the kernel populates its pgtables via __create_page_tables function. // boot.S __create_page_tables: mov x29, x30 // save return address First, the function saves x30 (LR). As we are going to call other functions from __create_page_tables , x30 will be overwritten. Usually x30 is saved on the stack but, as we know that we are not going to use recursion and nobody else will use x29 during __create_page_tables execution, this simple method of preserving link register also works fine. Q: What could go wrong if we push x30 to stack here? adrp x0, pg_dir // adrp: form PC-relative address to 4KB page mov x1, #PG_DIR_SIZE bl memzero Next, we clear the initial page tables area. An important thing to understand here is where this area is located (x0) and how do we know its size (x1)? Initial page tables area is defined in the linker script - this means that we are allocating the spot for this area in the kernel image itself. Calculating the size of this area is a little bit trickier. First, we need to understand the structure of the initial kernel page tables. We know that all our mappings are all inside 1 GB region (this is the size of RPi3 physical memory). One PGD descriptor can cover 2^39 = 512 GB and one PUD descriptor can cover 2^30 = 1 GB of continuous virtual mapping area. (Those values are calculated based on the PGD and PUD indexes location in the virtual address.) This means that we need just one PGD and one PUD to map the whole RPi memory, and even more - both PGD and PUD will contain a single descriptor (of course we still need to allocate at least one page for them each). If we have a single PUD entry there also must be a single PMD table, to which this entry will point. (Single PMD entry covers 2 MB, there are 512 items in a PMD, so in total the whole PMD table covers the same 1 GB of memory that is covered by a single PUD descriptor.) Next, we know that we need to map 1 GB region of memory, which is a multiple of 2 MB. This allows us to keep things simple -- using section mapping. This means that we don't need PTE at all. So in total, we need 3 pages: one for PGD, PUD and PMD - this is precisely the size of the initial page table area. Q: here, MMU is off and everything should be physical address. How could the kernel possibly address functions/variables like memzero, which are linked at virtual addresses? (Hint: check the disassembly of the kernel binary)","title":"Kernel boot: initializing kernel page tables"},{"location":"lesson06/rpi-os/#allocating-installing-a-new-pgtable","text":"Now we are going to step outside __create_page_tables function and take a look on 2 essential macros: create_table_entry and create_block_map . create_table_entry is responsible for allocating a new page table (In our case either PGD or PUD) The source code is listed below. .macro create_table_entry, tbl, virt, shift, tmp1, tmp2 lsr \\tmp1, \\virt, #\\shift and \\tmp1, \\tmp1, #PTRS_PER_TABLE - 1 // table index add \\tmp2, \\tbl, #PAGE_SIZE orr \\tmp2, \\tmp2, #MM_TYPE_PAGE_TABLE str \\tmp2, [\\tbl, \\tmp1, lsl #3] add \\tbl, \\tbl, #PAGE_SIZE // next level table page .endm This macro accepts the following arguments. tbl - a pointer to a memory region were new table has to be allocated. virt - virtual address that we are currently mapping. shift - shift that we need to apply to the virtual address in order to extract current table index. (39 in case of PGD and 30 in case of PUD) tmp1 , tmp2 - temporary registers. This macro is very important, so we are going to spend some time understanding it. lsr \\tmp1, \\virt, #\\shift and \\tmp1, \\tmp1, #PTRS_PER_TABLE - 1 // table index The first two lines of the macro are responsible for extracting table index from the virtual address. We are applying right shift first to strip everything to the right of the index and then using and operation to strip everything to the left. add \\tmp2, \\tbl, #PAGE_SIZE Then the address of the next page table is calculated. Here we are using the convention that all our initial page tables are located in one continuous memory region. We simply assume that the next page table in the hierarchy will be adjacent to the current page table. orr \\tmp2, \\tmp2, #MM_TYPE_PAGE_TABLE Next, a pointer to the next page table in the hierarchy is converted to a table descriptor. (A descriptor must have 2 lower bits set to 1 ) str \\tmp2, [\\tbl, \\tmp1, lsl #3] Then the descriptor is stored in the current page table. We use previously calculated index to find the right spot in the table. add \\tbl, \\tbl, #PAGE_SIZE // next level table page Finally, we change tbl parameter to point to the next page table in the hierarchy. This is convenient because now we can call create_table_entry one more time for the next table in the hierarchy without making any adjustments to the tbl parameter. This is precisely what we are doing in the create_pgd_entry macro, which is just a wrapper that allocates both PGD and PUD.","title":"Allocating &amp; installing a new pgtable"},{"location":"lesson06/rpi-os/#populating-a-pmd-table","text":"Next important macro is create_block_map . As you might guess this macro is responsible for populating entries of the PMD table. It looks like the following. .macro create_block_map, tbl, phys, start, end, flags, tmp1 lsr \\start, \\start, #SECTION_SHIFT and \\start, \\start, #PTRS_PER_TABLE - 1 // table index lsr \\end, \\end, #SECTION_SHIFT and \\end, \\end, #PTRS_PER_TABLE - 1 // table end index lsr \\phys, \\phys, #SECTION_SHIFT mov \\tmp1, #\\flags orr \\phys, \\tmp1, \\phys, lsl #SECTION_SHIFT // table entry 9999: str \\phys, [\\tbl, \\start, lsl #3] // store the entry add \\start, \\start, #1 // next entry add \\phys, \\phys, #SECTION_SIZE // next block cmp \\start, \\end b.ls 9999b .endm Parameters here are a little bit different. tbl - a pointer to the PMD table. phys - the start of the physical region to be mapped. start - virtual address of the first section to be mapped. end - virtual address of the last section to be mapped. flags - flags that need to be copied into lower attributes of the block descriptor. tmp1 - temporary register. Now, let's examine the source. lsr \\start, \\start, #SECTION_SHIFT and \\start, \\start, #PTRS_PER_TABLE - 1 // table index Those 2 lines extract the table index from start virtual address. This is done exactly in the same way as we did it before in the create_table_entry macro. lsr \\end, \\end, #SECTION_SHIFT and \\end, \\end, #PTRS_PER_TABLE - 1 // table end index The same thing is repeated for the end address. Now both start and end contains not virtual addresses, but indexes in the PMD table, corresponding to the original addresses. lsr \\phys, \\phys, #SECTION_SHIFT mov \\tmp1, #\\flags orr \\phys, \\tmp1, \\phys, lsl #SECTION_SHIFT // table entry Next, block descriptor is prepared and stored in the tmp1 variable. In order to prepare the descriptor phys parameter is first shifted to right then shifted back and merged with the flags parameter using orr instruction. If you wonder why do we have to shift the address back and forth - the answer is that this clears first 21 bit in the phys address and makes our macro universal, allowing it to be used with any address, not just the first address of the section. 9999: str \\phys, [\\tbl, \\start, lsl #3] // store the entry add \\start, \\start, #1 // next entry add \\phys, \\phys, #SECTION_SIZE // next block cmp \\start, \\end b.ls 9999b // jump back if \"Unsigned Less than or equal\" The final part of the function is executed inside a loop. Here we first store current descriptor at the right index in the PMD table. Next, we increase current index by 1 and update the descriptor to point to the next section. We repeat the same process until current index becomes equal to the last index.","title":"Populating a PMD table"},{"location":"lesson06/rpi-os/#putting-it-together-__create_page_tables","text":"Now, when you understand how create_table_entry and create_block_map macros work, it will be straightforward to understand the rest of the __create_page_tables function. adrp x0, pg_dir mov x1, #VA_START create_pgd_entry x0, x1, x2, x3 Here we create both PGD and PUD. We configure them to start mapping from VA_START virtual address. Because of the semantics of the create_table_entry macro, after create_pgd_entry finishes x0 will contain the address of the next table in the hierarchy - namely PMD. /* Mapping kernel and init stack*/ mov x1, xzr // start mapping from physical offset 0 mov x2, #VA_START // first virtual address ldr x3, =(VA_START + DEVICE_BASE - SECTION_SIZE) // last virtual address create_block_map x0, x1, x2, x3, MMU_FLAGS, x4 Next, we create virtual mapping of the whole memory, excluding device registers region. We use MMU_FLAGS constant as flags parameter - this marks all sections to be mapped as normal noncacheable memory. (Note, that MM_ACCESS flag is also specified as part of MMU_FLAGS constant. Without this flag each memory access will generate a synchronous exception.) /* Mapping device memory*/ mov x1, #DEVICE_BASE // start mapping from device base address ldr x2, =(VA_START + DEVICE_BASE) // first virtual address ldr x3, =(VA_START + PHYS_MEMORY_SIZE - SECTION_SIZE) // last virtual address create_block_map x0, x1, x2, x3, MMU_DEVICE_FLAGS, x4 Then device registers region is mapped. This is done exactly in the same way as in the previous code sample, with the exception that we are now using different start and end addresses and different flags. mov x30, x29 // restore return address ret Finally, the function restored link register and returns to the caller.","title":"Putting it together: __create_page_tables()"},{"location":"lesson06/rpi-os/#configuring-page-translation","text":"Now page tables are created and we are back to the el1_entry function. But there is still some work to be done before we can switch on the MMU. mov x0, #VA_START add sp, x0, #LOW_MEMORY We are updating init task stack pointer. Now it uses a virtual address, instead of a physical one. Therefore it could be used only after MMU is on. Recall that our kernel uses linear mapping therefore an offset is simply applied. adrp x0, pg_dir msr ttbr1_el1, x0 ttbr1_el1 is updated to point to the previously populated PGD table. Q: Isn't pg_dir a virtual address (e.g. ffff000000083000) set by the linker? ttbr shall expect a physical address, right? How could this work? ldr x0, =(TCR_VALUE) msr tcr_el1, x0 tcr_el1 of Translation Control Register is responsible for configuring some general parameters of the MMU. (For example, here we configure that both kernel and user page tables should use 4 KB pages.) ldr x0, =(MAIR_VALUE) msr mair_el1, x0 We already discussed mair register in the \"Configuring page attributes\" section. Here we just set its value. ldr x2, =kernel_main mov x0, #SCTLR_MMU_ENABLED msr sctlr_el1, x0 // BOOM! br x2 msr sctlr_el1, x0 is the line where MMU is actually enabled. Now we can jump to the kernel_main function. From this moment onward kernel runs on virtual addresses completely. An interesting question is why can't we just execute br kernel_main instruction? Indeed, we can't. Before the MMU was enabled we have been working with physical memory, the kernel is loaded at a physical offset 0 - this means that current program counter (PC) is very close to 0. Switching on the MMU doesn't update PC. br kernel_main uses offset relative to the current PC and jumps to the place were kernel_main would have been if we don't turn on the MMU. Example: in generating the kernel binary, the linker starts from base address 0xffff000000000000 as controlled by our linker script. It assigns the instruction \"br kernel_main\" to address 0xffff000000000080; it assigns kernel_main to 0xffff000000003190. The instruction \"br kernel_main\" will be a relative jump, and will be emitted as \"br #0x3110\" (we can verify this by disassembling the kernel binary). At run time, when we reach \"br kernel_main\", PC is 0x80. Executing the instruction will update PC is to 0x3190. As MMU is on now, CPU fetches instruction at 0x3190 via MMU. A translation fault! ldr x2, =kernel_main does not suffer from the problem. CPU loads x2 with the link address of kernel_main , e.g. 0xffff000000003190. Different from br kernel_main which uses PC-based offset, br x2 jumps to an absolute address stored in x2 (this is called long jmp). Therefore, PC will be updated with the link address of kernel_main which can be translated via MMU. In other words, by executing a long jmp, we \"synchronize\" the PC value with virtual addresses. Another question: why ldr x2, =kernel_main itself must be executed before we turn on the MMU? The reason is that ldr also uses pc relative offset. See the manual . On my build, it emitted as ldr x2, #0x10c . So if we execute this instruction after MMU is on but before we \"synchronize\" PC, MMU will give another translation fault.","title":"Configuring page translation"},{"location":"lesson06/rpi-os/#compiling-loading-user-programs","text":"Commodity kernels load user programs as ELF from filesystems. We won't be building a filesystem or ELF loader in this experiment. As a workaround, we will embed user programs in the kernel binary at link time, and load them at run time. For easy loading, we will store the user program in a separate ELF section of the kernel binary. Here is the relevant section of the linker script that is responsible for doing this. . = ALIGN(0x00001000); user_begin = .; .text.user : { build/user* (.text) } .rodata.user : { build/user* (.rodata) } .data.user : { build/user* (.data) } .bss.user : { build/user* (.bss) } user_end = .; I made a convention: user level source code should be defined in C source files named as \"userXXX\". The linker script then can isolate all user related code in a continuous region, of which the start and end are marked with user_begin and user_end symbols. At run time, the kernel simply copies everything between user_begin and user_end to the newly allocated process address space, thus simulating loading a user program. A simple hack, but suffice for our current purpose.","title":"Compiling &amp; loading user programs"},{"location":"lesson06/rpi-os/#aside-our-user-symbol-addresses","text":"As user programs will be linked as part the kernel binary, the linker will place all user symbols (functions & variables) in the kernel's address space (0xffff000000000000 onwards). You can verify this by, e.g. nm kernel8.elf|grep \" user_\" . How could such user programs work? We rely on an assumption: our user programs are simple enough; they always address memory with register-relative offsets but not absolute address. You can verify this by disassembly. However, if our programs, e.g. call functions via pointers, the entailed long jmp will target absolute virtual address inside kernel and will trigger exception. This assumption can't go a long way. The right solution would be link user programs and kernel separately. Right now there are 2 files that are compiled in the user region. user_sys.S This file contains definitions of the syscall wrapper functions. The RPi OS still supports the same syscalls as in the previous lesson, with the exception that now instead of clone syscall we are going to use fork syscall. The difference is that fork copies process virtual memory, and that is something we want to try doing. user.c User program source code. Almost the same as we've used in the previous lesson.","title":"Aside: our user symbol addresses"},{"location":"lesson06/rpi-os/#creating-first-user-process","text":"As it was the case in the previous lesson, move_to_user_mode function is responsible for creating the first user process. We call this function from a kernel thread. Here is how we do this. void kernel_process(){ printf(\"Kernel process started. EL %d\\r\\n\", get_el()); unsigned long begin = (unsigned long)&user_begin; unsigned long end = (unsigned long)&user_end; unsigned long process = (unsigned long)&user_process; int err = move_to_user_mode(begin, end - begin, process - begin); if (err < 0){ printf(\"Error while moving process to user mode\\n\\r\"); } } Now we need 3 arguments to call move_to_user_mode : a pointer to the beginning of the user code area, size of the area and offset of the startup function inside it. This information is calculated based on the previously discussed user_begin and user_end symbols (as global variables). move_to_user_mode function is listed below. int move_to_user_mode(unsigned long start, unsigned long size, unsigned long pc) { struct pt_regs *regs = task_pt_regs(current); regs->pstate = PSR_MODE_EL0t; regs->pc = pc; regs->sp = 2 * PAGE_SIZE; unsigned long code_page = allocate_user_page(current, 0); if (code_page == 0) { return -1; } memcpy(code_page, start, size); set_pgd(current->mm.pgd); return 0; } Now let's try to inspect in details what is going on here. struct pt_regs *regs = task_pt_regs(current); As it was the case in the previous lesson, we obtain a pointer to pt_regs area and set pstate , so that after kernel_exit we will end up in EL0. regs->pc = pc; pc now points to the offset of the startup function in the user region. regs->sp = 2 * PAGE_SIZE; We made a simple convention that our user program will not exceed 1 page in size. We allocate the second page to the stack. unsigned long code_page = allocate_user_page(current, 0); if (code_page == 0) { return -1; } allocate_user_page reserves 1 memory page and maps it to the virtual address, provided as a second argument. In the process of mapping it populates page tables, associated with the current process. We will investigate in details how this function works later in this chapter. memcpy(code_page, start, size); Next, we are going to copy the whole user region to the new address space (in the page that we have just mapped), starting from offset 0, so the offset in the user region will become an actual virtual address of the starting point. set_pgd(current->mm.pgd); Finally, we call set_pgd , which updates ttbr0_el1 register and thus activate cu4rrent process translation tables.","title":"Creating first user process"},{"location":"lesson06/rpi-os/#aside-tlb","text":"If you take a look at the set_pgd function you will see that after it sets ttbr0_el1 it also clears TLB (Translation lookaside buffer). TLB is a cache that is designed specifically to store the mapping between physical and virtual pages. The first time some virtual address is mapped into a physical one this mapping is stored in TLB. Next time we need to access the same page we no longer need to perform full page table walk. Therefore it makes perfect sense that we invalidate TLB after updating page tables - otherwise our change will not be applied for the pages already stored in the TLB. Usually, we try to avoid using all caches for simplicity, but without TLB any memory access would become extremely inefficient, and I don't think that it is even possible to completely disable TLB. Besides, TLB doesn't add any other complexity to the OS, in spite of the fact that we must clean it after switching ttbr0_el1 .","title":"Aside: TLB"},{"location":"lesson06/rpi-os/#mapping-a-virtual-page-to-user","text":"We have seen previously how allocate_user_page function is used - now it is time to see what is inside it. unsigned long allocate_user_page(struct task_struct *task, unsigned long va) { unsigned long page = get_free_page(); if (page == 0) { return 0; } map_page(task, va, page); return page + VA_START; } This function allocates a new page, maps it to the provided virtual address and returns a pointer to the page. When we say \"a pointer\" now we need to distinguish between 3 things: a pointer to a physical page, a pointer inside kernel address space and a pointer inside user address space - all these 3 different pointers can lead to the same location in memory. In our case page variable is a physical pointer (note its \"unsigned long\" type -- not a C pointer!) and the return value is a pointer inside kernel address space. This pointer can be easily calculated because we linearly map the whole physical memory starting at VA_START virtual address. Through the pointer, our kernel copies the user program to the page. Does our kernel have a virtual mapping for the new page? We do not have to worry, because kernel maps the entire physical memory in boot.S . User mapping is still required to be created and this happens in the map_page function, which we will explore next. void map_page(struct task_struct *task, unsigned long va, unsigned long page){ unsigned long pgd; if (!task->mm.pgd) { task->mm.pgd = get_free_page(); task->mm.kernel_pages[++task->mm.kernel_pages_count] = task->mm.pgd; } pgd = task->mm.pgd; int new_table; unsigned long pud = map_table((unsigned long *)(pgd + VA_START), PGD_SHIFT, va, &new_table); if (new_table) { task->mm.kernel_pages[++task->mm.kernel_pages_count] = pud; } unsigned long pmd = map_table((unsigned long *)(pud + VA_START) , PUD_SHIFT, va, &new_table); if (new_table) { task->mm.kernel_pages[++task->mm.kernel_pages_count] = pmd; } unsigned long pte = map_table((unsigned long *)(pmd + VA_START), PMD_SHIFT, va, &new_table); if (new_table) { task->mm.kernel_pages[++task->mm.kernel_pages_count] = pte; } map_table_entry((unsigned long *)(pte + VA_START), va, page); struct user_page p = {page, va}; task->mm.user_pages[task->mm.user_pages_count++] = p; } map_page in some way duplicates what we've been doing in the __create_page_tables function: it allocates and populates a page table hierarchy. There are 3 important difference, however: now we are doing this in C, instead of assembler. map_page maps a single page, instead of the whole memory, and use normal page mapping, instead of section mapping. There are 2 important functions involved in the process: map_table and map_table_entry . map_table is listed below. unsigned long map_table(unsigned long *table, unsigned long shift, unsigned long va, int* new_table) { unsigned long index = va >> shift; index = index & (PTRS_PER_TABLE - 1); if (!table[index]){ *new_table = 1; unsigned long next_level_table = get_free_page(); unsigned long entry = next_level_table | MM_TYPE_PAGE_TABLE; table[index] = entry; return next_level_table; } else { *new_table = 0; } return table[index] & PAGE_MASK; } This function has the following arguments. table This is a pointer to the parent page table. This page table is assumed to be already allocated, but might be empty. shift This argument is used to extract table index from the provided virtual address. va Virtual address itself. new_table This is an output parameter. It is set to 1 if a new child table has been allocated and left 0 otherwise. You can think of this function as an analog of the create_table_entry macro. It extracts table index from the virtual address and prepares a descriptor in the parent table that points to the child table. Unlike create_table_entry macro we don't assume that the child table should be adjacent into memory with the parent table - instead, we rely on get_free_table function to return whatever page is available. It also might be the case that child table was already allocated (This might happen if child page table covers the region where another page has been allocated previously.). In this case we set new_table to 0 and read child page table address from the parent table. map_page calls map_table 3 times: once for PGD, PUD and PMD. The last call allocates PTE and sets a descriptor in the PMD. Next, map_table_entry is called. You can see this function below. void map_table_entry(unsigned long *pte, unsigned long va, unsigned long pa) { unsigned long index = va >> PAGE_SHIFT; index = index & (PTRS_PER_TABLE - 1); unsigned long entry = pa | MMU_PTE_FLAGS; pte[index] = entry; } map_table_entry extracts PTE index from the virtual address and then prepares and sets PTE descriptor. It is similar to what we've been doing in the create_block_map macro. That's it about user page tables allocation, but map_page is responsible for one more important role: it keeps track of the pages that have been allocated during the process of virtual address mapping. All such pages are stored in the kernel_pages array. We need this array to be able to clean up allocated pages after a task exits. There is also user_pages array, which is also populated by the map_page function. This array store information about the correspondence between process virtual pages any physical pages. We need this information in order to be able to copy process virtual memory during fork (More on this later).","title":"Mapping a virtual page to user"},{"location":"lesson06/rpi-os/#forking-a-user-process","text":"Let's summarize where we are so far: we've seen how first user process is created, its page tables populated, code & data copied to the proper location and stack initialized. After all of this preparation, the process is ready to run. The code that is executed inside user process is listed below. void loop(char* str) { char buf[2] = {\"\"}; while (1){ for (int i = 0; i < 5; i++){ buf[0] = str[i]; call_sys_write(buf); user_delay(1000000); } } } void user_process() { call_sys_write(\"User process\\n\\r\"); int pid = call_sys_fork(); if (pid < 0) { call_sys_write(\"Error during fork\\n\\r\"); call_sys_exit(); return; } if (pid == 0){ loop(\"abcde\"); } else { loop(\"12345\"); } }","title":"Forking a user process"},{"location":"lesson06/rpi-os/#the-familiar-fork-semantics","text":"The code itself is very simple as we expect. Unlike clone , when doing fork we don't need to provide the function that needs to be executed in a new process. Also, the fork wrapper function is much easier than the clone one. All of this is possible because of the fact that fork make a full copy of the process virtual address space, so the fork wrapper function return twice: one time in the original process and one time in the new one. At this point, we have two identical processes, with identical stacks and pc positions. The only difference is the return value of the fork syscall: it returns child PID in the parent process and 0 in the child process. Starting from this point both processes begin completely independent life and can modify their stacks and write different things using same addresses in memory - all of this without affecting one another.","title":"The familiar fork() semantics"},{"location":"lesson06/rpi-os/#implementation","text":"Now let's see how fork system call is implemented. copy_process function does most of the job. int copy_process(unsigned long clone_flags, unsigned long fn, unsigned long arg) { preempt_disable(); struct task_struct *p; unsigned long page = allocate_kernel_page(); p = (struct task_struct *) page; struct pt_regs *childregs = task_pt_regs(p); if (!p) return -1; if (clone_flags & PF_KTHREAD) { p->cpu_context.x19 = fn; p->cpu_context.x20 = arg; } else { struct pt_regs * cur_regs = task_pt_regs(current); *childregs = *cur_regs; childregs->regs[0] = 0; copy_virt_memory(p); } p->flags = clone_flags; p->priority = current->priority; p->state = TASK_RUNNING; p->counter = p->priority; p->preempt_count = 1; //disable preemtion until schedule_tail p->cpu_context.pc = (unsigned long)ret_from_fork; p->cpu_context.sp = (unsigned long)childregs; int pid = nr_tasks++; task[pid] = p; preempt_enable(); return pid; } This function looks almost exactly the same as in the previous lesson with one exception: when copying user processes, now, instead of modifying new process stack pointer and program counter, we instead call copy_virt_memory . copy_virt_memory looks like this. int copy_virt_memory(struct task_struct *dst) { struct task_struct* src = current; for (int i = 0; i < src->mm.user_pages_count; i++) { unsigned long kernel_va = allocate_user_page(dst, src->mm.user_pages[i].virt_addr); if( kernel_va == 0) { return -1; } memcpy(kernel_va, src->mm.user_pages[i].virt_addr, PAGE_SIZE); } return 0; } It iterates over user_pages array, which contains all pages, allocated by the current process. Note, that in user_pages array we store only pages that are actually available to the process and contain its source code or data; we don't include here page table pages, which are stored in kernel_pages array. Next, for each page, we allocate another empty page and copy the original page content there. We also map the new page using the same virtual address, that is used by the original one. This is how we get the exact copy of the original process address space. All other details of the forking procedure work exactly in the same way, as they have been in the previous lesson. Q: does our fork() implement COW?","title":"Implementation"},{"location":"lesson06/rpi-os/#demand-paging","text":"If you go back and take a look at the move_to_user_mode function, you may notice that we only map a single page, starting at offset 0. But we also assume that the second page will be used as a stack. Our kernel will map stack page, as well as any other page that a process needs to access as soon as it will be requested for the first time. Now we are going to explore the inner-workings of this mechanism.","title":"Demand paging"},{"location":"lesson06/rpi-os/#setting-up-page-faults","text":"When a process tries to access some address which belongs to the page that is not yet mapped, a synchronous exception is generated. This is the second type of synchronous exception that we are going to support (the first type is an exception generated by the svc instruction which is a system call). Synchronous exception handler now looks like the following. el0_sync: kernel_entry 0 mrs x25, esr_el1 // read the syndrome register lsr x24, x25, #ESR_ELx_EC_SHIFT // exception class cmp x24, #ESR_ELx_EC_SVC64 // SVC in 64-bit state b.eq el0_svc cmp x24, #ESR_ELx_EC_DABT_LOW // data abort in EL0 b.eq el0_da handle_invalid_entry 0, SYNC_ERROR Here we use esr_el1 register to determine exception type. If it is a page fault exception (or, which is the same, data access exception) el0_da function is called. el0_da: bl enable_irq mrs x0, far_el1 mrs x1, esr_el1 bl do_mem_abort cmp x0, 0 b.eq 1f handle_invalid_entry 0, DATA_ABORT_ERROR 1: bl disable_irq kernel_exit 0 el0_da redirects the main work to the do_mem_abort function. This function takes two arguments 1. The memory address which we tried to access. This address is taken from far_el1 register (Fault address register) 1. The content of the esr_el1 (Exception syndrome register)","title":"Setting up page faults"},{"location":"lesson06/rpi-os/#handling-page-faults","text":"do_mem_abort is listed below. int do_mem_abort(unsigned long addr, unsigned long esr) { unsigned long dfs = (esr & 0b111111); if ((dfs & 0b111100) == 0b100) { unsigned long page = get_free_page(); if (page == 0) { return -1; } map_page(current, addr & PAGE_MASK, page); ind++; if (ind > 2){ return -1; } return 0; } return -1; } In order to understand this function, you need to know a little bit about the specifics of that esr_el1 register. Bits [32:26] of this register are called \"Exception Class\". We check those bits in the el0_sync handler to determine whether it is a syscall, or a data abort exception or potentially something else. Exception class determines the meaning of bits [24:0] - those bits are usually used to provide additional information about the exception. The meaning of [24:0] bits in case of the data abort exception is described on the page 2460 of the AArch64-Reference-Manual . In general, data abort exception can happen in many different scenarios (it could be a permission fault, or address size fault or a lot of other things). We are only interested in a translation fault which happens when some of the page tables for the current virtual address are not initialized. So in the first 2 lines of the do_mem_abort function, we check whether the current exception is actually a translation fault. If yes we allocate a new page and map it to the requested virtual address. All of this happens completely transparent for the user program - it doesn't notice that some of the memory accesses were interrupted and new page tables were allocated in the meantime.","title":"Handling page faults"},{"location":"lesson06/rpi-os/#conclusion","text":"This was a long and difficult chapter, but I hope it was useful as well. Virtual memory is really one of the most fundamental pieces of any operating system and I am glad we've passed through this chapter and, hopefully, started to understand how it works at the lowest level. With the introduction of virtual memory we now have full process isolation, but the RPi OS is still far from completion. It still doesn't support file systems, drivers, signals and interrupt waitlists, networking and a lot of other useful concepts, and we will continue to uncover them in the upcoming lessons. Previous Page 5.3 User processes and system calls: Exercises Next Page 6.2 Virtual memory management: Linux (in progress) 6.3 jump forward to Virtual memory management: Exercises","title":"Conclusion"}]}